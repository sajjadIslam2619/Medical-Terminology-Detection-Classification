{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Name Entity Recognition\n","This file will demonstrate NER model training, including model and input data creating, model fine-tuning and testing. "]},{"cell_type":"code","execution_count":1,"metadata":{"collapsed":true,"executionInfo":{"elapsed":3263,"status":"ok","timestamp":1659495702423,"user":{"displayName":"Jia W","userId":"01504281755790098209"},"user_tz":-480},"id":"8D_e4N40BHN0","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["import os\n","import pandas as pd\n","import math\n","import numpy as np\n","from seqeval.metrics import classification_report,accuracy_score,f1_score\n","\n","import torch\n","from torch.utils.data import Dataset\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.optim import Adam\n","from torch.utils.data import TensorDataset, Dataset, DataLoader\n","from torchcrf import CRF\n","from transformers import BertModel, BertTokenizer, BertConfig, BertForTokenClassification, \\\n","                            get_linear_schedule_with_warmup\n","\n","from sklearn import metrics\n"]},{"cell_type":"markdown","metadata":{},"source":["# Define parameters"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["BASE_MODEL = 'emilyalsentzer/Bio_ClinicalBERT'\n","\n","MAX_LEN = 64  # Length of input sequence\n","BATCH_SIZE = 32\n","EPOCHS = 50\n","LEARNING_RATE = 3e-5\n","\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","NER_MODEL_SAVED_DIR = 'trained_models/NER/'\n","MODEL_NAME = 'NER_Bert-CRF-test'\n","\n","tag2idx = {'B-problem': 0,\n","           'B-test': 1,\n","           'B-treatment': 2,\n","           'I-problem': 3,\n","           'I-test': 4,\n","           'I-treatment': 5,\n","           'O': 6,\n","           'X': 7,\n","           '[CLS]': 8,\n","           '[SEP]': 9\n","           }\n","idx2tag = {tag2idx[key]: key for key in tag2idx}\n","LABELS = ['B-problem',\n","           'B-test',\n","           'B-treatment',\n","           'I-problem',\n","           'I-test',\n","           'I-treatment']\n"]},{"cell_type":"markdown","metadata":{},"source":["## Create data loader"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["\n","class i2b2Dataset(Dataset):\n","    def __init__(self, dataframe):\n","        self.sentences = []\n","        self.labels = []\n","        self.tokenizer = BertTokenizer.from_pretrained(BASE_MODEL)\n","        self.MAX_LEN = MAX_LEN - 2\n","        agg_func = lambda s: [(w, t) for w, t in zip(s[\"word\"].values.tolist(),\n","                                                     s[\"tag\"].values.tolist())]\n","        grouped = dataframe.groupby(\"sentence #\").apply(agg_func)\n","        sentences_labels = [s for s in grouped]\n","\n","        self.sentences = [[s[0] for s in sent] for sent in sentences_labels]\n","        self.labels = [[s[1] for s in sent] for sent in sentences_labels]\n","\n","    def __getitem__(self, idx):\n","        sentence, label = self.sentences[idx], self.labels[idx]\n","        temp_lable = []\n","        temp_token = []\n","        # Tokenize each word \n","        # e.g. Admission -> Ad ##mi ##ssion\n","        #      O         -> O   X    X \n","        # Label 'X' to subtokens.\n","        for word, lab in zip(sentence, label):\n","            token_list = self.tokenizer.tokenize(word)\n","            for m, token in enumerate(token_list):\n","                temp_token.append(token)\n","                if m == 0:\n","                    temp_lable.append(lab)\n","                else:\n","                    temp_lable.append('X')\n","        # Add beginning tag and end tag to sequences.\n","        text = ['[CLS]'] + temp_token[:self.MAX_LEN] + ['[SEP]']\n","        label = ['[CLS]'] + temp_lable[:self.MAX_LEN] + ['[SEP]']\n","        # convert to ids\n","        sentence_ids = self.tokenizer.convert_tokens_to_ids(text)\n","        label_ids = [tag2idx.get(t) for t in label]\n","        seqlen = len(label_ids)\n","        return sentence_ids, label_ids, seqlen\n","\n","    def __len__(self):\n","        return len(self.sentences)\n","\n","# truncate or pad sequences\n","def pad_batch(batch):\n","    maxlen = max([i[2] for i in batch])\n","    token_tensors = torch.LongTensor([i[0] + [0] * (maxlen - len(i[0])) for i in batch])\n","    # 'O' is the label of [PAD].\n","    label_tensors = torch.LongTensor([i[1] + [tag2idx.get('O')] * (maxlen - len(i[1])) for i in batch])\n","    mask = (token_tensors > 0)\n","    return token_tensors, label_tensors, mask"]},{"cell_type":"markdown","metadata":{},"source":["## Create input data"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"CFqTMrGIBHN4","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["data_path_train = 'Data/processed/NER/merged/train.tsv'\n","data_path_dev = 'Data/processed/NER/merged/dev.tsv'\n","data_path_test = 'Data/processed/NER/merged/test.tsv'\n","train_data = pd.read_csv(data_path_train, sep=\"\\t\").astype(str)\n","dev_data = pd.read_csv(data_path_dev, sep=\"\\t\").astype(str)\n","test_data = pd.read_csv(data_path_test, sep=\"\\t\").astype(str)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"udB9CWDnBHN4","pycharm":{"name":"#%%\n"}},"outputs":[{"name":"stdout","output_type":"stream","text":["Data loaded\n"]}],"source":["train_dataset = i2b2Dataset(train_data)\n","train_iter = DataLoader(dataset=train_dataset,\n","                        batch_size=BATCH_SIZE,\n","                        shuffle=True,\n","                        collate_fn=pad_batch,\n","                        pin_memory=True\n","                        )\n","dev_dataset = i2b2Dataset(dev_data)\n","dev_iter = DataLoader(dataset=dev_dataset,\n","                        batch_size=BATCH_SIZE,\n","                        shuffle=True,\n","                        collate_fn=pad_batch,\n","                        pin_memory=True\n","                        )\n","test_dataset = i2b2Dataset(test_data)\n","test_iter = DataLoader(dataset=test_dataset,\n","                        batch_size=BATCH_SIZE,\n","                        shuffle=True,\n","                        collate_fn=pad_batch,\n","                        pin_memory=True\n","                        )\n","print('Data loaded')"]},{"cell_type":"markdown","metadata":{},"source":["## Create an NER model"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["EMBEDDING_DIM = 768\n","HIDDEN_DIM = 256\n","\n","class Bert_BiLSTM_CRF(nn.Module):\n","\n","    def __init__(self, tag2idx, embedding_dim=EMBEDDING_DIM, hidden_dim=HIDDEN_DIM):\n","        super(Bert_BiLSTM_CRF, self).__init__()\n","        self.tag_to_ix = tag2idx\n","        self.tagset_size = len(tag2idx)\n","        self.hidden_dim = hidden_dim\n","        self.embedding_dim = embedding_dim\n","\n","        self.bert = BertModel.from_pretrained(BASE_MODEL)\n","        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim // 2,\n","                            num_layers=2, bidirectional=True, batch_first=True)\n","        self.dropout = nn.Dropout(p=0.1)\n","        self.linear = nn.Linear(hidden_dim, self.tagset_size)\n","        self.crf = CRF(self.tagset_size, batch_first=True)\n","\n","    def getfeature(self, sentence):\n","        with torch.no_grad():\n","            # Two returns from BERT by default: last_hidden_state, pooler_output\n","            # last_hidden_state：the semantic vector of each position in an output sequence (batch_size, sequence_length, hidden_size)\n","            # pooler_output：\n","            # Semantic vector corresponding to [CLS] symbols with full connectivity layer and tanh activation; this vector can be used for downstream classification tasks\n","            embeds, _ = self.bert(sentence, return_dict=False)\n","        # Two returns from BERT by default: output, (h,c)\n","        # output:[batch_size,seq_len,hidden_dim * 2]   if birectional\n","        # h,c :[num_layers * 2,batch_size,hidden_dim]  if birectional\n","        # h is the hidden layer result of the last time step of the LSTM, c is the Cell state of the last time step of the LSTM\n","        out, _ = self.lstm(embeds)\n","        out = self.dropout(out)\n","        feats = self.linear(out)\n","        return feats\n","\n","    def forward(self, sentence, tags, mask, is_test=False):\n","        feature = self.getfeature(sentence)\n","        # training\n","        if not is_test:\n","            # return log-likelihood\n","            # make this value negative as our loss\n","            loss = -self.crf.forward(feature, tags, mask, reduction='mean')\n","            return loss\n","        # testing\n","        else:\n","            decode = self.crf.decode(feature, mask)\n","            return decode\n"]},{"cell_type":"markdown","metadata":{},"source":["## Load Model and define optimizer"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5158,"status":"ok","timestamp":1659083016250,"user":{"displayName":"Jia W","userId":"01504281755790098209"},"user_tz":-480},"id":"Q3aLF6glBHN_","outputId":"160936d5-4789-4f41-c847-1b13a78a0688","pycharm":{"name":"#%%\n"}},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["\n","model = Bert_BiLSTM_CRF(tag2idx).to(DEVICE)\n","optimizer = Adam(model.parameters(), lr=LEARNING_RATE, eps=1e-6)\n","# Warmup\n","len_dataset = len(train_dataset)\n","total_steps = (len_dataset // BATCH_SIZE) * EPOCHS if len_dataset % BATCH_SIZE == 0 \\\n","    else (len_dataset // BATCH_SIZE + 1) * EPOCHS\n","warm_up_ratio = 0.1\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warm_up_ratio * total_steps,\n","                                            num_training_steps=total_steps)\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"_Of5QXiyBHN-","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["best_model = None\n","_best_val_loss = float(\"inf\")\n","_best_val_acc = -float(\"inf\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## Training and test"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["def train(epoch, model, data_loader, optimizer, scheduler, device):\n","    model.train()\n","    losses = 0.0\n","    step = 0\n","    for i, batch in enumerate(data_loader):\n","        optimizer.zero_grad()\n","        step += 1\n","        contexts, labels, masks = batch\n","        contexts = contexts.to(device)\n","        labels = labels.to(device)\n","        masks = masks.to(device)\n","        loss = model(contexts, labels, masks)\n","        losses += loss.item()\n","\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","\n","    print(\"Epoch: {}, Loss:{:.4f}\".format(epoch, losses / step))\n","\n","def validate(epoch, model, data_loader, device):\n","    model.eval()\n","    Y, Y_hat = [], []\n","    losses = 0\n","    step = 0\n","    with torch.no_grad():\n","        for i, batch in enumerate(data_loader):\n","            step += 1\n","            contexts, labels, masks = batch\n","            contexts = contexts.to(device)\n","            labels = labels.to(device)\n","            masks = masks.to(device)\n","\n","            y_hat = model(contexts, labels, masks, is_test=True)\n","            loss = model(contexts, labels, masks)\n","\n","            losses += loss.item()\n","            # Save prediction\n","            for j in y_hat:\n","                # 1-dimension\n","                Y_hat.extend(j)\n","            # Save labels\n","            masks = (masks == 1)\n","            y_orig = torch.masked_select(labels, masks)\n","            Y.append(y_orig.cpu())\n","    # 2-dimension --> 1-dimension\n","    Y = torch.cat(Y, dim=0).numpy()\n","    Y_hat = np.array(Y_hat)\n","    acc = (Y_hat == Y).mean() * 100\n","\n","    print(\"Epoch: {}, Val Loss:{:.4f}, Val Acc:{:.3f}%\".format(epoch, losses / step, acc))\n","    return model, losses / step, acc\n","\n","\n","def test(model, data, device):\n","    model.eval()\n","    Y, Y_hat, y_true, y_pred = [], [], [], []\n","    with torch.no_grad():\n","        for i, batch in enumerate(data):\n","            contexts, labels, masks = batch\n","            contexts = contexts.to(device)\n","            labels = labels.to(device)\n","            masks = masks.to(device)\n","            y_hat = model(contexts, labels, masks, is_test=True)\n","            # Save prediction\n","            for j in y_hat:\n","                Y_hat.extend(j)\n","            # Save labels\n","            masks = (masks == 1)\n","            y_orig = torch.masked_select(labels, masks)\n","            Y.append(y_orig.cpu())\n","\n","    Y = torch.cat(Y, dim=0).numpy()\n","    for y_t, y_p in zip (Y, Y_hat):\n","        if idx2tag[y_t] not in [\"X\", \"[CLS]\", \"[SEP]\"]:    \n","            y_true.append(idx2tag[y_t])\n","            y_pred.append(idx2tag[y_p])\n","\n","    return y_true, y_pred"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":946400,"status":"ok","timestamp":1659083962646,"user":{"displayName":"Jia W","userId":"01504281755790098209"},"user_tz":-480},"id":"s33MtxP4BHN_","outputId":"91e0f38f-b0f0-46f2-92f4-ad02cf052514","pycharm":{"name":"#%%\n"}},"outputs":[{"name":"stdout","output_type":"stream","text":["Num examples = 14387\n","Batch size = 32\n","Epochs = 50\n","Train Start ...\n","Epoch: 1, Loss:33.7310\n","Epoch: 2, Loss:24.0599\n","Epoch: 3, Loss:17.7559\n","Epoch: 4, Loss:12.3068\n","Epoch: 5, Loss:8.5998\n","valid-->Epoch: 5, Val Loss:7.1443, Val Acc:85.833%\n","Epoch: 6, Loss:6.3561\n","Epoch: 7, Loss:5.1377\n","Epoch: 8, Loss:4.3769\n","Epoch: 9, Loss:3.8559\n","Epoch: 10, Loss:3.4792\n","valid-->Epoch: 10, Val Loss:3.0715, Val Acc:94.077%\n","Epoch: 11, Loss:3.1971\n","Epoch: 12, Loss:2.9764\n","Epoch: 13, Loss:2.7936\n","Epoch: 14, Loss:2.6477\n","Epoch: 15, Loss:2.5151\n","valid-->Epoch: 15, Val Loss:2.3384, Val Acc:95.141%\n","Epoch: 16, Loss:2.4192\n","Epoch: 17, Loss:2.3401\n","Epoch: 18, Loss:2.2411\n","Epoch: 19, Loss:2.1940\n","Epoch: 20, Loss:2.1300\n","valid-->Epoch: 20, Val Loss:2.1025, Val Acc:95.551%\n","Epoch: 21, Loss:2.0721\n","Epoch: 22, Loss:2.0265\n","Epoch: 23, Loss:1.9502\n","Epoch: 24, Loss:1.9317\n","Epoch: 25, Loss:1.8846\n","valid-->Epoch: 25, Val Loss:1.9526, Val Acc:95.788%\n","Epoch: 26, Loss:1.8510\n","Epoch: 27, Loss:1.8160\n","Epoch: 28, Loss:1.7804\n","Epoch: 29, Loss:1.7549\n","Epoch: 30, Loss:1.7100\n","valid-->Epoch: 30, Val Loss:1.9182, Val Acc:95.794%\n","Epoch: 31, Loss:1.7004\n","Epoch: 32, Loss:1.6623\n","Epoch: 33, Loss:1.6630\n","Epoch: 34, Loss:1.6258\n","Epoch: 35, Loss:1.6103\n","valid-->Epoch: 35, Val Loss:1.8752, Val Acc:95.811%\n","Epoch: 36, Loss:1.6134\n","Epoch: 37, Loss:1.5867\n","Epoch: 38, Loss:1.5856\n","Epoch: 39, Loss:1.5541\n","Epoch: 40, Loss:1.5531\n","valid-->Epoch: 40, Val Loss:1.8398, Val Acc:95.919%\n","Epoch: 41, Loss:1.5269\n","Epoch: 42, Loss:1.5161\n","Epoch: 43, Loss:1.5201\n","Epoch: 44, Loss:1.4957\n","Epoch: 45, Loss:1.4950\n","valid-->Epoch: 45, Val Loss:1.8008, Val Acc:95.998%\n","Epoch: 46, Loss:1.5068\n","Epoch: 47, Loss:1.4929\n","Epoch: 48, Loss:1.4996\n","Epoch: 49, Loss:1.4832\n","Epoch: 50, Loss:1.4907\n","valid-->Epoch: 50, Val Loss:1.8211, Val Acc:95.991%\n","Train End ... Model saved\n","Accuracy score: 0.934782\n","              precision    recall  f1-score   support\n","\n","   B-problem     0.8764    0.8665    0.8714     12420\n","      B-test     0.8812    0.8384    0.8593      9010\n"," B-treatment     0.8645    0.8617    0.8631      9056\n","   I-problem     0.8508    0.8685    0.8596     17390\n","      I-test     0.8591    0.7846    0.8202      7865\n"," I-treatment     0.8270    0.7905    0.8083      7800\n","\n","   micro avg     0.8601    0.8429    0.8514     63541\n","   macro avg     0.8598    0.8350    0.8470     63541\n","weighted avg     0.8602    0.8429    0.8512     63541\n","\n"]}],"source":["print(\"Num examples = %d\"%(len(train_dataset)))\n","print(\"Batch size = %d\"%(BATCH_SIZE))\n","print(\"Epochs = %d\"%(EPOCHS))\n","print('Train Start ...')\n","\n","for epoch in range(1, EPOCHS + 1):\n","    train(epoch, model, train_iter, optimizer, scheduler, DEVICE)\n","    if epoch % 5 == 0:\n","        print('valid-->', end='')\n","        candidate_model, loss, acc = validate(epoch, model, dev_iter, DEVICE)\n","        if loss < _best_val_loss and acc > _best_val_acc:\n","            best_model = candidate_model\n","            _best_val_loss = loss\n","            _best_val_acc = acc\n","y_test, y_pred = test(best_model, test_iter, DEVICE)\n","\n","torch.save({'model': best_model.state_dict()}, os.path.join(NER_MODEL_SAVED_DIR, MODEL_NAME + '.ckpt'))\n","print('Train End ... Model saved')\n","        \n","print(\"Accuracy score: %f\"%(metrics.accuracy_score(y_test, y_pred)))\n","print(metrics.classification_report(y_test, y_pred, labels=LABELS, digits=4))"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"ner_train_bert_crf.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.8.13 ('torch18')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"vscode":{"interpreter":{"hash":"947911e184de8b1d919e5089e3264232479411a27d726c88288dfe083da5969d"}}},"nbformat":4,"nbformat_minor":0}
