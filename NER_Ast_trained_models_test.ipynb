{
 "cells": [
  {
   "cell_type": "raw",
   "id": "1eeae809-794d-4057-a8bd-f7fbcbad213f",
   "metadata": {},
   "source": [
    "Load NER and Assertion Model from directory. \n",
    "Input is taken as string format in 'input_text' variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "630d2294-631b-4715-b171-275f551f2b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import copy\n",
    "import sys\n",
    "from seqeval.metrics import classification_report, accuracy_score, f1_score\n",
    "import stanza\n",
    "#import json\n",
    "from transformers import BertForTokenClassification, BertTokenizer\n",
    "from transformers import AutoConfig, AutoTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from scipy.special import softmax\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "328a86c9-27f6-4e6b-a1c2-3191973fbdda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58025f80d24a4621931d946a0d669cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json:   0%|   â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-06 01:11:20 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "========================\n",
      "\n",
      "2022-08-06 01:11:20 INFO: Use device: cpu\n",
      "2022-08-06 01:11:20 INFO: Loading: tokenize\n",
      "2022-08-06 01:11:21 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    nlp = stanza.Pipeline(lang=\"en\", processors=\"tokenize\")\n",
    "except Exception:\n",
    "    stanza.download(\"en\")\n",
    "    nlp = stanza.Pipeline(lang=\"en\", processors=\"tokenize\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92316412-c236-4d56-a2c8-cacd28ffa609",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2idx={'B-problem': 0,\n",
    " 'B-test': 1,\n",
    " 'B-treatment': 2,\n",
    " 'I-problem': 3,\n",
    " 'I-test': 4,\n",
    " 'I-treatment': 5,\n",
    " 'O': 6,\n",
    " 'X': 7,\n",
    " '[CLS]': 8,\n",
    " '[SEP]': 9\n",
    " }\n",
    "# Mapping index to name\n",
    "tag2name = {tag2idx[key]: key for key in tag2idx}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a7a344f-7212-41a5-b835-e5c5f1ef82b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ner_model():\n",
    "    num_labels = len(tag2idx)\n",
    "    save_model_address = './trained_models/NER/C-Bert-test'\n",
    "    model = BertForTokenClassification.from_pretrained(\n",
    "        save_model_address, num_labels=num_labels)\n",
    "    tokenizer = BertTokenizer.from_pretrained(\n",
    "        save_model_address, do_lower_case=False)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def load_assertion_model():\n",
    "    # no of classifier: present, not-present\n",
    "    num_labels = 6\n",
    "    MODEL_CLASSES = {\n",
    "        'bert': (AutoConfig, BertForSequenceClassification, AutoTokenizer),\n",
    "    }\n",
    "    MODEL_ADDRESS = 'emilyalsentzer/Bio_ClinicalBERT'\n",
    "    config_class, model_class, tokenizer_class = MODEL_CLASSES['bert']\n",
    "    model_config = config_class.from_pretrained(\n",
    "        MODEL_ADDRESS, num_labels=num_labels)\n",
    "    tokenizer = tokenizer_class.from_pretrained(\n",
    "        MODEL_ADDRESS, do_lower_case=False)\n",
    "    model = model_class.from_pretrained(MODEL_ADDRESS, config=model_config)\n",
    "    output_dir = './trained_models/Assertion/6_label_model_oversampling'\n",
    "    model = model_class.from_pretrained(output_dir)\n",
    "    tokenizer = tokenizer_class.from_pretrained(output_dir)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03590afa-110b-4e13-8a85-ea9197408e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_ner, tokenizer_ner = load_ner_model()\n",
    "model_assertion, tokenizer_assertion = load_assertion_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcaa42fb-1b44-423f-9821-153290c008ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82b277a4-87aa-47df-89fe-6b520767f747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_query(sentence, tokenizer):\n",
    "\n",
    "    temp_token = ['[CLS]']\n",
    "    # word_list = [token.text for token in sentence.tokens]\n",
    "    for word in sentence:\n",
    "        temp_token.extend(tokenizer.tokenize(word))\n",
    "    temp_token = temp_token[:128 - 1]\n",
    "    temp_token.append('[SEP]')\n",
    "    input_id = tokenizer.convert_tokens_to_ids(temp_token)\n",
    "    padding_len = MAX_LEN - len(input_id)\n",
    "    input_id = input_id + ([0] * padding_len)\n",
    "    tokenized_texts = [input_id]\n",
    "    attention_masks = [[int(i>0) for i in input_id]]\n",
    "\n",
    "    return temp_token, torch.tensor(tokenized_texts), torch.tensor(attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dedf044-b6ab-457d-acfd-2f22d16f1534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inference(model, input_ids):\n",
    "    #model.to(device)\n",
    "    #input_ids = input_ids.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, token_type_ids=None,\n",
    "                        attention_mask=None)\n",
    "        # For eval mode, the first result of outputs is logits\n",
    "        logits = outputs[0]\n",
    "    # Get NER predict result\n",
    "    predict_results = logits.detach().cpu().numpy()\n",
    "    result_arrays_soft = softmax(predict_results[0])\n",
    "\n",
    "    return np.argmax(result_arrays_soft, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5586a664-f22e-4dd5-8360-0ffcc3a9fd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_entities(long_text):\n",
    "    all_sentences = []\n",
    "    all_tags = []\n",
    "    doc = nlp(long_text)\n",
    "    for i, sentence in enumerate(doc.sentences):\n",
    "        # temp_token: tokenized words\n",
    "        # input_ids: convert temp_token to id\n",
    "        word_list = [token.text for token in sentence.tokens]\n",
    "        temp_token, input_ids, attention_masks = create_query(word_list, tokenizer_ner)\n",
    "        result_list = model_inference(model_ner, input_ids)\n",
    "        result = [tag2name[t] for t in result_list]\n",
    "        pretok_sent = \"\"\n",
    "        pretags = \"\"\n",
    "        for i, tok in enumerate(temp_token):\n",
    "            if tok.startswith(\"##\"):\n",
    "                pretok_sent += tok[2:]\n",
    "            else:\n",
    "                pretok_sent += f\" {tok}\"\n",
    "                pretags += f\" {result[i]}\"\n",
    "        pretok_sent = pretok_sent[1:]\n",
    "        pretags = pretags[1:]\n",
    "        s = pretok_sent.split()\n",
    "        t = pretags.split()\n",
    "        all_sentences.append(s)\n",
    "        all_tags.append(t)\n",
    "    return all_sentences, all_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e51d8e1d-5c14-4ec0-8300-d0b54bc92eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NER_input_text = \"He had no cardiac murmur .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a4aa8e9-a07c-494d-ba0d-976a4ae78312",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_sentences, all_tags = predict_entities (NER_input_text)\n",
    "\n",
    "#print (all_sentences)\n",
    "#print (all_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a84d62f-1d52-4c0f-a994-e057c3462a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract index of entity\n",
    "# extract sentence with assertion\n",
    "def entity_extractor(all_sentences, all_tags):\n",
    "    sentences_with_problem = []\n",
    "    all_problems_in_text_tmp = []\n",
    "    all_treatment_in_text = []\n",
    "    all_test_in_text = []\n",
    "\n",
    "    for s, t in zip(all_sentences, all_tags):\n",
    "        flag_treatment, flag_problem, flag_test = 0, 0, 0\n",
    "        problem_in_sentence = ''\n",
    "        treatment_in_sentence = []\n",
    "        test_in_sentence = []\n",
    "        for i in range(1, len(t)-1):\n",
    "            if t[i] == 'B-problem':\n",
    "                flag_problem = 1\n",
    "                # if there is entities, add the index of sentence to a list\n",
    "                # sentences_with_problem.append(n)\n",
    "                # append the index of entity to a list\n",
    "                if problem_in_sentence:\n",
    "                    problem_in_sentence = f'{problem_in_sentence}| {str(i)}'\n",
    "                else:\n",
    "                    problem_in_sentence += str(i)\n",
    "            elif t[i] == 'I-problem' or t[i] == 'X' and flag_problem == 1:\n",
    "                problem_in_sentence = f'{problem_in_sentence} {str(i)}'\n",
    "            elif t[i] == 'B-test':\n",
    "                flag_test = 1\n",
    "                test_in_sentence.append(i)\n",
    "            elif t[i] == 'I-test' or t[i] == 'X' and flag_test == 1:\n",
    "                test_in_sentence.append(i)\n",
    "            elif t[i] == 'B-treatment':\n",
    "                flag_treatment = 1\n",
    "                treatment_in_sentence.append(i)\n",
    "            elif t[i] == 'I-treatment' or t[i] == 'X' and flag_treatment == 1:\n",
    "                treatment_in_sentence.append(i)\n",
    "            elif t[i] in ['O', 'X']:\n",
    "                flag_treatment, flag_problem, flag_test = 0, 0, 0\n",
    "                # print(s[i], end=' ')\n",
    "        all_problems_in_text_tmp.append(problem_in_sentence)\n",
    "        all_treatment_in_text.append(treatment_in_sentence)\n",
    "        all_test_in_text.append(test_in_sentence)\n",
    "\n",
    "    # create sentences with '[entity]' tag\n",
    "    all_problems_in_text = []\n",
    "    sentences_with_problem = []\n",
    "    for sentence, problem_index in zip(all_sentences, all_problems_in_text_tmp):\n",
    "        # print(problem_index)\n",
    "        if problem_index:\n",
    "            index = problem_index.split('|')\n",
    "            tmp = [i.split() for i in index]\n",
    "            all_problems_in_text.append(tmp)\n",
    "            for i_list in tmp:\n",
    "                s = copy.deepcopy(sentence)\n",
    "                s.insert(int(i_list[-1])+1, '[entity]')\n",
    "                s.insert(int(i_list[0]), '[entity]')\n",
    "                s = ' '.join(s)\n",
    "                sentences_with_problem.append(s)\n",
    "        else:\n",
    "            # sentences_with_problem.append(sentence)\n",
    "            all_problems_in_text.append(problem_index)\n",
    "\n",
    "    return sentences_with_problem, all_problems_in_text, all_treatment_in_text, all_test_in_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ad1b0f3-3a23-49b3-9c9e-e0c9cd3f40dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assertion_input_creator(sentences, tokenizer, add_special_tokens=True):\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    # For every sentence...\n",
    "    for sent in sentences:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            sent,                      # Sentence to encode.\n",
    "            add_special_tokens=add_special_tokens,  # Add '[CLS]' and '[SEP]'\n",
    "            max_length=128,           # Pad & truncate all sentences.\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,   # Construct attn. masks.\n",
    "            return_tensors='pt',     # Return pytorch tensors.\n",
    "        )\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_mask.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    # Convert the lists into tensors.\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_mask = torch.cat(attention_mask, dim=0)\n",
    "    return torch.tensor(input_ids), torch.tensor(attention_mask)\n",
    "\n",
    "\n",
    "def assertion_model_inference(model, input_ids, attention_mask):\n",
    "    #model.to(device)\n",
    "    #input_ids = input_ids.to(device)\n",
    "    #attention_mask = attention_mask.to(device)\n",
    "\n",
    "    predictions, true_labels = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        result = model(input_ids, token_type_ids=None,\n",
    "                       attention_mask=attention_mask, return_dict=True)\n",
    "\n",
    "    logits = result.logits\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    predictions.append(logits)\n",
    "\n",
    "    pred_labels_i = np.argmax(logits, axis=1).flatten()\n",
    "\n",
    "    def index2label(x):\n",
    "        if x == 0:\n",
    "            return 'Present'\n",
    "        elif x == 1:\n",
    "            return 'Possible'\n",
    "        elif x == 2:\n",
    "            return 'Conditional'\n",
    "        elif x == 3:\n",
    "            return 'Associated with someone else'\n",
    "        elif x == 4:\n",
    "            return 'Hypothetical'\n",
    "        elif x == 5:\n",
    "            return 'Absent'\n",
    "\n",
    "    pred_labels = map(index2label, pred_labels_i)\n",
    "    return list(pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09129339-8db0-4be8-86de-231347ada7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_assertion(all_sentences, all_tags):\n",
    "\n",
    "    # extract index of entity\n",
    "    # extract sentence with assertion\n",
    "\n",
    "    (\n",
    "        sentences_with_problem,\n",
    "        all_problems_in_text,\n",
    "        all_treatment_in_text,\n",
    "        all_test_in_text,\n",
    "    ) = entity_extractor(all_sentences, all_tags)\n",
    "    input_ids, attention_mask = assertion_input_creator(\n",
    "        sentences_with_problem, tokenizer_assertion, add_special_tokens=False\n",
    "    )\n",
    "    pred_labels = assertion_model_inference(model_assertion, input_ids, attention_mask)\n",
    "    # map labels wordwisely.\n",
    "    \"\"\"\n",
    "    e.g. problem_list = [[[4, 5, 6], [10]], '', [[2, 3], [6,7]]]\n",
    "        label = ['yes', 'no', 'what', 'yes']\n",
    "    resutls: [['yes', 'yes', 'yes', 'no'], '', ['what', 'what', 'yes', 'yes']]\n",
    "    \"\"\"\n",
    "    i_label = 0\n",
    "    labels_in_sentence = []\n",
    "    for index in all_problems_in_text:\n",
    "        if index:\n",
    "            tmp = []\n",
    "            for i_p in index:\n",
    "                tmp.extend([pred_labels[i_label]] * len(i_p))\n",
    "                i_label += 1\n",
    "            labels_in_sentence.append(tmp)\n",
    "        else:\n",
    "            labels_in_sentence.append(index)\n",
    "\n",
    "    all_problems_in_text_flatten = list(\n",
    "        map(lambda l: [int(item) for elem in l for item in elem], all_problems_in_text)\n",
    "    )\n",
    "    return (\n",
    "        labels_in_sentence,\n",
    "        all_problems_in_text_flatten,\n",
    "        all_treatment_in_text,\n",
    "        all_test_in_text,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c226e46-1eec-4d61-a847-62f1509b604c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method add start-tag if there is no Begin (B) before Inside (I) and Unseen (X). \n",
    "# Ast : - Present   Present  - Present   Absent    Absent    -\n",
    "# Tag : O I-problem I-roblem O I-problem I-problem I-problem O\n",
    "# output : O <present> B-problem </present> <present> B-problem X I-problem </present> <absent> B-problem I-problem </absent> O \n",
    "\n",
    "def add_problem_start_tag(\n",
    "    output_text_with_classification,\n",
    "    word,\n",
    "    i,\n",
    "    tags,\n",
    "    assertion_index,\n",
    "    pred_assertion,\n",
    "    list_ast_entity,\n",
    "    assertion,\n",
    "    tag_string,\n",
    "):\n",
    "    if i-1 not in assertion_index:\n",
    "        # Ast   : -    P   -     P    -\n",
    "        # Tag   : O <> I I O <> I I I O\n",
    "        list_ast_entity.append(word)\n",
    "        output_text_with_classification = (\n",
    "            output_text_with_classification + tag_string + word\n",
    "        )\n",
    "    else: \n",
    "        prev_index = assertion_index.index(i)\n",
    "        if pred_assertion[prev_index] != assertion:\n",
    "            # Ast   : -     - P     A   -\n",
    "            # Tag   : O I I O I <>  I I O\n",
    "            list_ast_entity.append(word)\n",
    "            output_text_with_classification = (\n",
    "                output_text_with_classification + tag_string + word\n",
    "            )\n",
    "        else :\n",
    "            list_ast_entity[-1] = (\n",
    "                list_ast_entity[-1] + \" \" + word\n",
    "            )\n",
    "            output_text_with_classification = (\n",
    "                output_text_with_classification + \" \" + word\n",
    "            )\n",
    "\n",
    "    return output_text_with_classification, list_ast_entity\n",
    "\n",
    "\n",
    "# This method add end-tag for all cases. \n",
    "# Ast : - Present   Present  Present Present   Absent    Absent    -\n",
    "# Tag : O B-problem B-roblem X       I-problem B-problem I-problem O\n",
    "# output : O <present> B-problem </present> <present> B-problem X I-problem </present> <absent> B-problem I-problem </absent> O \n",
    "\n",
    "def add_problem_end_tag(\n",
    "    output_text_with_classification,\n",
    "    i,\n",
    "    tags,\n",
    "    assertion_index,\n",
    "    pred_assertion,\n",
    "    assertion,\n",
    "    tag_string,\n",
    "):\n",
    "    if i + 1 in assertion_index:\n",
    "        next_index = assertion_index.index(i + 1)\n",
    "        if pred_assertion[next_index] == assertion:\n",
    "            # Ast :  P     P\n",
    "            # Tag: O B </> B X I B I O\n",
    "            if tags[i+1] == \"B-problem\": \n",
    "                output_text_with_classification = (\n",
    "                    output_text_with_classification + tag_string\n",
    "                )\n",
    "        else :\n",
    "            # Ast :        P     A \n",
    "            # Tag: O B B X I </> B I O\n",
    "            output_text_with_classification = (\n",
    "                output_text_with_classification + tag_string\n",
    "            )\n",
    "            \n",
    "    else:\n",
    "        # Ast :             A\n",
    "        # Tag : O B B X I B I </> O\n",
    "        output_text_with_classification = output_text_with_classification + tag_string\n",
    "\n",
    "    return output_text_with_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f571eeb4-cfd0-49b7-a76e-d130f9dc523c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentence(\n",
    "    sentence,\n",
    "    tags,\n",
    "    pred_assertion,\n",
    "    assertion_index,\n",
    "    treatment_index,\n",
    "    test_index,\n",
    "    output_text_with_classification,\n",
    "    list_ast_present_entity,\n",
    "    list_ast_absent_entity,\n",
    "    list_ast_posssible_entity,\n",
    "    list_ast_conditional_entity,\n",
    "    list_ast_hyphothetical_entity,\n",
    "    list_ast_associated_entity,\n",
    "    list_treatment_entity,\n",
    "    list_test_entity,\n",
    "):\n",
    "    #print('sentence ::: ', sentence)\n",
    "    #print('tags ::: ', tags)\n",
    "    #print('pred_assertion ::: ', pred_assertion)\n",
    "    #print('assertion_index ::: ', assertion_index)\n",
    "    #print('treatment_index ::: ', treatment_index)\n",
    "    #print('test_index ::: ', test_index)\n",
    "\n",
    "    for i, word in enumerate(sentence):\n",
    "\n",
    "        if i in assertion_index:\n",
    "            index = assertion_index.index(i)\n",
    "            if tags[i] == \"B-problem\":\n",
    "                if pred_assertion[index] == \"Present\":\n",
    "                    list_ast_present_entity.append(word)\n",
    "                    output_text_with_classification = (\n",
    "                        output_text_with_classification + \" <Problem-present> \" + word\n",
    "                    )\n",
    "                    # For one word problem-entity, there is only 'beginning' tag no 'inside' tag\n",
    "                    # tags : 'O', 'O', 'B-problem', 'O', 'O',\n",
    "                    output_text_with_classification = add_problem_end_tag(\n",
    "                        output_text_with_classification,\n",
    "                        i,\n",
    "                        tags,\n",
    "                        assertion_index,\n",
    "                        pred_assertion,\n",
    "                        \"Present\",\n",
    "                        \" </Problem-present> \",\n",
    "                    )\n",
    "\n",
    "                elif pred_assertion[index] == \"Possible\":\n",
    "                    list_ast_posssible_entity.append(word)\n",
    "                    output_text_with_classification = (\n",
    "                        output_text_with_classification + \" <Problem-possible> \" + word\n",
    "                    )\n",
    "                    output_text_with_classification = add_problem_end_tag(\n",
    "                        output_text_with_classification,\n",
    "                        i,\n",
    "                        tags,\n",
    "                        assertion_index,\n",
    "                        pred_assertion,\n",
    "                        \"Possible\",\n",
    "                        \" </Problem-possible> \",\n",
    "                    )\n",
    "\n",
    "                elif pred_assertion[index] == \"Conditional\":\n",
    "                    list_ast_conditional_entity.append(word)\n",
    "                    output_text_with_classification = (\n",
    "                        output_text_with_classification\n",
    "                        + \" <Problem-conditional> \"\n",
    "                        + word\n",
    "                    )\n",
    "                    output_text_with_classification = add_problem_end_tag(\n",
    "                        output_text_with_classification,\n",
    "                        i,\n",
    "                        tags,\n",
    "                        assertion_index,\n",
    "                        pred_assertion,\n",
    "                        \"Conditional\",\n",
    "                        \" </Problem-conditional> \",\n",
    "                    )\n",
    "\n",
    "                elif pred_assertion[index] == \"Hypothetical\":\n",
    "                    list_ast_hyphothetical_entity.append(word)\n",
    "                    output_text_with_classification = (\n",
    "                        output_text_with_classification\n",
    "                        + \" <Problem-hypothetical> \"\n",
    "                        + word\n",
    "                    )\n",
    "                    output_text_with_classification = add_problem_end_tag(\n",
    "                        output_text_with_classification,\n",
    "                        i,\n",
    "                        tags,\n",
    "                        assertion_index,\n",
    "                        pred_assertion,\n",
    "                        \"Hypothetical\",\n",
    "                        \" </Problem-hypothetical> \",\n",
    "                    )\n",
    "\n",
    "                elif pred_assertion[index] == \"Associated with someone else\":\n",
    "                    list_ast_associated_entity.append(word)\n",
    "                    output_text_with_classification = (\n",
    "                        output_text_with_classification\n",
    "                        + \" <Problem-associated> \"\n",
    "                        + word\n",
    "                    )\n",
    "                    output_text_with_classification = add_problem_end_tag(\n",
    "                        output_text_with_classification,\n",
    "                        i,\n",
    "                        tags,\n",
    "                        assertion_index,\n",
    "                        pred_assertion,\n",
    "                        \"Associated with someone else\",\n",
    "                        \" </Problem-associated> \",\n",
    "                    )\n",
    "\n",
    "                elif pred_assertion[index] == \"Absent\":\n",
    "                    list_ast_absent_entity.append(word)\n",
    "                    output_text_with_classification = (\n",
    "                        output_text_with_classification + \" <Problem-absent> \" + word\n",
    "                    )\n",
    "                    output_text_with_classification = add_problem_end_tag(\n",
    "                        output_text_with_classification,\n",
    "                        i,\n",
    "                        tags,\n",
    "                        assertion_index,\n",
    "                        pred_assertion,\n",
    "                        \"Absent\",\n",
    "                        \" </Problem-absent> \",\n",
    "                    )\n",
    "\n",
    "            elif tags[i] == \"I-problem\" or tags[i] == \"X\":\n",
    "                if pred_assertion[index] == \"Present\":\n",
    "                    (output_text_with_classification, list_ast_present_entity) = add_problem_start_tag(\n",
    "                        output_text_with_classification,\n",
    "                        word,\n",
    "                        i,\n",
    "                        tags,\n",
    "                        assertion_index,\n",
    "                        pred_assertion,\n",
    "                        list_ast_present_entity,\n",
    "                        \"Present\",\n",
    "                        \" <Problem-present> \",\n",
    "                    )\n",
    "\n",
    "                    output_text_with_classification = add_problem_end_tag(\n",
    "                        output_text_with_classification,\n",
    "                        i,\n",
    "                        tags,\n",
    "                        assertion_index,\n",
    "                        pred_assertion,\n",
    "                        \"Present\",\n",
    "                        \" </Problem-present> \",\n",
    "                    )\n",
    "\n",
    "                elif pred_assertion[index] == \"Possible\":\n",
    "\n",
    "                    (output_text_with_classification, list_ast_posssible_entity) = add_problem_start_tag(\n",
    "                        output_text_with_classification,\n",
    "                        word,\n",
    "                        i,\n",
    "                        tags,\n",
    "                        assertion_index,\n",
    "                        pred_assertion,\n",
    "                        list_ast_posssible_entity,\n",
    "                        \"Possible\",\n",
    "                        \" <Problem-possible> \",\n",
    "                    )\n",
    "                    output_text_with_classification = add_problem_end_tag(\n",
    "                        output_text_with_classification,\n",
    "                        i,\n",
    "                        tags,\n",
    "                        assertion_index,\n",
    "                        pred_assertion,\n",
    "                        \"Possible\",\n",
    "                        \" </Problem-possible> \",\n",
    "                    )\n",
    "\n",
    "                elif pred_assertion[index] == \"Conditional\":\n",
    "\n",
    "                    (output_text_with_classification, list_ast_conditional_entity) = add_problem_start_tag(\n",
    "                        output_text_with_classification,\n",
    "                        word,\n",
    "                        i,\n",
    "                        tags,\n",
    "                        assertion_index,\n",
    "                        pred_assertion,\n",
    "                        list_ast_conditional_entity,\n",
    "                        \"Conditional\",\n",
    "                        \" <Problem-conditional> \",\n",
    "                    )\n",
    "                    output_text_with_classification = add_problem_end_tag(\n",
    "                        output_text_with_classification,\n",
    "                        i,\n",
    "                        tags,\n",
    "                        assertion_index,\n",
    "                        pred_assertion,\n",
    "                        \"Conditional\",\n",
    "                        \" </Problem-conditional> \",\n",
    "                    )\n",
    "\n",
    "                elif pred_assertion[index] == \"Hypothetical\":\n",
    "\n",
    "                    (output_text_with_classification, list_ast_hyphothetical_entity) = add_problem_start_tag(\n",
    "                        output_text_with_classification,\n",
    "                        word,\n",
    "                        i,\n",
    "                        tags,\n",
    "                        assertion_index,\n",
    "                        pred_assertion,\n",
    "                        list_ast_hyphothetical_entity,\n",
    "                        \"Hypothetical\",\n",
    "                        \" <Problem-hypothetical> \",\n",
    "                    )\n",
    "                    output_text_with_classification = add_problem_end_tag(\n",
    "                        output_text_with_classification,\n",
    "                        i,\n",
    "                        tags,\n",
    "                        assertion_index,\n",
    "                        pred_assertion,\n",
    "                        \"Hypothetical\",\n",
    "                        \" </Problem-hypothetical> \",\n",
    "                    )\n",
    "\n",
    "                elif pred_assertion[index] == \"Associated with someone else\":\n",
    "\n",
    "                    (output_text_with_classification, list_ast_associated_entity) = add_problem_start_tag(\n",
    "                        output_text_with_classification,\n",
    "                        word,\n",
    "                        i,\n",
    "                        tags,\n",
    "                        assertion_index,\n",
    "                        pred_assertion,\n",
    "                        list_ast_associated_entity,\n",
    "                        \"Associated with someone else\",\n",
    "                        \" <Problem-associated> \",\n",
    "                    )\n",
    "                    output_text_with_classification = add_problem_end_tag(\n",
    "                        output_text_with_classification,\n",
    "                        i,\n",
    "                        tags,\n",
    "                        assertion_index,\n",
    "                        pred_assertion,\n",
    "                        \"Associated with someone else\",\n",
    "                        \" </Problem-associated> \",\n",
    "                    )\n",
    "\n",
    "                elif pred_assertion[index] == \"Absent\":\n",
    "\n",
    "                    (output_text_with_classification, list_ast_absent_entity) = add_problem_start_tag(\n",
    "                        output_text_with_classification,\n",
    "                        word,\n",
    "                        i,\n",
    "                        tags,\n",
    "                        assertion_index,\n",
    "                        pred_assertion,\n",
    "                        list_ast_absent_entity,\n",
    "                        \"Absent\",\n",
    "                        \" <Problem-absent> \",\n",
    "                    )\n",
    "                    output_text_with_classification = add_problem_end_tag(\n",
    "                        output_text_with_classification,\n",
    "                        i,\n",
    "                        tags,\n",
    "                        assertion_index,\n",
    "                        pred_assertion,\n",
    "                        \"Absent\",\n",
    "                        \" </Problem-absent> \",\n",
    "                    )\n",
    "\n",
    "        elif i in treatment_index:\n",
    "            if tags[i] == \"B-treatment\":\n",
    "                list_treatment_entity.append(word)\n",
    "                output_text_with_classification = (\n",
    "                    output_text_with_classification + \" <Treatment> \" + word\n",
    "                )\n",
    "                # For one word Treatment-entity, there is only 'beginning' tag no 'inside' tag\n",
    "                if tags[i + 1] != \"I-treatment\" and tags[i + 1] != \"X\":\n",
    "                    output_text_with_classification = (\n",
    "                        output_text_with_classification + \" </Treatment> \"\n",
    "                    )\n",
    "            elif tags[i] == \"I-treatment\" or tags[i] == \"X\":\n",
    "                if i-1 not in treatment_index:\n",
    "                    # Tag: O I-treatment O I-treatment X 0 \n",
    "                    list_treatment_entity.append(word)\n",
    "                    output_text_with_classification = (\n",
    "                        output_text_with_classification + \" <Treatment> \" + word\n",
    "                    )\n",
    "                else : \n",
    "                    list_treatment_entity[-1] = list_treatment_entity[-1] + \" \" + word\n",
    "                    output_text_with_classification = (\n",
    "                        output_text_with_classification + \" \" + word\n",
    "                    )\n",
    "\n",
    "                if tags[i + 1] != \"I-treatment\" and tags[i + 1] != \"X\":\n",
    "                    output_text_with_classification = (\n",
    "                        output_text_with_classification + \" </Treatment> \"\n",
    "                    )\n",
    "        elif i in test_index:\n",
    "            if tags[i] == \"B-test\":\n",
    "                list_test_entity.append(word)\n",
    "                output_text_with_classification = (\n",
    "                    output_text_with_classification + \" <Test> \" + word\n",
    "                )\n",
    "                # For one word Test-entity, there is only 'beginning' tag no 'inside' tag\n",
    "                if tags[i + 1] != \"I-test\" and tags[i + 1] != \"X\":\n",
    "                    output_text_with_classification = (\n",
    "                        output_text_with_classification + \" </Test> \"\n",
    "                    )\n",
    "            elif tags[i] == \"I-test\" or tags[i] == \"X\":\n",
    "                if i-1 not in test_index:\n",
    "                    # Tag: O I-test O I-test X 0 \n",
    "                    list_test_entity.append(word)\n",
    "                    output_text_with_classification = (\n",
    "                        output_text_with_classification + \" <Test> \" + word\n",
    "                    )\n",
    "                else :\n",
    "                    list_test_entity[-1] = list_test_entity[-1] + \" \" + word\n",
    "                    output_text_with_classification = (\n",
    "                        output_text_with_classification + \" \" + word\n",
    "                    )\n",
    "                if tags[i + 1] != \"I-test\" and tags[i + 1] != \"X\":\n",
    "                    output_text_with_classification = (\n",
    "                        output_text_with_classification + \" </Test> \"\n",
    "                    )\n",
    "        else:\n",
    "            if word.strip() != \"[SEP]\" and word.strip() != \"[CLS]\":\n",
    "                output_text_with_classification = (\n",
    "                    output_text_with_classification + \" \" + word\n",
    "                )\n",
    "\n",
    "    return (\n",
    "        output_text_with_classification,\n",
    "        list_ast_present_entity,\n",
    "        list_ast_absent_entity,\n",
    "        list_ast_posssible_entity,\n",
    "        list_ast_conditional_entity,\n",
    "        list_ast_hyphothetical_entity,\n",
    "        list_ast_associated_entity,\n",
    "        list_treatment_entity,\n",
    "        list_test_entity,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6f0ecd-641c-4de4-9304-1e60397791d8",
   "metadata": {},
   "source": [
    "# Insert input text.\n",
    "\n",
    "If input is long, it may take few seconds/minutes to predict.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02eac5ad-5bae-4eb7-9a42-b950b6bb17fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Brief Hospital Course : Admitted 05-08 to cardiology service for surgical work-up . INR 2.7 and echo repeated , coumadin held , and heparin drips started . Review of OSH echo revealed peak gradient of 64mm (  not 30 's ) . New echo showed pannus involving the valve and ? thrombus . Seen by Dr. Laura of cardiac surgery and underwent redo AVR with aortic root enlargement on 05-11 . Transferred to the CSRU in stable condition on phenylephrine and propofol drips . Weaned to extubation on POD #2 and off all drips . Episode of AFib that evening treated with amiodarone and converted to SR .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2f994c8-952e-4e71-87c8-55bebdc64981",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Significant for chronic atrial fibrillation , managed with an Italian variation of Digoxin , and Amioadrone , hypertension managed with an ACE inhibitor , chronic obstructive pulmonary disease managed with a zanthine preparation and low dose steroids , as well as an occasional inhaler .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83f26e5e-9631-4c7a-abfc-d3de786936d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/Users/sajjadislam/opt/anaconda3/envs/py_venv_nmdsi/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2263: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/var/folders/p3/s4n39zb50rb2l96w9n054ch80000gn/T/ipykernel_930/2074899399.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(input_ids), torch.tensor(attention_mask)\n"
     ]
    }
   ],
   "source": [
    "all_sentences, all_tags = predict_entities(input_text)\n",
    "\n",
    "(assertion_in_sentence,\n",
    "    all_problems_in_text_flatten,\n",
    "    all_treatment_in_text,\n",
    "    all_test_in_text,\n",
    ") = predict_assertion(all_sentences, all_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "65d386ae-8e34-4e23-bbaa-534e25397515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These lists are to collect problem-entity label-wise and display on the UI Table\n",
    "list_ast_present_entity = []\n",
    "list_ast_absent_entity = []\n",
    "list_ast_posssible_entity = []\n",
    "list_ast_conditional_entity = []\n",
    "list_ast_hyphothetical_entity = []\n",
    "list_ast_associated_entity = []\n",
    "\n",
    "list_treatment_entity = []\n",
    "list_test_entity = []\n",
    "\n",
    "# This string is to format full clinical text and add <tag>, like  <Problem-present> problem-entity </Problem-present>\n",
    "output_text_with_classification = \"\"\n",
    "\n",
    "for (\n",
    "    sentence,\n",
    "    tags,\n",
    "    pred_assertion,\n",
    "    assertion_index,\n",
    "    treatment_index,\n",
    "    test_index,\n",
    ") in zip(\n",
    "    all_sentences,\n",
    "    all_tags,\n",
    "    assertion_in_sentence,\n",
    "    all_problems_in_text_flatten,\n",
    "    all_treatment_in_text,\n",
    "    all_test_in_text,\n",
    "):\n",
    "    (\n",
    "        output_text_with_classification,\n",
    "        list_ast_present_entity,\n",
    "        list_ast_absent_entity,\n",
    "        list_ast_posssible_entity,\n",
    "        list_ast_conditional_entity,\n",
    "        list_ast_hyphothetical_entity,\n",
    "        list_ast_associated_entity,\n",
    "        list_treatment_entity,\n",
    "        list_test_entity,\n",
    "    ) = process_sentence(\n",
    "        sentence,\n",
    "        tags,\n",
    "        pred_assertion,\n",
    "        assertion_index,\n",
    "        treatment_index,\n",
    "        test_index,\n",
    "        output_text_with_classification,\n",
    "        list_ast_present_entity,\n",
    "        list_ast_absent_entity,\n",
    "        list_ast_posssible_entity,\n",
    "        list_ast_conditional_entity,\n",
    "        list_ast_hyphothetical_entity,\n",
    "        list_ast_associated_entity,\n",
    "        list_treatment_entity,\n",
    "        list_test_entity,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b0a21094-9cd3-44af-98c3-5a6ed88405b2",
   "metadata": {},
   "source": [
    "# In output text entity is enclosed between <start-tag> and </end-tag>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "00b21563-8421-4bfb-8d33-8caf2a8a308a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output text ::   Significant for <Problem-present> chronic atrial fibrillation </Problem-present>  , managed with an Italian variation of <Treatment> Digoxin </Treatment>  , and <Treatment> Amioadrone </Treatment>  , <Problem-present> hypertension </Problem-present>  managed with <Treatment> an ACE inhibitor </Treatment>  , <Problem-present> chronic obstructive pulmonary disease </Problem-present>  managed with <Treatment> a zanthine preparation </Treatment>  and <Treatment> low dose steroids </Treatment>  , as well as <Treatment> an </Treatment>  <Problem-present> occasional inhaler </Problem-present>  .\n"
     ]
    }
   ],
   "source": [
    "print(\"output text :: \", output_text_with_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4452634a-7bd4-4ccc-aa10-078d3c55d392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Present entity list :  ['chronic atrial fibrillation', 'hypertension', 'chronic obstructive pulmonary disease', 'occasional inhaler']\n",
      "Absent entity list :  []\n",
      "Possible entity list :  []\n",
      "Conditional entity list :  []\n",
      "Hyphothetical entity list :  []\n",
      "Associated entity list :  []\n",
      "Treatment entity list :  ['Digoxin', 'Amioadrone', 'an ACE inhibitor', 'a zanthine preparation', 'low dose steroids', 'an']\n",
      "Test entity list :  []\n"
     ]
    }
   ],
   "source": [
    "print ('Present entity list : ', list_ast_present_entity)\n",
    "print ('Absent entity list : ', list_ast_absent_entity)\n",
    "print ('Possible entity list : ', list_ast_posssible_entity)\n",
    "print ('Conditional entity list : ', list_ast_conditional_entity)\n",
    "print ('Hyphothetical entity list : ', list_ast_hyphothetical_entity)\n",
    "print ('Associated entity list : ', list_ast_associated_entity)\n",
    "print ('Treatment entity list : ', list_treatment_entity)\n",
    "print ('Test entity list : ', list_test_entity)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
