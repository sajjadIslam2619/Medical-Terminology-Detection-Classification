{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6106,"status":"ok","timestamp":1650230226851,"user":{"displayName":"sajjad islam","userId":"16503418985020638021"},"user_tz":300},"id":"rhikf-Vjezzh","outputId":"a7aa73e6-7a84-4bb9-e53d-ddac7926f79d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: seqeval in /Users/sajjadislam/opt/anaconda3/envs/py_venv_nmdsi/lib/python3.8/site-packages (1.2.2)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /Users/sajjadislam/opt/anaconda3/envs/py_venv_nmdsi/lib/python3.8/site-packages (from seqeval) (1.0.2)\n","Requirement already satisfied: numpy>=1.14.0 in /Users/sajjadislam/opt/anaconda3/envs/py_venv_nmdsi/lib/python3.8/site-packages (from seqeval) (1.18.5)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/sajjadislam/opt/anaconda3/envs/py_venv_nmdsi/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /Users/sajjadislam/opt/anaconda3/envs/py_venv_nmdsi/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval) (1.1.0)\n","Requirement already satisfied: scipy>=1.1.0 in /Users/sajjadislam/opt/anaconda3/envs/py_venv_nmdsi/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval) (1.8.0)\n","Requirement already satisfied: transformers in /Users/sajjadislam/opt/anaconda3/envs/py_venv_nmdsi/lib/python3.8/site-packages (4.18.0)\n","Requirement already satisfied: regex!=2019.12.17 in /Users/sajjadislam/opt/anaconda3/envs/py_venv_nmdsi/lib/python3.8/site-packages (from transformers) (2022.3.15)\n","Requirement already satisfied: numpy>=1.17 in /Users/sajjadislam/opt/anaconda3/envs/py_venv_nmdsi/lib/python3.8/site-packages (from transformers) (1.18.5)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /Users/sajjadislam/opt/anaconda3/envs/py_venv_nmdsi/lib/python3.8/site-packages (from transformers) (0.12.1)\n","Requirement already satisfied: packaging>=20.0 in /Users/sajjadislam/opt/anaconda3/envs/py_venv_nmdsi/lib/python3.8/site-packages (from transformers) (21.3)\n","Requirement already satisfied: filelock in /Users/sajjadislam/opt/anaconda3/envs/py_venv_nmdsi/lib/python3.8/site-packages (from transformers) (3.6.0)\n","Requirement already satisfied: requests in /Users/sajjadislam/opt/anaconda3/envs/py_venv_nmdsi/lib/python3.8/site-packages (from transformers) (2.27.1)\n","Requirement already satisfied: tqdm>=4.27 in /Users/sajjadislam/opt/anaconda3/envs/py_venv_nmdsi/lib/python3.8/site-packages (from transformers) (4.64.0)\n","Requirement already satisfied: pyyaml>=5.1 in /Users/sajjadislam/opt/anaconda3/envs/py_venv_nmdsi/lib/python3.8/site-packages (from transformers) (6.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /Users/sajjadislam/opt/anaconda3/envs/py_venv_nmdsi/lib/python3.8/site-packages (from transformers) (0.5.1)\n","Requirement already satisfied: sacremoses in /Users/sajjadislam/opt/anaconda3/envs/py_venv_nmdsi/lib/python3.8/site-packages (from transformers) (0.0.49)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/sajjadislam/opt/anaconda3/envs/py_venv_nmdsi/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/sajjadislam/opt/anaconda3/envs/py_venv_nmdsi/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.4)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/sajjadislam/opt/anaconda3/envs/py_venv_nmdsi/lib/python3.8/site-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/sajjadislam/opt/anaconda3/envs/py_venv_nmdsi/lib/python3.8/site-packages (from requests->transformers) (1.26.9)\n","Requirement already satisfied: idna<4,>=2.5 in /Users/sajjadislam/opt/anaconda3/envs/py_venv_nmdsi/lib/python3.8/site-packages (from requests->transformers) (3.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /Users/sajjadislam/opt/anaconda3/envs/py_venv_nmdsi/lib/python3.8/site-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: six in /Users/sajjadislam/opt/anaconda3/envs/py_venv_nmdsi/lib/python3.8/site-packages (from sacremoses->transformers) (1.16.0)\n","Requirement already satisfied: click in /Users/sajjadislam/opt/anaconda3/envs/py_venv_nmdsi/lib/python3.8/site-packages (from sacremoses->transformers) (8.1.2)\n","Requirement already satisfied: joblib in /Users/sajjadislam/opt/anaconda3/envs/py_venv_nmdsi/lib/python3.8/site-packages (from sacremoses->transformers) (1.1.0)\n"]}],"source":["!pip install seqeval\n","!pip install transformers"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1650232545460,"user":{"displayName":"sajjad islam","userId":"16503418985020638021"},"user_tz":300},"id":"W7-j5WcdYRdj"},"outputs":[],"source":["import os\n","import pandas as pd\n","import math\n","import numpy as np\n","from tqdm import tqdm, trange\n","from seqeval.metrics import classification_report, accuracy_score, f1_score\n","import torch\n","import torch.nn.functional as F\n","from torch.optim import Adam\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from transformers import AutoModel, AutoConfig, AutoTokenizer\n","from transformers import AdamW\n","from transformers import AutoModelForSequenceClassification, BertForSequenceClassification"]},{"cell_type":"code","execution_count":90,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":284,"status":"ok","timestamp":1650232536704,"user":{"displayName":"sajjad islam","userId":"16503418985020638021"},"user_tz":300},"id":"StDym7jWhfJE","outputId":"ef118480-a0b1-4e36-b949-fcff03d19fc5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found GPU at: /device:GPU:0\n","There are 1 GPU(s) available.\n","Use the GPU: Tesla T4\n"]}],"source":["import tensorflow as tf\n","import torch\n","\n","# Get the GPU device name.\n","device_name = tf.test.gpu_device_name()\n","\n","# The device name should look like the following:\n","if device_name == '/device:GPU:0':\n","    print('Found GPU at: {}'.format(device_name))\n","else:\n","    print('GPU device not found')\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","    print('Use the GPU:', torch.cuda.get_device_name(0))\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12598,"status":"ok","timestamp":1650230251077,"user":{"displayName":"sajjad islam","userId":"16503418985020638021"},"user_tz":300},"id":"la3lM8PAfit4","outputId":"c2164a0e-8631-4612-ff8c-6dffd18d5050"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["# no of classifier: present, not-present\n","num_labels = 2\n","MODEL_CLASSES = {\n","  'bert': (AutoConfig, BertForSequenceClassification, AutoTokenizer),\n","}\n","MODEL_ADDRESS = 'emilyalsentzer/Bio_ClinicalBERT'\n","config_class, model_class, tokenizer_class = MODEL_CLASSES['bert']\n","model_config = config_class.from_pretrained(MODEL_ADDRESS, num_labels=num_labels)\n","tokenizer = tokenizer_class.from_pretrained(MODEL_ADDRESS, do_lower_case=False)\n","model = model_class.from_pretrained(MODEL_ADDRESS, config=model_config)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":284,"status":"ok","timestamp":1650230255958,"user":{"displayName":"sajjad islam","userId":"16503418985020638021"},"user_tz":300},"id":"IiQLq2MZglIk"},"outputs":[],"source":["def modify_label(label):\n","    if label == 'present':\n","        return int(0)\n","    elif label == 'not-present':\n","        return int(1)"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":379,"status":"ok","timestamp":1650230260281,"user":{"displayName":"sajjad islam","userId":"16503418985020638021"},"user_tz":300},"id":"1aeedeDsYqdz","outputId":"8948604c-c3be-44a6-a4ed-9d109a09f0a5"},"outputs":[{"name":"stdout","output_type":"stream","text":["The electrocardiogram showed normal sinus rhythm with [entity] old Q wave inferior myocardial infarction [entity] .\n","[0 0 0 1 1 1 1 0 0 0]\n"]},{"data":{"text/plain":["(6365, 3)"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["data_path_train_url = \"https://raw.githubusercontent.com/sajjadIslam2619/sample_files/main/processed/merged/assertion_label_modified_train.tsv\"\n","#data_path_train_url = 'https://raw.githubusercontent.com/sajjadIslam2619/sample_files/main/processed/merged/assertion_label_modified_train_small.tsv'\n","df_data_train = pd.read_csv(data_path_train_url, sep=\"\\t\").astype(str)\n","\n","df_data_train['label'] = df_data_train['label'].apply(modify_label)\n","df_data_train.sample(5)\n","\n","sentences_train = df_data_train.sentence.values\n","labels_train = df_data_train.label.values\n","print(sentences_train[0])\n","print(labels_train[:10])\n","df_data_train.shape"]},{"cell_type":"code","execution_count":89,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":319,"status":"ok","timestamp":1650232494950,"user":{"displayName":"sajjad islam","userId":"16503418985020638021"},"user_tz":300},"id":"5VA_1n-5hCPe","outputId":"cf5ddd07-5312-43e5-ff9a-feeeee6c2294"},"outputs":[{"name":"stdout","output_type":"stream","text":["Original:  The electrocardiogram showed normal sinus rhythm with [entity] old Q wave inferior myocardial infarction [entity] .\n","Tokenized:  ['The', 'electro', '##card', '##io', '##gram', 'showed', 'normal', 'sin', '##us', 'rhythm', 'with', '[', 'entity', ']', 'old', 'Q', 'wave', 'inferior', 'my', '##oc', '##ard', '##ial', 'in', '##far', '##ction', '[', 'entity', ']', '.']\n","Token IDs:  [1109, 24266, 10542, 2660, 12139, 2799, 2999, 11850, 1361, 6795, 1114, 164, 9127, 166, 1385, 154, 4003, 15543, 1139, 13335, 2881, 2916, 1107, 14794, 5796, 164, 9127, 166, 119]\n"]}],"source":["# Print the original sentence.\n","print('Original: ', sentences_train[0])\n","# Print the sentence split into tokens.\n","print('Tokenized: ', tokenizer.tokenize(sentences_train[0]))\n","# Print the sentence mapped to token ids.\n","print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences_train[0])))"]},{"cell_type":"code","execution_count":88,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3553,"status":"ok","timestamp":1650232480687,"user":{"displayName":"sajjad islam","userId":"16503418985020638021"},"user_tz":300},"id":"eWY23-J0mlIN","outputId":"08e99599-72e4-4103-b045-3175b7b34e23"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]}],"source":["input_ids = []\n","attention_masks = []\n","\n","for sent in sentences_train:\n","    encoded_dict = tokenizer.encode_plus(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = 128,           # Pad & truncate all sentences.\n","                        pad_to_max_length = True,\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","    \n","    input_ids.append(encoded_dict['input_ids'])\n","    \n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(labels_train)\n","\n","# print('Original: ', sentences_train[0])\n","# print('Token IDs:', input_ids[0])"]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":274,"status":"ok","timestamp":1650230275674,"user":{"displayName":"sajjad islam","userId":"16503418985020638021"},"user_tz":300},"id":"X_UHum0upLzO"},"outputs":[],"source":["train_dataset = TensorDataset(input_ids, attention_masks, labels)"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":308,"status":"ok","timestamp":1650230278561,"user":{"displayName":"sajjad islam","userId":"16503418985020638021"},"user_tz":300},"id":"vWNNpL0Yp6ZB","outputId":"88448995-b80d-4eef-fcd4-1cf7b121fed8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Exploratory laparotomy , left colectomy , end-transverse colostomy , drainage of [entity] pancreatic necrosis [entity] .\n","[0 1 0 0 1 0 0 0 0 0]\n"]},{"data":{"text/plain":["(10671, 3)"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["data_path_test_url = \"https://raw.githubusercontent.com/sajjadIslam2619/sample_files/main/processed/merged/assertion_label_modified_test.tsv\"\n","# data_path_test_url = 'https://raw.githubusercontent.com/sajjadIslam2619/sample_files/main/processed/merged/assertion_label_modified_test_small.tsv'\n","df_data_test = pd.read_csv(data_path_test_url, sep=\"\\t\").astype(str)\n","\n","df_data_test['label'] = df_data_test['label'].apply(modify_label)\n","df_data_test.sample(5)\n","\n","sentences_test = df_data_test.sentence.values\n","labels_test = df_data_test.label.values\n","print(sentences_test[0])\n","print(labels_test[:10])\n","df_data_test.shape"]},{"cell_type":"code","execution_count":87,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3824,"status":"ok","timestamp":1650232448601,"user":{"displayName":"sajjad islam","userId":"16503418985020638021"},"user_tz":300},"id":"3CNAd8L3qflq","outputId":"dd142cdf-3e83-469f-c422-4a149638ba61"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]}],"source":["input_ids = []\n","attention_masks = []\n","\n","# For every sentence...\n","for sent in sentences_test:\n","    encoded_dict = tokenizer.encode_plus(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = 128,           # Pad & truncate all sentences.\n","                        pad_to_max_length = True,\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","    \n","    input_ids.append(encoded_dict['input_ids'])\n","\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(labels_test)\n","\n","# print('Original: ', sentences_test[0])\n","# print('Token IDs:', input_ids[0])"]},{"cell_type":"code","execution_count":34,"metadata":{"executionInfo":{"elapsed":544,"status":"ok","timestamp":1650230289971,"user":{"displayName":"sajjad islam","userId":"16503418985020638021"},"user_tz":300},"id":"_kuNjX5Fq6aB"},"outputs":[],"source":["val_dataset = TensorDataset(input_ids, attention_masks, labels)"]},{"cell_type":"code","execution_count":35,"metadata":{"executionInfo":{"elapsed":1223,"status":"ok","timestamp":1650230293048,"user":{"displayName":"sajjad islam","userId":"16503418985020638021"},"user_tz":300},"id":"nlwiexEPrH0K"},"outputs":[],"source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","batch_size = 32 \n","train_dataloader = DataLoader(\n","            train_dataset,  # The training samples.\n","            sampler = RandomSampler(train_dataset), # Select batches randomly\n","            batch_size = batch_size # Trains with this batch size.\n","        )\n","\n","validation_dataloader = DataLoader(\n","            val_dataset, # The validation samples.\n","            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n","            batch_size = batch_size # Evaluate with this batch size.\n","        )"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1024,"status":"ok","timestamp":1650230296934,"user":{"displayName":"sajjad islam","userId":"16503418985020638021"},"user_tz":300},"id":"ElqkDc6OrQtJ","outputId":"84c61fd5-eff9-4ae8-e7d9-0ad1e6ffc875"},"outputs":[{"data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["# Tell pytorch to run this model on the GPU.\n","model.cuda()"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":314,"status":"ok","timestamp":1650230301036,"user":{"displayName":"sajjad islam","userId":"16503418985020638021"},"user_tz":300},"id":"fRdtF1nZrSy9","outputId":"4fb2bb9b-84d0-41d2-ec59-703bfc6ed7e2"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n"]}],"source":["optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n","                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n","                )"]},{"cell_type":"code","execution_count":38,"metadata":{"executionInfo":{"elapsed":985,"status":"ok","timestamp":1650230304799,"user":{"displayName":"sajjad islam","userId":"16503418985020638021"},"user_tz":300},"id":"KgCsv7Torq0O"},"outputs":[],"source":["from transformers import get_linear_schedule_with_warmup\n","epochs = 4\n","total_steps = len(train_dataloader) * epochs\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0, # Default value in run_glue.py\n","                                            num_training_steps = total_steps)"]},{"cell_type":"code","execution_count":39,"metadata":{"executionInfo":{"elapsed":271,"status":"ok","timestamp":1650230308350,"user":{"displayName":"sajjad islam","userId":"16503418985020638021"},"user_tz":300},"id":"gRr4WfK9r5Rv"},"outputs":[],"source":["import numpy as np\n","\n","# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"]},{"cell_type":"code","execution_count":86,"metadata":{"executionInfo":{"elapsed":333,"status":"ok","timestamp":1650232411697,"user":{"displayName":"sajjad islam","userId":"16503418985020638021"},"user_tz":300},"id":"_HMqXjNor9ye"},"outputs":[],"source":["import time\n","import datetime\n","\n","def format_time(elapsed):\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":332,"status":"ok","timestamp":1650232574719,"user":{"displayName":"sajjad islam","userId":"16503418985020638021"},"user_tz":300},"id":"4us3baqXty6i"},"outputs":[],"source":["# To calculate F1 score and accurecy and generate classification report.\n","y_true = []\n","y_pred = []\n","predictions , true_labels = [], []"]},{"cell_type":"markdown","metadata":{"id":"16uzF3zmhtT9"},"source":["**Model train and validation**"]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":856817,"status":"ok","timestamp":1650231174374,"user":{"displayName":"sajjad islam","userId":"16503418985020638021"},"user_tz":300},"id":"u9sTXc0CsMcE","outputId":"aa4caf7f-9ee2-4e30-fa1a-91c82ec25c20"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","======== Epoch 1 / 4 ========\n","Training...\n","  Batch    40  of    199.    Elapsed: 0:00:26.\n","  Batch    80  of    199.    Elapsed: 0:00:52.\n","  Batch   120  of    199.    Elapsed: 0:01:19.\n","  Batch   160  of    199.    Elapsed: 0:01:46.\n","\n","  Average training loss: 0.25\n","  Training epcoh took: 0:02:12\n","\n","Running Validation...\n","  Accuracy: 0.94\n","  Validation Loss: 0.17\n","  Validation took: 0:01:21\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","  Batch    40  of    199.    Elapsed: 0:00:27.\n","  Batch    80  of    199.    Elapsed: 0:00:54.\n","  Batch   120  of    199.    Elapsed: 0:01:20.\n","  Batch   160  of    199.    Elapsed: 0:01:47.\n","\n","  Average training loss: 0.12\n","  Training epcoh took: 0:02:13\n","\n","Running Validation...\n","  Accuracy: 0.95\n","  Validation Loss: 0.16\n","  Validation took: 0:01:21\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","  Batch    40  of    199.    Elapsed: 0:00:27.\n","  Batch    80  of    199.    Elapsed: 0:00:54.\n","  Batch   120  of    199.    Elapsed: 0:01:20.\n","  Batch   160  of    199.    Elapsed: 0:01:47.\n","\n","  Average training loss: 0.07\n","  Training epcoh took: 0:02:13\n","\n","Running Validation...\n","  Accuracy: 0.95\n","  Validation Loss: 0.17\n","  Validation took: 0:01:21\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","  Batch    40  of    199.    Elapsed: 0:00:27.\n","  Batch    80  of    199.    Elapsed: 0:00:54.\n","  Batch   120  of    199.    Elapsed: 0:01:21.\n","  Batch   160  of    199.    Elapsed: 0:01:47.\n","\n","  Average training loss: 0.04\n","  Training epcoh took: 0:02:14\n","\n","Running Validation...\n","  Accuracy: 0.95\n","  Validation Loss: 0.19\n","  Validation took: 0:01:21\n","\n","Training complete!\n","Total training took 0:14:17 (h:mm:ss)\n"]}],"source":["import random\n","import numpy as np\n","\n","seed_val = 42\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","training_stats = []\n","total_t0 = time.time()\n","\n","# For each epoch...\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    # Perform one full pass over the training set.\n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    t0 = time.time()\n","    total_train_loss = 0\n","\n","    model.train()\n","\n","    # For each batch of training data...\n","    for step, batch in enumerate(train_dataloader):\n","\n","        # Progress update every 40 batches.\n","        if step % 40 == 0 and not step == 0:\n","            elapsed = format_time(time.time() - t0)\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","        model.zero_grad()        \n","\n","        result = model(b_input_ids, \n","                       token_type_ids=None, \n","                       attention_mask=b_input_mask, \n","                       labels=b_labels,\n","                       return_dict=True)\n","\n","        loss = result.loss\n","        logits = result.logits\n","        total_train_loss += loss.item()\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        optimizer.step()\n","        scheduler.step()\n","\n","    avg_train_loss = total_train_loss / len(train_dataloader)            \n","\n","    training_time = format_time(time.time() - t0)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epcoh took: {:}\".format(training_time))\n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    # After the completion of each training epoch, measure our performance on\n","    # our validation set.\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","    model.eval()\n","    total_eval_accuracy = 0\n","    total_eval_loss = 0\n","    nb_eval_steps = 0\n","\n","    # Evaluate data for one epoch\n","    for batch in validation_dataloader:\n","\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","        with torch.no_grad():        \n","\n","            result = model(b_input_ids, \n","                           token_type_ids=None, \n","                           attention_mask=b_input_mask,\n","                           labels=b_labels,\n","                           return_dict=True)\n","\n","        loss = result.loss\n","        logits = result.logits\n","\n","        total_eval_loss += loss.item()\n","\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","\n","        predictions.append(logits)\n","        true_labels.append(label_ids)\n","\n","        total_eval_accuracy += flat_accuracy(logits, label_ids)\n","\n","    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n","    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n","\n","    avg_val_loss = total_eval_loss / len(validation_dataloader)\n","\n","    validation_time = format_time(time.time() - t0)\n","    \n","    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n","    print(\"  Validation took: {:}\".format(validation_time))\n","\n","    # Record all statistics from this epoch.\n","    training_stats.append(\n","        {\n","            'epoch': epoch_i + 1,\n","            'Training Loss': avg_train_loss,\n","            'Valid. Loss': avg_val_loss,\n","            'Valid. Accur.': avg_val_accuracy,\n","            'Training Time': training_time,\n","            'Validation Time': validation_time\n","        }\n","    )\n","\n","print(\"\")\n","print(\"Training complete!\")\n","\n","print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"]},{"cell_type":"code","execution_count":93,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":279,"status":"ok","timestamp":1650232732927,"user":{"displayName":"sajjad islam","userId":"16503418985020638021"},"user_tz":300},"id":"KNS-3Kplufj6","outputId":"71beebca-735e-498d-9282-c6e38a0f2b55"},"outputs":[{"name":"stdout","output_type":"stream","text":["f1 socre (validation phase): 0.000000\n","Accuracy score (validation phase): nan\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1580: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n","/usr/local/lib/python3.7/dist-packages/numpy/lib/function_base.py:380: RuntimeWarning: Mean of empty slice.\n","  avg = a.mean(axis)\n","/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n","  ret = ret.dtype.type(ret / rcount)\n"]}],"source":["import numpy as np\n","from sklearn.metrics import f1_score,accuracy_score, classification_report\n","\n","# For each input batch...\n","for i in range(len(true_labels)):\n","  pred_labels_i = np.argmax(predictions[i], axis=1).flatten() \n","  y_true.extend(true_labels[i])\n","  y_pred.extend(pred_labels_i)         \n","\n","print(\"f1 socre (validation phase): %f\"%(f1_score(y_true, y_pred)))\n","print(\"Accuracy score (validation phase): %f\"%(accuracy_score(y_true, y_pred)))"]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":358,"status":"ok","timestamp":1650231193188,"user":{"displayName":"sajjad islam","userId":"16503418985020638021"},"user_tz":300},"id":"MMczdJclvz2H","outputId":"8ba20353-31b8-45b7-ed5e-c1aa7c480b7c"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-cef63d1a-9794-4846-aa11-f9fa1ed9decb\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Training Loss</th>\n","      <th>Valid. Loss</th>\n","      <th>Valid. Accur.</th>\n","      <th>Training Time</th>\n","      <th>Validation Time</th>\n","    </tr>\n","    <tr>\n","      <th>epoch</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>0.25</td>\n","      <td>0.17</td>\n","      <td>0.94</td>\n","      <td>0:02:12</td>\n","      <td>0:01:21</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.12</td>\n","      <td>0.16</td>\n","      <td>0.95</td>\n","      <td>0:02:13</td>\n","      <td>0:01:21</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.07</td>\n","      <td>0.17</td>\n","      <td>0.95</td>\n","      <td>0:02:13</td>\n","      <td>0:01:21</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.04</td>\n","      <td>0.19</td>\n","      <td>0.95</td>\n","      <td>0:02:14</td>\n","      <td>0:01:21</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cef63d1a-9794-4846-aa11-f9fa1ed9decb')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-cef63d1a-9794-4846-aa11-f9fa1ed9decb button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-cef63d1a-9794-4846-aa11-f9fa1ed9decb');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n","epoch                                                                         \n","1               0.25         0.17           0.94       0:02:12         0:01:21\n","2               0.12         0.16           0.95       0:02:13         0:01:21\n","3               0.07         0.17           0.95       0:02:13         0:01:21\n","4               0.04         0.19           0.95       0:02:14         0:01:21"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","\n","pd.set_option('precision', 2)\n","df_stats = pd.DataFrame(data=training_stats)\n","df_stats = df_stats.set_index('epoch')\n","\n","# Display the table.\n","df_stats"]},{"cell_type":"code","execution_count":45,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":416,"status":"ok","timestamp":1650231201881,"user":{"displayName":"sajjad islam","userId":"16503418985020638021"},"user_tz":300},"id":"B4fxKWNvx1bT","outputId":"a3ec5c78-da6e-408a-fb1a-c88d71e74c7c"},"outputs":[{"name":"stdout","output_type":"stream","text":["There was [entity] a dense hemianopsia on the left side [entity] .\n","[0 1 0 0 0 0 0 1 0 0]\n"]},{"data":{"text/plain":["(708, 3)"]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["data_path_dev_url = \"https://raw.githubusercontent.com/sajjadIslam2619/sample_files/main/processed/merged/assertion_label_modified_dev.tsv\"\n","# data_path_dev_url = 'https://raw.githubusercontent.com/sajjadIslam2619/sample_files/main/processed/merged/assertion_label_modified_dev_small.tsv'\n","df_data_dev = pd.read_csv(data_path_dev_url, sep=\"\\t\").astype(str)\n","\n","df_data_dev['label'] = df_data_dev['label'].apply(modify_label)\n","df_data_dev.sample(5)\n","\n","sentences_dev = df_data_dev.sentence.values\n","labels_dev = df_data_dev.label.values\n","print(sentences_dev[0])\n","print(labels_dev[:10])\n","df_data_dev.shape"]},{"cell_type":"code","execution_count":85,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":603,"status":"ok","timestamp":1650232379837,"user":{"displayName":"sajjad islam","userId":"16503418985020638021"},"user_tz":300},"id":"mECbFlnSyvXT","outputId":"e307355f-970a-4ffe-f292-27cd06701f87"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]}],"source":["# Tokenize all of the sentences and map the tokens to thier word IDs.\n","input_ids = []\n","attention_masks = []\n","\n","# For every sentence...\n","for sent in sentences_dev:\n","    encoded_dict = tokenizer.encode_plus(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = 128,           # Pad & truncate all sentences.\n","                        pad_to_max_length = True,\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","      \n","    input_ids.append(encoded_dict['input_ids'])\n","\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","# Convert the lists into tensors.\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(labels_dev)\n","\n","# Print sentence 0, now as a list of IDs.\n","# print('Original: ', sentences_dev[0])\n","# print('Token IDs:', input_ids[0])"]},{"cell_type":"code","execution_count":47,"metadata":{"executionInfo":{"elapsed":434,"status":"ok","timestamp":1650231212601,"user":{"displayName":"sajjad islam","userId":"16503418985020638021"},"user_tz":300},"id":"5pHXmEOdzDTV"},"outputs":[],"source":["prediction_data = TensorDataset(input_ids, attention_masks, labels)\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"]},{"cell_type":"code","execution_count":48,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5605,"status":"ok","timestamp":1650231221046,"user":{"displayName":"sajjad islam","userId":"16503418985020638021"},"user_tz":300},"id":"VNKhBtyBz9fQ","outputId":"0ca8ff38-529f-4810-8fca-33b08a070462"},"outputs":[{"name":"stdout","output_type":"stream","text":["Predicting labels for 708 test sentences...\n","DONE.\n"]}],"source":["# Prediction on test set\n","\n","print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n","model.eval()\n","predictions , true_labels = [], []\n","\n","for batch in prediction_dataloader:\n","  batch = tuple(t.to(device) for t in batch)\n","  b_input_ids, b_input_mask, b_labels = batch\n","  with torch.no_grad():\n","      result = model(b_input_ids, \n","                     token_type_ids=None, \n","                     attention_mask=b_input_mask,\n","                     return_dict=True)\n","\n","  logits = result.logits\n","\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","\n","  predictions.append(logits)\n","  true_labels.append(label_ids)\n","\n","print('DONE.')"]},{"cell_type":"code","execution_count":49,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":671,"status":"ok","timestamp":1650231224301,"user":{"displayName":"sajjad islam","userId":"16503418985020638021"},"user_tz":300},"id":"iRynePqr0DQz","outputId":"591b3469-0825-4d8b-af02-d55d065dbcf4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Positive samples: 235 of 708 (33.19%)\n"]}],"source":["print('Positive samples: %d of %d (%.2f%%)' % (df_data_dev.label.sum(), len(df_data_dev.label), (df_data_dev.label.sum() / len(df_data_dev.label) * 100.0)))"]},{"cell_type":"code","execution_count":50,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":308,"status":"ok","timestamp":1650231231367,"user":{"displayName":"sajjad islam","userId":"16503418985020638021"},"user_tz":300},"id":"gFKo3ew-1Cax","outputId":"15cc4628-60ef-4fad-8c69-eb04ba042327"},"outputs":[{"name":"stdout","output_type":"stream","text":["Calculating Matthews Corr. Coef. for each batch...\n","f1 socre: 0.918700\n","Accuracy score: 0.949945\n","***** Eval results *****\n","\n","              precision    recall  f1-score   support\n","\n","           0     0.9540    0.9739    0.9638     29725\n","           1     0.9405    0.8979    0.9187     13667\n","\n","    accuracy                         0.9499     43392\n","   macro avg     0.9472    0.9359    0.9413     43392\n","weighted avg     0.9498    0.9499    0.9496     43392\n","\n"]}],"source":["from sklearn.metrics import matthews_corrcoef\n","import numpy as np\n","from sklearn.metrics import f1_score,accuracy_score, classification_report\n","\n","matthews_set = []\n","print('Calculating Matthews Corr. Coef. for each batch...')\n","\n","# For each input batch...\n","for i in range(len(true_labels)):\n","  pred_labels_i = np.argmax(predictions[i], axis=1).flatten() \n","  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)\n","  y_true.extend(true_labels[i])\n","  y_pred.extend(pred_labels_i)         \n","  matthews_set.append(matthews)\n","\n","\n","print(\"f1 socre: %f\"%(f1_score(y_true, y_pred)))\n","print(\"Accuracy score: %f\"%(accuracy_score(y_true, y_pred)))\n","report = classification_report(y_true, y_pred, digits=4)\n","print(\"***** Eval results *****\")\n","print(\"\\n%s\"%(report))"]},{"cell_type":"code","execution_count":51,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":351,"status":"ok","timestamp":1650231249292,"user":{"displayName":"sajjad islam","userId":"16503418985020638021"},"user_tz":300},"id":"KmfXNLLM1RVp","outputId":"693b2d79-b9eb-4d8f-fd67-c0007ec3c943"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total MCC: 0.883\n"]}],"source":["flat_predictions = np.concatenate(predictions, axis=0)\n","flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n","flat_true_labels = np.concatenate(true_labels, axis=0)\n","mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n","\n","print('Total MCC: %.3f' % mcc)"]},{"cell_type":"markdown","metadata":{"id":"djyuYuBpg6eM"},"source":["**Save model**"]},{"cell_type":"code","execution_count":52,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1706,"status":"ok","timestamp":1650231258714,"user":{"displayName":"sajjad islam","userId":"16503418985020638021"},"user_tz":300},"id":"MHOOpG1Ag-Uj","outputId":"f2d6de9f-3369-48ca-8bd6-13932a90870f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Saving model to ./model_save/\n"]},{"data":{"text/plain":["('./model_save/tokenizer_config.json',\n"," './model_save/special_tokens_map.json',\n"," './model_save/vocab.txt',\n"," './model_save/added_tokens.json',\n"," './model_save/tokenizer.json')"]},"execution_count":52,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","\n","# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n","\n","output_dir = './trained_model/2_label_model'\n","\n","# Create output directory if needed\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)\n","\n","print(\"Saving model to %s\" % output_dir)\n","\n","# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n","# They can then be reloaded using `from_pretrained()`\n","model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n","model_to_save.save_pretrained(output_dir)\n","tokenizer.save_pretrained(output_dir)\n","\n","# Good practice: save your training arguments together with the trained model\n","# torch.save(args, os.path.join(output_dir, 'training_args.bin'))"]},{"cell_type":"code","execution_count":53,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":868,"status":"ok","timestamp":1650231263177,"user":{"displayName":"sajjad islam","userId":"16503418985020638021"},"user_tz":300},"id":"-yx3ELR8hJ7b","outputId":"90fb2883-8e22-4718-db7e-3484c5f0efc7"},"outputs":[{"name":"stdout","output_type":"stream","text":["total 424048K\n","-rw-r--r-- 1 root root      1K Apr 17 21:34 config.json\n","-rw-r--r-- 1 root root 423163K Apr 17 21:34 pytorch_model.bin\n","-rw-r--r-- 1 root root      1K Apr 17 21:34 special_tokens_map.json\n","-rw-r--r-- 1 root root      1K Apr 17 21:34 tokenizer_config.json\n","-rw-r--r-- 1 root root    654K Apr 17 21:34 tokenizer.json\n","-rw-r--r-- 1 root root    209K Apr 17 21:34 vocab.txt\n"]}],"source":["!ls -l --block-size=K ./model_save/"]},{"cell_type":"code","execution_count":54,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":313,"status":"ok","timestamp":1650231266497,"user":{"displayName":"sajjad islam","userId":"16503418985020638021"},"user_tz":300},"id":"56PKHGJ9kT6T","outputId":"ef8ddfde-b309-44f1-9882-4f50e6496a68"},"outputs":[{"name":"stdout","output_type":"stream","text":["-rw-r--r-- 1 root root 414M Apr 17 21:34 ./model_save/pytorch_model.bin\n"]}],"source":["!ls -l --block-size=M ./model_save/pytorch_model.bin"]},{"cell_type":"code","execution_count":55,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24273,"status":"ok","timestamp":1650231294184,"user":{"displayName":"sajjad islam","userId":"16503418985020638021"},"user_tz":300},"id":"G2kK4MFXkbVf","outputId":"df83bc2e-a6e2-4da9-eed0-1910e10e82f9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["# Mount Google Drive to this Notebook instance.\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":56,"metadata":{"executionInfo":{"elapsed":2686,"status":"ok","timestamp":1650231299182,"user":{"displayName":"sajjad islam","userId":"16503418985020638021"},"user_tz":300},"id":"NDROp7cLk6fJ"},"outputs":[],"source":["# drive directory\n","!cp -r ./model_save/ \"./drive/My Drive/MU/NMDSI/ast_model_save/\""]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":240},"executionInfo":{"elapsed":475,"status":"error","timestamp":1650232892722,"user":{"displayName":"sajjad islam","userId":"16503418985020638021"},"user_tz":300},"id":"Jtj5KcloltQT","outputId":"9c7ae962-ffdd-470f-af70-f8fabed969f9"},"outputs":[],"source":["output_dir = './trained_model/2_label_model'\n","# Load a trained model and vocabulary that you have fine-tuned\n","model = model_class.from_pretrained(output_dir)\n","tokenizer = tokenizer_class.from_pretrained(output_dir)\n","\n","# Copy the model to the GPU.\n","# model.to(device)"]},{"cell_type":"markdown","metadata":{"id":"0t1dqY1BglAQ"},"source":["**Predict with model**"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":291,"status":"ok","timestamp":1650231814561,"user":{"displayName":"sajjad islam","userId":"16503418985020638021"},"user_tz":300},"id":"SeW-GTxjMoVF","outputId":"20a9eec9-0d18-4391-cf5c-474fb173456f"},"outputs":[{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/Users/sajjadislam/opt/anaconda3/envs/py_venv_nmdsi/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2263: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]}],"source":["sentence = 'Patient has [entity] fever [entity].'\n","sentences = []\n","sentences.append(sentence)\n","input_ids = []\n","attention_masks = []\n","\n","# For every sentence...\n","for sent in sentences:\n","    encoded_dict = tokenizer.encode_plus(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = 128,           # Pad & truncate all sentences.\n","                        pad_to_max_length = True,\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","      \n","    input_ids.append(encoded_dict['input_ids'])\n","\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","# Convert the lists into tensors.\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","# labels = torch.tensor(labels)\n"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":333,"status":"ok","timestamp":1650231820004,"user":{"displayName":"sajjad islam","userId":"16503418985020638021"},"user_tz":300},"id":"DD5KJwxWULSS","outputId":"a9656fd7-251b-4d66-db5a-a1d64f7c5f61"},"outputs":[{"name":"stderr","output_type":"stream","text":["/var/folders/p3/s4n39zb50rb2l96w9n054ch80000gn/T/ipykernel_3977/2464600030.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_ids = torch.tensor(input_ids)\n","/var/folders/p3/s4n39zb50rb2l96w9n054ch80000gn/T/ipykernel_3977/2464600030.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  attention_masks = torch.tensor(attention_masks)\n"]}],"source":["input_ids = torch.tensor(input_ids)\n","attention_masks = torch.tensor(attention_masks)"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"4MEG12tsUhAv"},"outputs":[{"name":"stdout","output_type":"stream","text":["sentence:  Patient has [entity] fever [entity].\n","Label prediction:  [0]\n","Present\n"]}],"source":["model.eval()\n","\n","with torch.no_grad():\n","    result = model(input_ids, token_type_ids=None, attention_mask=attention_masks, return_dict=True)\n","\n","logits = result.logits\n","logits = logits.detach().cpu().numpy()\n","predictions.append(logits)\n","\n","print('sentence: ', sentence)\n","pred_labels_i = np.argmax(logits, axis=1).flatten()\n","print('Label prediction: ', pred_labels_i) \n","\n","if pred_labels_i[0] == 0:\n","  print ('Present')\n","elif pred_labels_i[0] == 1:\n","  print ('Not-present')\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOm5EQlIC4X4+jo4CBS0FNF","collapsed_sections":[],"name":"ast_model.ipynb","provenance":[]},"interpreter":{"hash":"33fa62565346c5b00f7c01ac7dab8740690287f5b700f97087b1b874bc66ae30"},"kernelspec":{"display_name":"Python 3.8.13 ('py_venv_nmdsi')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"}},"nbformat":4,"nbformat_minor":0}
