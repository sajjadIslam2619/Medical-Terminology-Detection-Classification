{"cells":[{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6106,"status":"ok","timestamp":1650230226851,"user":{"displayName":"sajjad islam","userId":"16503418985020638021"},"user_tz":300},"id":"rhikf-Vjezzh","outputId":"a7aa73e6-7a84-4bb9-e53d-ddac7926f79d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: seqeval in /Users/sajjadislam/opt/anaconda3/envs/py_venv_nmdsi/lib/python3.8/site-packages (1.2.2)\n","Requirement already satisfied: numpy>=1.14.0 in /Users/sajjadislam/opt/anaconda3/envs/py_venv_nmdsi/lib/python3.8/site-packages (from seqeval) (1.18.5)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /Users/sajjadislam/opt/anaconda3/envs/py_venv_nmdsi/lib/python3.8/site-packages (from seqeval) (1.0.2)\n","Requirement already satisfied: joblib>=0.11 in /Users/sajjadislam/opt/anaconda3/envs/py_venv_nmdsi/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval) (1.1.0)\n","Requirement already satisfied: scipy>=1.1.0 in /Users/sajjadislam/opt/anaconda3/envs/py_venv_nmdsi/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval) (1.8.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/sajjadislam/opt/anaconda3/envs/py_venv_nmdsi/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n","Requirement already satisfied: transformers in /Users/sajjadislam/opt/anaconda3/envs/py_venv_nmdsi/lib/python3.8/site-packages (4.18.0)\n","Requirement already satisfied: tqdm>=4.27 in /Users/sajjadislam/opt/anaconda3/envs/py_venv_nmdsi/lib/python3.8/site-packages (from transformers) (4.64.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /Users/sajjadislam/opt/anaconda3/envs/py_venv_nmdsi/lib/python3.8/site-packages (from transformers) (0.5.1)\n","Requirement already satisfied: numpy>=1.17 in /Users/sajjadislam/opt/anaconda3/envs/py_venv_nmdsi/lib/python3.8/site-packages (from transformers) (1.18.5)\n","Requirement already satisfied: filelock in /Users/sajjadislam/opt/anaconda3/envs/py_venv_nmdsi/lib/python3.8/site-packages (from transformers) (3.6.0)\n","Requirement already satisfied: packaging>=20.0 in /Users/sajjadislam/opt/anaconda3/envs/py_venv_nmdsi/lib/python3.8/site-packages (from transformers) (21.3)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /Users/sajjadislam/opt/anaconda3/envs/py_venv_nmdsi/lib/python3.8/site-packages (from transformers) (0.12.1)\n","Requirement already satisfied: sacremoses in /Users/sajjadislam/opt/anaconda3/envs/py_venv_nmdsi/lib/python3.8/site-packages (from transformers) (0.0.49)\n","Requirement already satisfied: pyyaml>=5.1 in /Users/sajjadislam/opt/anaconda3/envs/py_venv_nmdsi/lib/python3.8/site-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /Users/sajjadislam/opt/anaconda3/envs/py_venv_nmdsi/lib/python3.8/site-packages (from transformers) (2022.3.15)\n","Requirement already satisfied: requests in /Users/sajjadislam/opt/anaconda3/envs/py_venv_nmdsi/lib/python3.8/site-packages (from transformers) (2.27.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/sajjadislam/opt/anaconda3/envs/py_venv_nmdsi/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/sajjadislam/opt/anaconda3/envs/py_venv_nmdsi/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /Users/sajjadislam/opt/anaconda3/envs/py_venv_nmdsi/lib/python3.8/site-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/sajjadislam/opt/anaconda3/envs/py_venv_nmdsi/lib/python3.8/site-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/sajjadislam/opt/anaconda3/envs/py_venv_nmdsi/lib/python3.8/site-packages (from requests->transformers) (1.26.9)\n","Requirement already satisfied: idna<4,>=2.5 in /Users/sajjadislam/opt/anaconda3/envs/py_venv_nmdsi/lib/python3.8/site-packages (from requests->transformers) (3.3)\n","Requirement already satisfied: joblib in /Users/sajjadislam/opt/anaconda3/envs/py_venv_nmdsi/lib/python3.8/site-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: click in /Users/sajjadislam/opt/anaconda3/envs/py_venv_nmdsi/lib/python3.8/site-packages (from sacremoses->transformers) (8.1.2)\n","Requirement already satisfied: six in /Users/sajjadislam/opt/anaconda3/envs/py_venv_nmdsi/lib/python3.8/site-packages (from sacremoses->transformers) (1.16.0)\n"]}],"source":["!pip install seqeval\n","!pip install transformers"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1650232545460,"user":{"displayName":"sajjad islam","userId":"16503418985020638021"},"user_tz":300},"id":"W7-j5WcdYRdj"},"outputs":[],"source":["import os\n","import pandas as pd\n","import math\n","import numpy as np\n","from tqdm import tqdm, trange\n","from seqeval.metrics import classification_report, accuracy_score, f1_score\n","import torch\n","import torch.nn.functional as F\n","from torch.optim import Adam\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from transformers import AutoModel, AutoConfig, AutoTokenizer\n","from transformers import AdamW\n","from transformers import AutoModelForSequenceClassification, BertForSequenceClassification"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12598,"status":"ok","timestamp":1650230251077,"user":{"displayName":"sajjad islam","userId":"16503418985020638021"},"user_tz":300},"id":"la3lM8PAfit4","outputId":"c2164a0e-8631-4612-ff8c-6dffd18d5050"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["# no of classifier: present, not-present\n","num_labels = 3\n","MODEL_CLASSES = {\n","  'bert': (AutoConfig, BertForSequenceClassification, AutoTokenizer),\n","}\n","MODEL_ADDRESS = 'emilyalsentzer/Bio_ClinicalBERT'\n","config_class, model_class, tokenizer_class = MODEL_CLASSES['bert']\n","model_config = config_class.from_pretrained(MODEL_ADDRESS, num_labels=num_labels)\n","tokenizer = tokenizer_class.from_pretrained(MODEL_ADDRESS, do_lower_case=False)\n","model = model_class.from_pretrained(MODEL_ADDRESS, config=model_config)"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":284,"status":"ok","timestamp":1650230255958,"user":{"displayName":"sajjad islam","userId":"16503418985020638021"},"user_tz":300},"id":"IiQLq2MZglIk"},"outputs":[],"source":["def modify_label(label):\n","    if label == 'not-present':\n","        return 1\n","    elif label == 'present':\n","        return 0"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":333,"status":"ok","timestamp":1650232411697,"user":{"displayName":"sajjad islam","userId":"16503418985020638021"},"user_tz":300},"id":"_HMqXjNor9ye"},"outputs":[],"source":["import time\n","import datetime\n","\n","def format_time(elapsed):\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":240},"executionInfo":{"elapsed":475,"status":"error","timestamp":1650232892722,"user":{"displayName":"sajjad islam","userId":"16503418985020638021"},"user_tz":300},"id":"Jtj5KcloltQT","outputId":"9c7ae962-ffdd-470f-af70-f8fabed969f9"},"outputs":[],"source":["output_dir = './trained_models/Assertion/3_lable_model'\n","model = model_class.from_pretrained(output_dir)\n","tokenizer = tokenizer_class.from_pretrained(output_dir)\n","\n","# Copy the model to the GPU.\n","# model.to(device)"]},{"cell_type":"markdown","metadata":{"id":"0t1dqY1BglAQ"},"source":["**Predict with model**"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":291,"status":"ok","timestamp":1650231814561,"user":{"displayName":"sajjad islam","userId":"16503418985020638021"},"user_tz":300},"id":"SeW-GTxjMoVF","outputId":"20a9eec9-0d18-4391-cf5c-474fb173456f"},"outputs":[{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/Users/sajjadislam/opt/anaconda3/envs/py_venv_nmdsi/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2263: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]}],"source":["sentence1 = 'Patient has [entity] fever [entity].'\n","sentence2 = 'Patient denies [entity] fever [entity].'\n","sentences = [sentence1, sentence2]\n","input_ids = []\n","attention_masks = []\n","\n","# For every sentence...\n","for sent in sentences:\n","    encoded_dict = tokenizer.encode_plus(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = 128,           # Pad & truncate all sentences.\n","                        pad_to_max_length = True,\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","\n","    input_ids.append(encoded_dict['input_ids'])\n","\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","# Convert the lists into tensors.\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","# labels = torch.tensor(labels)\n"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":333,"status":"ok","timestamp":1650231820004,"user":{"displayName":"sajjad islam","userId":"16503418985020638021"},"user_tz":300},"id":"DD5KJwxWULSS","outputId":"a9656fd7-251b-4d66-db5a-a1d64f7c5f61"},"outputs":[{"name":"stderr","output_type":"stream","text":["/var/folders/p3/s4n39zb50rb2l96w9n054ch80000gn/T/ipykernel_2780/2464600030.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_ids = torch.tensor(input_ids)\n","/var/folders/p3/s4n39zb50rb2l96w9n054ch80000gn/T/ipykernel_2780/2464600030.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  attention_masks = torch.tensor(attention_masks)\n"]}],"source":["input_ids = torch.tensor(input_ids)\n","attention_masks = torch.tensor(attention_masks)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["predictions , true_labels = [], []"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"4MEG12tsUhAv"},"outputs":[{"name":"stdout","output_type":"stream","text":["Patient has [entity] fever [entity].\n","0\n","Present\n","Patient denies [entity] fever [entity].\n","2\n","Present\n"]}],"source":["model.eval()\n","\n","with torch.no_grad():\n","    result = model(input_ids, token_type_ids=None,\n","                   attention_mask=attention_masks, return_dict=True)\n","\n","logits = result.logits\n","logits = logits.detach().cpu().numpy()\n","predictions.append(logits)\n","\n","# print('sentences: ', sentences)\n","pred_labels_i = np.argmax(logits, axis=1).flatten()\n","# print('Label prediction: ', pred_labels_i)\n","\n","for index, sentence in enumerate(sentences):\n","    print(sentence)\n","    print(pred_labels_i[index])\n","    if pred_labels_i[index] == 0:\n","        print('Present')\n","    elif pred_labels_i[index] == 1:\n","        print('Possible')\n","    elif pred_labels_i[index] == 2:\n","        print('Not-present')\n"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOm5EQlIC4X4+jo4CBS0FNF","collapsed_sections":[],"name":"ast_model.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"}},"nbformat":4,"nbformat_minor":0}
