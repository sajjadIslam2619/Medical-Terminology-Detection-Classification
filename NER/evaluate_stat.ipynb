{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\Data\\processed\\llm\\entity_only\\test\\0001.txt\n",
      "..\\Results\\test\\DeepSeek\\0001_sentence_level.txt\n",
      "..\\Results\\test\\DeepSeek\\0001_sentence_level_stat_2.txt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "# Define paths\n",
    "input_dir = Path(\"../Data/processed/llm/entity_only/test/\")\n",
    "input_file_name = \"0001.txt\"\n",
    "actual_file = input_dir / input_file_name\n",
    "print(actual_file)\n",
    "\n",
    "model_name = \"DeepSeek\"\n",
    "predicted_dir = Path(f\"../Results/test/{model_name}/\")\n",
    "predicted_filename = \"0001_sentence_level.txt\"\n",
    "predicted_file = predicted_dir / predicted_filename\n",
    "print(predicted_file)\n",
    "\n",
    "output_filename = \"0001_sentence_level_stat.txt\"\n",
    "output_file = predicted_dir/output_filename\n",
    "print(output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read and parse data from a file\n",
    "def read_data(file_path):\n",
    "    data = {}\n",
    "    with open(file_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            if 'entity=' in line and 'label=' in line:\n",
    "                parts = line.strip().split('\" ')\n",
    "                entity = parts[0].split('=\"')[1]  # Extract entity\n",
    "                label = parts[1].split('=\"')[1]  # Extract label\n",
    "                data[entity] = label\n",
    "    return data\n",
    "# Read actual and predicted data\n",
    "actual_data = read_data(actual_file)\n",
    "predicted_data = read_data(predicted_file)\n",
    "\n",
    "# Clean all labels in advance\n",
    "actual_data = {k: v.strip().strip('\"') for k, v in actual_data.items()}\n",
    "predicted_data = {k: v.strip().strip('\"') for k, v in predicted_data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nipua\\AppData\\Local\\anaconda3\\envs\\ner\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load ClinicalBERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Read data ---\n",
    "def read_data(file_path):\n",
    "    data = {}\n",
    "    with open(file_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            if 'entity=' in line and 'label=' in line:\n",
    "                parts = line.strip().split('\" ')\n",
    "                entity = parts[0].split('=\"')[1]\n",
    "                label = parts[1].split('=\"')[1]\n",
    "                data[entity] = label\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: Get BERT embedding ---\n",
    "def get_entity_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    cls_embedding = outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "    return cls_embedding.squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "actual_label_list = []\n",
    "predicted_label_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_extraction_and_classification(actual_data, predicted_data, threshold):\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "    matched_data = []\n",
    "    label_mismatches = []\n",
    "    unmatched_predictions = []\n",
    "    unknown_label_count = 0\n",
    "\n",
    "    # Precompute actual embeddings\n",
    "    actual_embeddings = {ent: get_entity_embedding(ent) for ent in actual_data}\n",
    "    matched_actual_entities = set()\n",
    "\n",
    "    for pred_ent, pred_label in predicted_data.items():\n",
    "        pred_label_clean = pred_label.strip().lower()\n",
    "        if pred_label_clean == \"unknown\":\n",
    "            unknown_label_count += 1\n",
    "            continue\n",
    "\n",
    "        pred_emb = get_entity_embedding(pred_ent)\n",
    "        best_sim = 0\n",
    "        best_match = None\n",
    "\n",
    "        for actual_ent in actual_data:\n",
    "            if actual_ent in matched_actual_entities:\n",
    "                continue\n",
    "            sim = cosine_similarity([pred_emb], [actual_embeddings[actual_ent]])[0][0]\n",
    "            if sim > threshold and sim > best_sim:\n",
    "                best_sim = sim\n",
    "                best_match = actual_ent\n",
    "\n",
    "        if best_match:\n",
    "            matched_actual_entities.add(best_match)\n",
    "            actual_label_clean = actual_data[best_match].strip().lower()\n",
    "            matched_data.append({\n",
    "                \"predicted_entity\": pred_ent,\n",
    "                \"predicted_label\": pred_label_clean,\n",
    "                \"actual_entity\": best_match,\n",
    "                \"actual_label\": actual_label_clean\n",
    "            })\n",
    "\n",
    "            if pred_label_clean != actual_label_clean:\n",
    "                label_mismatches.append({\n",
    "                    \"predicted_entity\": pred_ent,\n",
    "                    \"predicted_label\": pred_label_clean,\n",
    "                    \"matched_actual_entity\": best_match,\n",
    "                    \"actual_label\": actual_label_clean\n",
    "                })\n",
    "        else:\n",
    "            unmatched_predictions.append(pred_ent)\n",
    "\n",
    "    # Calculate based only on the 70 predictions\n",
    "    total_predictions = len(predicted_data) - unknown_label_count\n",
    "    matched = len(matched_data)\n",
    "    misclassified = len(label_mismatches)\n",
    "    unmatched = len(unmatched_predictions)\n",
    "    tp = matched - misclassified\n",
    "    fp = misclassified + unmatched\n",
    "\n",
    "    # Label metrics (only on predictions)\n",
    "    precision = tp / (tp + fp) if (tp + fp) else 0\n",
    "    recall = tp / matched if matched else 0\n",
    "    accuracy = tp / total_predictions if total_predictions else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0\n",
    "\n",
    "    return {\n",
    "        \"Total Entities in Actual\": len(actual_data),\n",
    "        \"Total Entities in Prediction\": len(predicted_data),\n",
    "        \"Total Unknown Entities in Prediction\": unknown_label_count,\n",
    "        \"Total Matched Entities\": matched,\n",
    "        \"Total Unmatched Predictions\": unmatched,\n",
    "        \"Extraction Accuracy\": round(matched / len(actual_data), 4) if len(actual_data) else 0,\n",
    "\n",
    "        # Label classification metrics based only on predictions\n",
    "        \"Label Accuracy (on predictions)\": round(accuracy, 4),\n",
    "        \"Label Precision\": round(precision, 4),\n",
    "        \"Label Recall (on matched)\": round(recall, 4),\n",
    "        \"Label F1 Score\": round(f1, 4),\n",
    "\n",
    "        \"Label Mismatches\": label_mismatches\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation\n",
    "metrics = evaluate_extraction_and_classification(actual_data, predicted_data, threshold=0.92)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Entities in Actual: 190\n",
      "Total Entities in Prediction: 106\n",
      "Total Unknown Entities in Prediction: 0\n",
      "Total Matched Entities: 99\n",
      "Total Unmatched Predictions: 7\n",
      "Extraction Accuracy: 0.5211\n",
      "Label Accuracy (on predictions): 0.8962\n",
      "Label Precision: 0.8962\n",
      "Label Recall (Sensitivity): 0.9596\n",
      "Label F1 Score: 0.9268\n",
      "Label Specificity: 0.0000\n",
      "Label Mismatches:\n",
      "[\n",
      "    {\n",
      "        \"predicted_entity\": \"HD\",\n",
      "        \"predicted_label\": \"treatment\",\n",
      "        \"matched_actual_entity\": \"dvt\",\n",
      "        \"actual_label\": \"problem\"\n",
      "    },\n",
      "    {\n",
      "        \"predicted_entity\": \"ETOH/IVDA\",\n",
      "        \"predicted_label\": \"problem\",\n",
      "        \"matched_actual_entity\": \"spironolactone/hctz\",\n",
      "        \"actual_label\": \"treatment\"\n",
      "    },\n",
      "    {\n",
      "        \"predicted_entity\": \"tob\",\n",
      "        \"predicted_label\": \"problem\",\n",
      "        \"matched_actual_entity\": \"actos\",\n",
      "        \"actual_label\": \"treatment\"\n",
      "    },\n",
      "    {\n",
      "        \"predicted_entity\": \"Sertraline\",\n",
      "        \"predicted_label\": \"treatment\",\n",
      "        \"matched_actual_entity\": \"nitrite\",\n",
      "        \"actual_label\": \"test\"\n",
      "    }\n",
      "]\n",
      "Results saved to ..\\Results\\test\\DeepSeek\\0001_sentence_level_stat_2.txt\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    for k, v in metrics.items():\n",
    "        if k == \"Label Mismatches\":\n",
    "            f.write(\"Label Mismatches:\\n\")\n",
    "            json_output = json.dumps(v, indent=4)\n",
    "            print(\"Label Mismatches:\")\n",
    "            print(json_output)\n",
    "            f.write(json_output + \"\\n\")\n",
    "        else:\n",
    "            line = f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\"\n",
    "            print(line)\n",
    "            f.write(line + \"\\n\")\n",
    "print(f\"Results saved to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
