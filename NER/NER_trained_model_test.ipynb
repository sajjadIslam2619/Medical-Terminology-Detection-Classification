{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6173dfb-ada8-4b93-9f1d-648f2ef1daa9",
   "metadata": {},
   "source": [
    "# Load 'NER Model' from directory and evaluate the model. \n",
    "# This model takes imput as string.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e6acb23-cdb2-4c35-bd8a-dfd810681a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from transformers import BertForTokenClassification, BertTokenizer\n",
    "from transformers import AutoConfig, AutoTokenizer, BertForSequenceClassification\n",
    "import stanza\n",
    "import torch\n",
    "from scipy.special import softmax\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f2654d5-eda9-47db-a95a-1750c1339745",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2idx={'B-problem': 0,\n",
    " 'B-test': 1,\n",
    " 'B-treatment': 2,\n",
    " 'I-problem': 3,\n",
    " 'I-test': 4,\n",
    " 'I-treatment': 5,\n",
    " 'O': 6,\n",
    " 'X': 7,\n",
    " '[CLS]': 8,\n",
    " '[SEP]': 9\n",
    " }\n",
    "# Mapping index to name\n",
    "tag2name = {tag2idx[key]: key for key in tag2idx}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dc18699-9376-4b10-9792-0dc3438afe73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ner_model():\n",
    "    num_labels = len(tag2idx)\n",
    "    save_model_address = '../trained_models/NER/C-Bert-test'\n",
    "    model = BertForTokenClassification.from_pretrained(\n",
    "        save_model_address, num_labels=num_labels)\n",
    "    tokenizer = BertTokenizer.from_pretrained(\n",
    "        save_model_address, do_lower_case=False)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b1c70bd-4be9-4bef-a72d-5a22250ba1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ner, tokenizer_ner = load_ner_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4a946c5-09b6-4d8d-be55-17dd63b4d409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e7a70acec824f09ada5c18b6dbc3a3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json:   0%|   â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-06 00:33:55 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "========================\n",
      "\n",
      "2022-08-06 00:33:55 INFO: Use device: cpu\n",
      "2022-08-06 00:33:55 INFO: Loading: tokenize\n",
      "2022-08-06 00:33:55 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    nlp = stanza.Pipeline(lang=\"en\", processors=\"tokenize\")\n",
    "except Exception:\n",
    "    stanza.download(\"en\")\n",
    "    nlp = stanza.Pipeline(lang=\"en\", processors=\"tokenize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d402bbd-bf67-4b1d-8255-4b302196260e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "723205d8-6f8d-4f3e-a3de-0ff0840c077e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_query(sentence, tokenizer):\n",
    "\n",
    "    temp_token = ['[CLS]']\n",
    "    # word_list = [token.text for token in sentence.tokens]\n",
    "    for word in sentence:\n",
    "        temp_token.extend(tokenizer.tokenize(word))\n",
    "    temp_token = temp_token[:128 - 1]\n",
    "    temp_token.append('[SEP]')\n",
    "    input_id = tokenizer.convert_tokens_to_ids(temp_token)\n",
    "    padding_len = MAX_LEN - len(input_id)\n",
    "    input_id = input_id + ([0] * padding_len)\n",
    "    tokenized_texts = [input_id]\n",
    "    attention_masks = [[int(i>0) for i in input_id]]\n",
    "\n",
    "    return temp_token, torch.tensor(tokenized_texts), torch.tensor(attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4124a50-11f5-45c0-964e-ea70559f209e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inference(model, input_ids):\n",
    "    #model.to(device)\n",
    "    #input_ids = input_ids.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, token_type_ids=None,\n",
    "                        attention_mask=None)\n",
    "        # For eval mode, the first result of outputs is logits\n",
    "        logits = outputs[0]\n",
    "    # Get NER predict result\n",
    "    predict_results = logits.detach().cpu().numpy()\n",
    "    result_arrays_soft = softmax(predict_results[0])\n",
    "\n",
    "    return np.argmax(result_arrays_soft, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6726fd0-4977-44df-822f-8eb20703b357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_entities(long_text):\n",
    "    all_sentences = []\n",
    "    all_tags = []\n",
    "    doc = nlp(long_text)\n",
    "    for i, sentence in enumerate(doc.sentences):\n",
    "        # temp_token: tokenized words\n",
    "        # input_ids: convert temp_token to id\n",
    "        word_list = [token.text for token in sentence.tokens]\n",
    "        temp_token, input_ids, attention_masks = create_query(word_list, tokenizer_ner)\n",
    "        result_list = model_inference(model_ner, input_ids)\n",
    "        result = [tag2name[t] for t in result_list]\n",
    "        pretok_sent = \"\"\n",
    "        pretags = \"\"\n",
    "        for i, tok in enumerate(temp_token):\n",
    "            if tok.startswith(\"##\"):\n",
    "                pretok_sent += tok[2:]\n",
    "            else:\n",
    "                pretok_sent += f\" {tok}\"\n",
    "                pretags += f\" {result[i]}\"\n",
    "        pretok_sent = pretok_sent[1:]\n",
    "        pretags = pretags[1:]\n",
    "        s = pretok_sent.split()\n",
    "        t = pretags.split()\n",
    "        all_sentences.append(s)\n",
    "        all_tags.append(t)\n",
    "    return all_sentences, all_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2856c2a-65ed-4424-88a9-0b5f8113da55",
   "metadata": {},
   "source": [
    "# Insert sentences in string format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4f6c4be-6c30-4fdb-9598-a5c8e3eb1bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"He had no cardiac murmur .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ae374fb-7c59-445b-b27d-f53d26e6195f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Significant for chronic atrial fibrillation , managed with an Italian variation of Digoxin , and Amioadrone , hypertension managed with an ACE inhibitor , chronic obstructive pulmonary disease managed with a zanthine preparation and low dose steroids , as well as an occasional inhaler\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e943310-d742-467b-91b9-4b037055aab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['[CLS]', 'Significant', 'for', 'chronic', 'atrial', 'fibrillation', ',', 'managed', 'with', 'an', 'Italian', 'variation', 'of', 'Digoxin', ',', 'and', 'Amioadrone', ',', 'hypertension', 'managed', 'with', 'an', 'ACE', 'inhibitor', ',', 'chronic', 'obstructive', 'pulmonary', 'disease', 'managed', 'with', 'a', 'zanthine', 'preparation', 'and', 'low', 'dose', 'steroids', ',', 'as', 'well', 'as', 'an', 'occasional', 'inhaler', '[SEP]']]\n",
      "[['O', 'O', 'O', 'B-problem', 'I-problem', 'I-problem', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-treatment', 'O', 'O', 'B-treatment', 'O', 'B-problem', 'O', 'O', 'B-treatment', 'I-treatment', 'I-treatment', 'O', 'B-problem', 'I-problem', 'I-problem', 'I-problem', 'O', 'O', 'B-treatment', 'I-treatment', 'O', 'O', 'B-treatment', 'I-treatment', 'I-treatment', 'O', 'O', 'O', 'O', 'B-treatment', 'I-problem', 'I-treatment', '[SEP]']]\n"
     ]
    }
   ],
   "source": [
    "all_sentences, all_tags = predict_entities (input_text)\n",
    "\n",
    "print (all_sentences)\n",
    "print (all_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3bce7d-335d-46a5-a34e-311de6fcbed7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
