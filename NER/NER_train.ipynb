{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2,1\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from seqeval.metrics import classification_report,accuracy_score,f1_score\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm, trange\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import BertTokenizer, BertConfig, BertForTokenClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_train = \"../processed/merged/train.tsv\" \n",
    "train_data = pd.read_csv(data_path_train, sep=\"\\t\").astype(str)\n",
    "\n",
    "data_path_dev= \"../processed/merged/dev.tsv\" \n",
    "dev_data = pd.read_csv(data_path_dev, sep=\"\\t\").astype(str)\n",
    "\n",
    "data_path_test= \"../processed/merged/test.tsv\" \n",
    "test_data = pd.read_csv(data_path_test, sep=\"\\t\").astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parser data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, t) for w, t in zip(s[\"word\"].values.tolist(),\n",
    "                                                        #    s[\"POS\"].values.tolist(),\n",
    "                                                           s[\"tag\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"sentence #\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    # def get_next(self):\n",
    "    #     try:\n",
    "    #         s = self.grouped[\"sentence: {}\".format(self.n_sent)]\n",
    "    #         self.n_sent += 1\n",
    "    #         return s\n",
    "    #     except:\n",
    "    #         return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get full document data struce\n",
    "train_getter = SentenceGetter(train_data)\n",
    "# Get sentence data\n",
    "train_sentences = [[s[0] for s in sent] for sent in train_getter.sentences]\n",
    "train_labels = [[s[1] for s in sent] for sent in train_getter.sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get full document data struce\n",
    "dev_getter = SentenceGetter(dev_data)\n",
    "# Get sentence data\n",
    "dev_sentences = [[s[0] for s in sent] for sent in dev_getter.sentences]\n",
    "dev_labels = [[s[1] for s in sent] for sent in dev_getter.sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get full document data struce\n",
    "test_getter = SentenceGetter(test_data)\n",
    "# Get sentence data\n",
    "test_sentences = [[s[0] for s in sent] for sent in test_getter.sentences]\n",
    "test_labels = [[s[1] for s in sent] for sent in test_getter.sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-problem',\n",
       " 'I-problem']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[125]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a dict for mapping id to tag name\n",
    "#tag2idx = {t: i for i, t in enumerate(tags_vals)}\n",
    "\n",
    "# Recommend to set it by manual define, good for reusing\n",
    "tag2idx={'B-problem': 0,\n",
    " 'B-test': 1,\n",
    " 'B-treatment': 2,\n",
    " 'I-problem': 3,\n",
    " 'I-test': 4,\n",
    " 'I-treatment': 5,\n",
    " 'O': 6,\n",
    " 'X': 7,\n",
    " '[CLS]': 8,\n",
    " '[SEP]': 9\n",
    " }\n",
    "# Mapping index to name\n",
    "tag2name = {tag2idx[key]: key for key in tag2idx}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-problem': 0,\n",
       " 'B-test': 1,\n",
       " 'B-treatment': 2,\n",
       " 'I-problem': 3,\n",
       " 'I-test': 4,\n",
       " 'I-treatment': 5,\n",
       " 'O': 6,\n",
       " 'X': 7,\n",
       " '[CLS]': 8,\n",
       " '[SEP]': 9}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'B-problem',\n",
       " 1: 'B-test',\n",
       " 2: 'B-treatment',\n",
       " 3: 'I-problem',\n",
       " 4: 'I-test',\n",
       " 5: 'I-treatment',\n",
       " 6: 'O',\n",
       " 7: 'X',\n",
       " 8: '[CLS]',\n",
       " 9: '[SEP]'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag2name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make raw data into trainable data for BERT, including:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Set gpu environment\n",
    "- Load tokenizer and tokenize\n",
    "- Set 3 embedding, token embedding, mask word embedding, segmentation embedding\n",
    "- Split data set into train and validate, then send them to dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set up gpu environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can download the tokenizer file into local folder first :\n",
    "- [vocab.txt](https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Manual define vocabulary address, if you download the tokenzier file in local\n",
    "# # vocab.txt, download from: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt\n",
    "# vocabulary = \"Bio_ClinicalBERT/vocab.txt\"\n",
    "# # load tokenizer, with manual file address or pretrained address\n",
    "# tokenizer=BertTokenizer(vocab_file=vocabulary, do_lower_case=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Len of the sentence must be not bigger than the training model\n",
    "# See model's 'max_position_embeddings' = 512\n",
    "max_len  = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenizer text**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In hunggieface for bert, when come across OOV, will word piece the word\n",
    "- We need to adjust the labels base on the tokenize result, “##abc” need to set label \"X\" \n",
    "- Need to set \"[CLS]\" at front and \"[SEP]\" at the end, as what the paper do, [BERT indexer should add [CLS] and [SEP] tokens](https://github.com/allenai/allennlp/issues/2141)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(sentences, labels):\n",
    "    tokenized_texts = []\n",
    "    word_piece_labels = []\n",
    "    i_inc = 0\n",
    "    for word_list,label in (zip(sentences,labels)):\n",
    "        temp_lable = []\n",
    "        temp_token = []\n",
    "        \n",
    "        # # Add [CLS] at the front \n",
    "        # temp_lable.append('[CLS]')\n",
    "        # temp_token.append('[CLS]')\n",
    "        \n",
    "        for word, lab in zip(word_list,label):\n",
    "            token_list = tokenizer.tokenize(word)\n",
    "            for m,token in enumerate(token_list):\n",
    "                temp_token.append(token)\n",
    "                if m==0:\n",
    "                    temp_lable.append(lab)\n",
    "                else:\n",
    "                    temp_lable.append('X')  \n",
    "                    \n",
    "        # # Add [SEP] at the end\n",
    "        # temp_lable.append('[SEP]')\n",
    "        # temp_token.append('[SEP]')\n",
    "        \n",
    "        tokenized_texts.append(temp_token)\n",
    "        word_piece_labels.append(temp_lable)\n",
    "    \n",
    "    id_list = []\n",
    "    target_list = [] \n",
    "    attention_mask_list = []   \n",
    "        \n",
    "    # PADING        \n",
    "    for text, label in zip(tokenized_texts, word_piece_labels):\n",
    "        \n",
    "        # Add [CLS] and [SEP], \n",
    "        # Truncate seq if it is too long\n",
    "        text = ['[CLS]'] + text[:max_len-2] + ['[SEP]']\n",
    "        label = ['[CLS]'] + label[:max_len-2] + ['[SEP]']\n",
    "        \n",
    "        # convert to ids\n",
    "        ids = tokenizer.convert_tokens_to_ids(text)\n",
    "        target_tag =[tag2idx.get(t) for t in label]\n",
    "        \n",
    "        # padding \n",
    "        # Label [PAD] with O (other)\n",
    "        padding_len = max_len - len(ids)\n",
    "        ids = ids + [0] * padding_len\n",
    "        target_tag = target_tag + [tag2idx['O']] * padding_len\n",
    "        \n",
    "        # create masks\n",
    "        attention_masks = [int(i>0) for i in ids]\n",
    "        \n",
    "        id_list.append(ids)\n",
    "        target_list.append(target_tag)\n",
    "        attention_mask_list.append(attention_masks)\n",
    "    \n",
    "    return id_list, target_list, attention_mask_list\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_ids, train_input_tags, train_attention_masks = process_data(train_sentences, train_labels)\n",
    "dev_input_ids, dev_input_tags, dev_attention_masks = process_data(dev_sentences, dev_labels)\n",
    "test_input_ids, test_input_tags, test_attention_masks = process_data(test_sentences, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = torch.tensor(train_input_ids)\n",
    "dev_inputs = torch.tensor(dev_input_ids)\n",
    "test_inputs = torch.tensor(test_input_ids)\n",
    "train_tags = torch.tensor(train_input_tags)\n",
    "dev_tags = torch.tensor(dev_input_tags)\n",
    "test_tags = torch.tensor(test_input_tags)\n",
    "train_masks = torch.tensor(train_attention_masks)\n",
    "dev_masks = torch.tensor(dev_attention_masks)\n",
    "test_masks = torch.tensor(test_attention_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Put data into data loader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set batch num\n",
    "batch_num = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only set token embedding, attention embedding, no segment embedding\n",
    "train_dataset = TensorDataset(train_inputs, train_masks, train_tags)\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "# Drop last can make batch training better for the last one\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_num, drop_last=True)\n",
    "\n",
    "dev_dataset = TensorDataset(dev_inputs, dev_masks, dev_tags)\n",
    "dev_sampler = SequentialSampler(dev_dataset)\n",
    "dev_dataloader = DataLoader(dev_dataset, sampler=dev_sampler, batch_size=batch_num)\n",
    "\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_tags)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load BERT model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this folder, contain model confg(json) and model weight(bin) files\n",
    "# pytorch_model.bin, download from: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin\n",
    "# config.json, downlaod from: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json\n",
    "model_file_address = 'emilyalsentzer/Bio_ClinicalBERT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForTokenClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Will load config and weight with from_pretrained()\n",
    "model = BertForTokenClassification.from_pretrained(model_file_address, num_labels=len(tag2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to GPU,if you are using GPU machine\n",
    "model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add multi GPU support\n",
    "if n_gpu >1:\n",
    "    model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set epoch and grad max num\n",
    "epochs = 10\n",
    "max_grad_norm = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cacluate train optimiazaion num\n",
    "num_train_optimization_steps = int(math.ceil(len(train_inputs) / batch_num) / 1) * epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set fine tuning method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Manual optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True: fine tuning all the layers \n",
    "# False: only fine tuning the classifier layers\n",
    "FULL_FINETUNING = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FULL_FINETUNING:\n",
    "    # Fine tune model all layer parameters\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    # Only fine tune classifier parameters\n",
    "    param_optimizer = list(model.classifier.named_parameters()) \n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "    \n",
    "optimizer = Adam(optimizer_grouped_parameters, lr=5e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TRAIN loop\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 12790\n",
      "  Batch size = 32\n",
      "  Num steps = 4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  10%|█         | 1/10 [00:52<07:50, 52.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.05536287849626614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  20%|██        | 2/10 [01:45<06:59, 52.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.008693494840862374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  30%|███       | 3/10 [02:38<06:08, 52.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.005215539501566804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  40%|████      | 4/10 [03:31<05:16, 52.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0035362142988102497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|█████     | 5/10 [04:25<04:25, 53.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.00272121988088832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  60%|██████    | 6/10 [05:18<03:32, 53.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0021927864580353336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  70%|███████   | 7/10 [06:12<02:39, 53.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.00206140744129536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  80%|████████  | 8/10 [07:05<01:46, 53.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0016493910344733032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  90%|█████████ | 9/10 [07:59<00:53, 53.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0015490532072941815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 10/10 [08:52<00:00, 53.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0014289507929159545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"***** Running training *****\")\n",
    "print(\"  Num examples = %d\"%(len(train_inputs)))\n",
    "print(\"  Batch size = %d\"%(batch_num))\n",
    "print(\"  Num steps = %d\"%(num_train_optimization_steps))\n",
    "for _ in trange(epochs,desc=\"Epoch\"):\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for batch in train_dataloader:\n",
    "        # add batch to gpu\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(b_input_ids, token_type_ids=None,\n",
    "        attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss, scores = outputs[:2]\n",
    "        if n_gpu>1:\n",
    "            # When multi gpu, average it\n",
    "            loss = loss.mean()\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # track train loss\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # print train loss per epoch\n",
    "    print(f\"Train loss: {tr_loss / nb_tr_steps}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_out_address = '../trained_models/NER/C-Bert-test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dir if not exits\n",
    "if not os.path.exists(bert_out_address):\n",
    "        os.makedirs(bert_out_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a trained model, configuration and tokenizer\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we save using the predefined names, we can load using `from_pretrained`\n",
    "output_model_file = os.path.join(bert_out_address, \"pytorch_model.bin\")\n",
    "output_config_file = os.path.join(bert_out_address, \"config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./trained_models/C-Bert-test/vocab.txt',)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model into file\n",
    "torch.save(model_to_save.state_dict(), output_model_file)\n",
    "model_to_save.config.to_json_file(output_config_file)\n",
    "tokenizer.save_vocabulary(bert_out_address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForTokenClassification.from_pretrained(bert_out_address, num_labels=len(tag2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to GPU\n",
    "model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_gpu >1:\n",
    "    model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evalue loop\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n",
      "  Num examples = 27625\n",
      "  Batch size = 32\n",
      "f1 socre: 0.897269\n",
      "Accuracy score: 0.964981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    }
   ],
   "source": [
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "print(\"***** Running evaluation *****\")\n",
    "print(f\"  Num examples = {len(test_inputs)}\")\n",
    "print(f\"  Batch size = {batch_num}\")\n",
    "for batch in test_dataloader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    input_ids, input_mask, label_ids = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, token_type_ids=None,\n",
    "        attention_mask=input_mask,)\n",
    "        # For eval mode, the first result of outputs is logits\n",
    "        logits = outputs[0] \n",
    "\n",
    "    # Get NER predict result\n",
    "    logits = torch.argmax(F.log_softmax(logits, dim=2),dim=2)\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    # Get NER true result\n",
    "    label_ids = label_ids.to('cpu').numpy()\n",
    "    # Only predict the real word, mark=0, will not calculate\n",
    "    input_mask = input_mask.to('cpu').numpy()\n",
    "    # Compare the valuable predict result\n",
    "    for i,mask in enumerate(input_mask):\n",
    "        # Real one\n",
    "        temp_1 = []\n",
    "        # Predict one\n",
    "        temp_2 = []\n",
    "\n",
    "        for j, m in enumerate(mask):\n",
    "            # Mark=0, meaning its a pad word, dont compare\n",
    "            if m:\n",
    "                if tag2name[label_ids[i][j]] not in [\"X\", \"[CLS]\", \"[SEP]\"]: # Exclude the X label\n",
    "                    # print(tag2name[logits[i][j]])\n",
    "                    temp_1.append(tag2name[label_ids[i][j]])\n",
    "                    temp_2.append(tag2name[logits[i][j]])\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        y_true.append(temp_1)\n",
    "        y_pred.append(temp_2)\n",
    "\n",
    "\n",
    "print(\"f1 socre: %f\"%(f1_score(y_true, y_pred)))\n",
    "print(\"Accuracy score: %f\"%(accuracy_score(y_true, y_pred)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classification_report' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-35b401ca59b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdigits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'classification_report' is not defined"
     ]
    }
   ],
   "source": [
    "classification_report(y_true, y_pred, digits=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Eval results *****\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     problem     0.9119    0.9477    0.9295       153\n",
      "        test     0.8298    0.8966    0.8619        87\n",
      "   treatment     0.9389    0.8601    0.8978       143\n",
      "\n",
      "   micro avg     0.9010    0.9034    0.9022       383\n",
      "   macro avg     0.8936    0.9015    0.8964       383\n",
      "weighted avg     0.9034    0.9034    0.9023       383\n",
      "\n",
      "f1 socre: 0.902216\n",
      "Accuracy score: 0.967076\n"
     ]
    }
   ],
   "source": [
    "# Get acc , recall, F1 result report\n",
    "report = classification_report(y_true, y_pred, digits=4)\n",
    "\n",
    "# Save the report into file\n",
    "output_eval_file = os.path.join(bert_out_address, \"eval_results.txt\")\n",
    "with open(output_eval_file, \"w\") as writer:\n",
    "    print(\"***** Eval results *****\")\n",
    "    print(\"\\n%s\"%(report))\n",
    "    print(\"f1 socre: %f\"%(f1_score(y_true, y_pred)))\n",
    "    print(\"Accuracy score: %f\"%(accuracy_score(y_true, y_pred)))\n",
    "    \n",
    "    writer.write(\"f1 socre:\\n\")\n",
    "    writer.write(str(f1_score(y_true, y_pred)))\n",
    "    writer.write(\"\\n\\nAccuracy score:\\n\")\n",
    "    writer.write(str(accuracy_score(y_true, y_pred)))\n",
    "    writer.write(\"\\n\\n\")  \n",
    "    writer.write(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "33fa62565346c5b00f7c01ac7dab8740690287f5b700f97087b1b874bc66ae30"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
