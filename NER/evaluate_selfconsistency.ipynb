{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eec17fa4",
   "metadata": {},
   "source": [
    "Install and import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f32b61a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\nipua\\appdata\\local\\anaconda3\\envs\\ner\\lib\\site-packages (4.47.1)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\nipua\\appdata\\local\\anaconda3\\envs\\ner\\lib\\site-packages (4.0.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\nipua\\appdata\\local\\anaconda3\\envs\\ner\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\nipua\\appdata\\local\\anaconda3\\envs\\ner\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\nipua\\appdata\\local\\anaconda3\\envs\\ner\\lib\\site-packages (from transformers) (0.27.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\nipua\\appdata\\local\\anaconda3\\envs\\ner\\lib\\site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nipua\\appdata\\local\\anaconda3\\envs\\ner\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\nipua\\appdata\\local\\anaconda3\\envs\\ner\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\nipua\\appdata\\local\\anaconda3\\envs\\ner\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\nipua\\appdata\\local\\anaconda3\\envs\\ner\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\nipua\\appdata\\local\\anaconda3\\envs\\ner\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\nipua\\appdata\\local\\anaconda3\\envs\\ner\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\nipua\\appdata\\local\\anaconda3\\envs\\ner\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\nipua\\appdata\\local\\anaconda3\\envs\\ner\\lib\\site-packages (from sentence-transformers) (2.5.1+cu124)\n",
      "Requirement already satisfied: scipy in c:\\users\\nipua\\appdata\\local\\anaconda3\\envs\\ner\\lib\\site-packages (from sentence-transformers) (1.14.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\nipua\\appdata\\local\\anaconda3\\envs\\ner\\lib\\site-packages (from sentence-transformers) (10.2.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\nipua\\appdata\\local\\anaconda3\\envs\\ner\\lib\\site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\nipua\\appdata\\local\\anaconda3\\envs\\ner\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\nipua\\appdata\\local\\anaconda3\\envs\\ner\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\nipua\\appdata\\local\\anaconda3\\envs\\ner\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.2.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\nipua\\appdata\\local\\anaconda3\\envs\\ner\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nipua\\appdata\\local\\anaconda3\\envs\\ner\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\nipua\\appdata\\local\\anaconda3\\envs\\ner\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\nipua\\appdata\\local\\anaconda3\\envs\\ner\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\nipua\\appdata\\local\\anaconda3\\envs\\ner\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nipua\\appdata\\local\\anaconda3\\envs\\ner\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nipua\\appdata\\local\\anaconda3\\envs\\ner\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nipua\\appdata\\local\\anaconda3\\envs\\ner\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nipua\\appdata\\local\\anaconda3\\envs\\ner\\lib\\site-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nipua\\appdata\\local\\anaconda3\\envs\\ner\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers sentence-transformers scikit-learn\n",
    "\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8134803",
   "metadata": {},
   "source": [
    "Load ClinicalBERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0367765d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ClinicalBERT\n",
    "model_name = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "def embed_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "    return embeddings[0].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9001e59",
   "metadata": {},
   "source": [
    "Load the four files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3fa288a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WindowsPath('../Results/test/DeepSeek/0001_zeroshot.txt'), WindowsPath('../Results/test/DeepSeek/0001_document_level.txt'), WindowsPath('../Results/test/DeepSeek/0001_sentence_level.txt'), WindowsPath('../Results/test/DeepSeek/0001_entity_unseen.txt')]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"DeepSeek\"\n",
    "file_path = Path(f\"../Results/test/{model_name}/\")\n",
    "\n",
    "zeroshot_filename = \"0001_zeroshot.txt\"\n",
    "document_filename = \"0001_document_level.txt\"\n",
    "sentence_filename = \"0001_sentence_level.txt\"\n",
    "entity_filename = \"0001_entity_unseen.txt\"\n",
    "\n",
    "zeroshot_doc = file_path / zeroshot_filename\n",
    "document_doc = file_path / document_filename\n",
    "sentence_doc = file_path / sentence_filename\n",
    "entity_doc = file_path / entity_filename\n",
    "\n",
    "file_paths = [zeroshot_doc, document_doc, sentence_doc, entity_doc]\n",
    "\n",
    "print(file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795c8060",
   "metadata": {},
   "source": [
    "parse all entity-label pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26a32791",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "all_entities = []  # List of dicts: {\"entity\": ..., \"label\": ..., \"file\": ..., \"embedding\": ...}\n",
    "\n",
    "pattern = r'entity=\"(.*?)\"\\s+label=\"(.*?)\"'\n",
    "\n",
    "for idx, path in enumerate(file_paths):\n",
    "    with open(path, 'r') as f:\n",
    "        content = f.readlines()\n",
    "        for line in content:\n",
    "            match = re.search(pattern, line.strip())\n",
    "            if match:\n",
    "                entity, label = match.groups()\n",
    "                all_entities.append({\n",
    "                    \"entity\": entity,\n",
    "                    \"label\": label,\n",
    "                    \"file\": f\"doc{idx+1}\",\n",
    "                    \"embedding\": embed_text(entity)\n",
    "                })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a5f9db",
   "metadata": {},
   "source": [
    "Cluster similar entities (â‰¥ 0.92 cosine similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dfae70e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = []  # Each cluster is a list of indices into `all_entities`\n",
    "visited = set()\n",
    "\n",
    "for i, ent1 in enumerate(all_entities):\n",
    "    if i in visited:\n",
    "        continue\n",
    "    cluster = [i]\n",
    "    visited.add(i)\n",
    "    for j in range(i + 1, len(all_entities)):\n",
    "        if j in visited:\n",
    "            continue\n",
    "        sim = cosine_similarity(\n",
    "            [ent1[\"embedding\"]],\n",
    "            [all_entities[j][\"embedding\"]]\n",
    "        )[0][0]\n",
    "        if sim >= 0.92:\n",
    "            cluster.append(j)\n",
    "            visited.add(j)\n",
    "    clusters.append(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d368d0",
   "metadata": {},
   "source": [
    "Reduce clusters based on document coverage and assign majority label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c43b7589",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_outputs = []\n",
    "\n",
    "for cluster in clusters:\n",
    "    files_covered = set(all_entities[i][\"file\"] for i in cluster)\n",
    "    if len(files_covered) < 2:\n",
    "        continue  # Skip entities not in at least 2 docs\n",
    "\n",
    "    labels = [all_entities[i][\"label\"] for i in cluster]\n",
    "    label_counts = Counter(labels)\n",
    "    most_common = label_counts.most_common()\n",
    "\n",
    "    if len(most_common) == 1 or (len(most_common) > 1 and most_common[0][1] > most_common[1][1]):\n",
    "        final_label = most_common[0][0]\n",
    "    else:\n",
    "        final_label = \"unknown\"\n",
    "\n",
    "    representative_entity = all_entities[cluster[0]][\"entity\"]  # You can change this to centroid logic\n",
    "    final_outputs.append(f'entity=\"{representative_entity}\" label=\"{final_label}\"')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd8e949",
   "metadata": {},
   "source": [
    "Write final results to output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "753b5bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved output to ..\\Results\\test\\DeepSeek\\0001_self_consistency.txt\n"
     ]
    }
   ],
   "source": [
    "output_filename = \"0001_self_consistency.txt\"\n",
    "output_file = file_path / output_filename\n",
    "\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    for line in final_outputs:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "print(f\"Saved output to {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
