{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1eebfc41ac147e29350477f8eeaa766",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-03 02:39:19 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "========================\n",
      "\n",
      "2022-05-03 02:39:19 INFO: Use device: gpu\n",
      "2022-05-03 02:39:19 INFO: Loading: tokenize\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-03 02:39:21 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertForTokenClassification, BertTokenizer\n",
    "from termcolor import colored\n",
    "import copy\n",
    "import stanza\n",
    "try:\n",
    "    nlp = stanza.Pipeline(lang='en', processors='tokenize')\n",
    "except Exception:\n",
    "    stanza.download('en')\n",
    "    nlp = stanza.Pipeline(lang='en', processors='tokenize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "Admission Date:  [**2118-6-2**]       Discharge Date:  [**2118-6-14**]  Date of Birth:                    Sex:  F  Service:  MICU and then to [**Doctor Last Name **] Medicine  HISTORY OF PRESENT ILLNESS:  This is an 81-year-old female with a history of emphysema (not on home O2), who presents with three days of shortness of breath thought by her primary care doctor to be a COPD flare.  Two days prior to admission, she was started on a prednisone taper and one day prior to admission she required oxygen at home in order to maintain oxygen saturation greater than 90%.  She has also been on levofloxacin and nebulizers, and was not getting better, and presented to the [**Hospital1 18**] Emergency Room.  In the [**Hospital3 **] Emergency Room, her oxygen saturation was 100% on CPAP.  She was not able to be weaned off of this despite nebulizer treatment and Solu-Medrol 125 mg IV x2.  Review of systems is negative for the following:  Fevers, chills, nausea, vomiting, night sweats, change in weight, gastrointestinal complaints, neurologic changes, rashes, palpitations, orthopnea.  Is positive for the following: Chest pressure occasionally with shortness of breath with exertion, some shortness of breath that is positionally related, but is improved with nebulizer treatment.  PAST MEDICAL HISTORY: 1. COPD.  Last pulmonary function tests in [**2117-11-3**] demonstrated a FVC of 52% of predicted, a FEV1 of 54% of predicted, a MMF of 23% of predicted, and a FEV1:FVC ratio of 67% of predicted, that does not improve with bronchodilator treatment.  The FVC, however, does significantly improve with bronchodilator treatment consistent with her known reversible air flow obstruction in addition to an underlying restrictive ventilatory defect.  The patient has never been on home oxygen prior to this recent episode.  She has never been on steroid taper or been intubated in the past. 2. Lacunar CVA.  MRI of the head in [**2114-11-4**] demonstrates \"mild degree of multiple small foci of high T2 signal within the white matter of both cerebral hemispheres as well as the pons, in the latter region predominantly to the right of midline.  The abnormalities, while nonspecific in etiology, are most likely secondary to chronic microvascular infarction.  There is no mass, lesion, shift of the normal midline strictures or hydrocephalus.  The major vascular flow patterns are preserved.  There is moderate right maxillary, moderate bilateral ethmoid, mild left maxillary, minimal right sphenoid, and frontal sinus mucosal thickening.  These abnormalities could represent an allergic or some other type of inflammatory process.  Additionally noted is a moderately enlarged subtotally empty sella turcica\". 3. Angina:  Most recent stress test was in [**2118-1-3**] going for four minutes with a rate pressure product of 10,000, 64% of maximum predicted heart rate without evidence of ischemic EKG changes or symptoms.  The imaging portion of the study demonstrated no evidence of myocardial ischemia and a calculated ejection fraction of 84%.  The patient denies angina at rest and gets angina with walking a few blocks. Are alleviated by sublingual nitroglycerin. 4. Hypothyroidism on Synthroid. 5. Depression on Lexapro. 6. Motor vehicle accident with head injury approximately 10 years ago.  MEDICATIONS ON ADMISSION: 1. Hydrochlorothiazide 25 q.d. 2. Prednisone 60 mg, 50 mg, 40 mg, 20 mg. 3. Levofloxacin 500 mg q.d. 4. Imdur 60 mg q.d. 5. Synthroid 75 mcg q.d. 6. Pulmicort nebulizer b.i.d. 7. Albuterol nebulizer q.4. prn. 8. Lexapro 10 mg q.d. 9. Protonix 40 mg q.d. 10. Aspirin 81 mg q.d.  ALLERGIES:  Norvasc leads to lightheadedness and headache.  FAMILY HISTORY:  Noncontributory.  SOCIAL HISTORY:  Lives with her husband, Dr. [**Known lastname 1809**] an eminent Pediatric Neurologist at [**Hospital3 1810**].  The patient is a prior smoker, but has not smoked in over 10 years.  She has no known alcohol use and she is a full code.  PHYSICAL EXAM AT TIME OF ADMISSION:  Blood pressure 142/76, heart rate 100 and regular, respirations at 17-21, and 97% axillary temperature.  She was saturating at 100% on CPAP with dry mucous membranes.  An elderly female in no apparent distress.  Pupils are equal, round, and reactive to light and accommodation.  Extraocular movements are intact.  Oropharynx difficult to assess due to CPAP machine.  No evidence of jugular venous pressure, however, the strap from the CPAP machine obscures the neck exam.  Cranial nerves II through XII are grossly intact.  Neck is supple without lymphadenopathy.  Heart exam:  Tachycardic, regular, obscured by loud bilateral wheezing with increase in the expiratory phase as well as profuse scattered rhonchi throughout the lung fields.  Positive bowel sounds, soft, nontender, nondistended, obese, no masses.  Mild edema of the lower extremities without clubbing or cyanosis, no rashes.  There is a right hand hematoma.  Strength is assessed as [**5-9**] in the lower extremities, [**5-9**] in the upper extremities with a normal mental status and cognition.  LABORATORY STUDIES:  White count 19, hematocrit 41, platelets 300.  Chem-7:  127, 3.6, 88, 29, 17, 0.6, 143.  Troponin was negative.  CKs were negative times three.  Initial blood gas showed a pH of 7.4, pO2 of 66, pCO2 of 54.  Chest x-ray demonstrates a moderate sized hiatal hernia, segmental atelectasis, left lower lobe infiltrate versus segmental atelectasis.  EKG shows normal sinus rhythm at 113 beats per minute, normal axis, no evidence of ST-T wave changes.  BRIEF SUMMARY OF HOSPITAL COURSE: 1. COPD/dyspnea/pneumonia:  The patient was initially placed on an aggressive steroid taper and admitted to the Medical Intensive Care Unit due to her difficulty with oxygenation despite CPAP machine.  She was also given nebulizer treatments q.4h. as well as chest PT.  The nebulizers were increased to q.1h. due to the fact that she continued to have labored breathing.  Due to persistent respiratory failure and labored breathing, the patient was intubated on [**2118-6-7**] in order to improve oxygenation, ventilation, and ability to suction.  A bronchoscopy was performed on [**2118-6-7**], which demonstrated marked narrowing of the airways with expiration consistent with tracheomalacia.  On [**2118-6-9**], two silicone stents were placed, one in the left main stem (12 x 25 and one in the trachea 16 x 40) by Dr. [**First Name (STitle) **] [**Name (STitle) **] under rigid bronchoscopy with general anesthesia.  On [**2118-6-11**], the patient was extubated to a cool mist shovel mask and her oxygen was titrated down to 2 liters nasal cannula at which time she was transferred to the medical floor.  On the medical floor, the steroids were weaned to off on [**2118-6-14**], and the patient was saturating at 97% on 2 liters, 92% on room air.  On [**2118-6-14**], the patient was seen again by the Interventional Pulmonology service, who agreed that she looked much improved and recommended that she go to pulmonary rehabilitation with followup within six weeks\\cf4 \\\\'\\cf2  time status post placement of stents in respiratory failure.  2. Cardiovascular:  The patient was ruled out for a MI.  She did have another episode on the medical floor of chest pain, which showed no evidence of EKG changes and negative troponin, negative CKs x3.  She was continued on aspirin, Imdur, and diltiazem for rate control per her outpatient regimen.  3. Hypertension:  She was maintained on diltiazem and hydrochlorothiazide with adequate blood pressure control and normalization of electrolytes.  4. Hematuria:  The patient had intermittent hematuria likely secondary to Foley placement.  The Foley catheter was discontinued on [**2118-6-14**].  She had serial urinalyses, which were all negative for signs of infection.  5. Hyperglycemia:  Patient was placed on insulin-sliding scale due to hyperglycemia, which was steroid induced.  This worked quite well and her glucose came back to normal levels once the steroids were tapered to off.  6. Leukocytosis:  Patient did have a profound leukocytosis of 20 to 22 during much of her hospital course.  As the steroids were tapered to off, her white blood cell count on [**2118-6-14**] was 15,000.  It was felt that the leukocytosis was secondary to both steroids as well as question of a left lower lobe pneumonia.  7. For the left lower lobe pneumonia, the patient had initially received a course of levofloxacin 500 p.o. q.d. from [**2118-6-4**] to [**2118-6-10**].  This was restarted on [**2118-6-12**] for an additional seven day course given the fact that she still had the leukocytosis and still had marked rales at the left lower lobe.  8. Hypothyroidism:  The patient was continued on outpatient medical regimen.  9. Depression:  The patient was continued on Lexapro per outpatient regimen.  It is recommended that she follow up with a therapist as an outpatient due to the fact that she did have a blunted affect throughout much of the hospital course, and did appear clinically to be depressed.  10. Prophylaxis:  She was maintained on proton-pump inhibitor with subQ Heparin.  11. Sore throat:  The patient did have a sore throat for much of the hospital course post extubation.  This was treated with Cepacol lozenges as well as KBL liquid (a solution containing Kaopectate, Bismuth, and lidocaine) at bedtime.  12. Communication/code status:  The patient was full code throughout her hospital course, and communication was maintained with the patient and her husband.  13. Muscle weakness:  The patient did have profound muscle weakness and was evaluated by Physical Therapy, and was found to have impaired functional mobility, impaired musculoskeletal performance, impaired gas exchange, impaired endurance, impaired ventilation, and needed help with supine to sit.  However, she was able to tolerate sitting in a chair for approximately one hour.  On motor exam, her flexors and extensors of the lower extremities were [**4-8**] at the knee, [**4-8**] at the ankle, [**4-8**] at the elbows, and [**4-8**] hips.  It was felt that this weakness was most likely due to a combination of steroid myopathy as well as muscle atrophy secondary to deconditioning after a prolonged hospital course.  14. Speech/swallow:  The patient had a Speech and Swallow evaluation showing no evidence of dysphagia, no evidence of vocal cord damage status post tracheal stent placement.  DISCHARGE CONDITION:  The patient was able to oxygenate on room air at 93% at the time of discharge.  She was profoundly weak, but was no longer tachycardic and had a normal blood pressure.  Her respirations were much improved albeit with transmitted upper airway sounds.  DISCHARGE STATUS:  The patient will be discharged to [**Hospital1 **] for both pulmonary and physical rehabilitation.  DISCHARGE MEDICATIONS: 1. Levothyroxine 75 mcg p.o. q.d. 2. Citalopram 10 mg p.o. q.d. 3. Aspirin 81 mg p.o. q.d. 4. Fluticasone 110 mcg two puffs inhaled b.i.d. 5. Salmeterol Diskus one inhalation b.i.d. 6. Acetaminophen 325-650 mg p.o. q.4-6h. prn. 7. Ipratropium bromide MDI two puffs inhaled q.2h. prn. 8. Albuterol 1-2 puffs inhaled q.2h. prn. 9. Zolpidem tartrate 5 mg p.o. q.h.s. prn. 10. Isosorbide dinitrate 10 mg p.o. t.i.d. 11. Diltiazem 60 mg p.o. q.i.d. 12. Pantoprazole 40 mg p.o. q.24h. 13. Trazodone 25 mg p.o. q.h.s. prn. 14. SubQ Heparin 5000 units subcutaneous b.i.d. until such time that the patient is able to get out of bed twice a day. 15. Cepacol lozenges q.2h. prn. 16. Levofloxacin 500 mg p.o. q.d. for a seven day course to be completed on [**2118-6-21**]. 17. Kaopectate/Benadryl/lidocaine 5 mL p.o. b.i.d. prn, not to be given around mealtimes for concern of dysphagia induced by lidocaine. 18. Lorazepam 0.5-2 mg IV q.6h. prn.  FOLLOW-UP PLANS:  The patient is recommended to followup with Dr. [**First Name4 (NamePattern1) **] [**Last Name (NamePattern1) 1407**], [**Telephone/Fax (1) 1408**] within two weeks of leaving of the hospital.  She is also recommended to followup with the Interventional Pulmonary service for followup status post stent placement.  She is also recommended to followup with a neurologist if her muscle weakness does not improve within one week on physical therapy with concern for steroid-induced myopathy.  FINAL DIAGNOSES: 1. Tracheomalacia status post tracheal and left main stem bronchial stent placement. 2. Hypertension. 3. Hypothyroidism. 4. Restrictive lung defect. 5. Depression.                        DR.[**Last Name (STitle) **],[**First Name3 (LF) **] 12-207   Dictated By:[**Last Name (NamePattern1) 1811**] MEDQUIST36  D:  [**2118-6-14**]  11:30 T:  [**2118-6-14**]  11:33 JOB#:  [**Job Number 1812**]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using  cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from NER.processor import *\n",
    "from NER.ner_utils import *\n",
    "from NER import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = len(tag2idx)\n",
    "save_model_address = '../trained_models/NER/C-Bert-test'\n",
    "model = BertForTokenClassification.from_pretrained(save_model_address, num_labels=num_labels)\n",
    "tokenizer = BertTokenizer.from_pretrained(save_model_address, do_lower_case=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_long_text(long_text):\n",
    "    all_sentences = []\n",
    "    all_tags = []\n",
    "    doc = nlp(long_text)\n",
    "    for i, sentence in enumerate(doc.sentences):\n",
    "        # temp_token: tokenized words\n",
    "        # input_ids: convert temp_token to id\n",
    "        temp_token, input_ids, attention_masks = create_query(sentence, tokenizer)\n",
    "        result_list = model_inference(model, input_ids)\n",
    "        result = [tag2name[t] for t in result_list]\n",
    "        pretok_sent = \"\"\n",
    "        pretags = \"\"\n",
    "        for i, tok in enumerate(temp_token):\n",
    "            if tok.startswith(\"##\"):\n",
    "                pretok_sent += tok[2:]\n",
    "            else:\n",
    "                pretok_sent += f\" {tok}\"\n",
    "                pretags += f\" {result[i]}\"\n",
    "        pretok_sent = pretok_sent[1:]\n",
    "        pretags = pretags[1:]\n",
    "        s = pretok_sent.split()\n",
    "        t = pretags.split()\n",
    "        all_sentences.append(s)\n",
    "        all_tags.append(t)\n",
    "    return all_sentences, all_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences, all_tags = predict_long_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_with_problem = []\n",
    "all_problems_in_text_tmp = []\n",
    "all_treatment_in_text = []\n",
    "all_test_in_text = []\n",
    "\n",
    "for s, t in zip(all_sentences, all_tags):\n",
    "    flag_treatment, flag_problem, flag_test = 0, 0, 0\n",
    "    problem_in_sentence = ''\n",
    "    treatment_in_sentence = []\n",
    "    test_in_sentence = []\n",
    "\n",
    "    for i in range(1, len(t)-1):\n",
    "        if t[i] == 'B-problem':\n",
    "            flag_problem = 1\n",
    "            # if there is entities, add the index of sentence to a list\n",
    "            # sentences_with_problem.append(n)\n",
    "            # append the index of entity to a list\n",
    "            if problem_in_sentence: \n",
    "                problem_in_sentence = f'{problem_in_sentence}| {str(i)}'\n",
    "            else: \n",
    "                problem_in_sentence += str(i)\n",
    "        elif t[i] == 'I-problem' or t[i] == 'X' and flag_problem == 1:\n",
    "            problem_in_sentence = f'{problem_in_sentence} {str(i)}'\n",
    "\n",
    "        elif t[i] == 'B-test':\n",
    "            flag_test = 1\n",
    "            test_in_sentence.append(i)\n",
    "        elif t[i] == 'I-test' or t[i] == 'X' and flag_test == 1 :\n",
    "            test_in_sentence.append(i)\n",
    "\n",
    "        elif t[i] == 'B-treatment':\n",
    "            flag_treatment = 1\n",
    "            treatment_in_sentence.append(i)\n",
    "        elif t[i] == 'I-treatment' or t[i] == 'X' and flag_treatment == 1 :\n",
    "            treatment_in_sentence.append(i)   \n",
    "\n",
    "        elif t[i] in ['O', 'X']:\n",
    "            flag_treatment, flag_problem, flag_test = 0, 0, 0 \n",
    "            # print(s[i], end=' ')\n",
    "\n",
    "    all_problems_in_text_tmp.append(problem_in_sentence)\n",
    "    all_treatment_in_text.append(treatment_in_sentence)\n",
    "    all_test_in_text.append(test_in_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get index of entities in each sentence\n",
    "all_problems_in_text = []\n",
    "for x in all_problems_in_text_tmp:\n",
    "    if x:\n",
    "        index = x.split('|')\n",
    "        tmp = [i.split() for i in index]\n",
    "        all_problems_in_text.append(tmp)\n",
    "    else: \n",
    "        all_problems_in_text.append(x)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sentences with '[entity]' tag\n",
    "sentences_with_problem = []\n",
    "for sentence, problem_index in zip(all_sentences, all_problems_in_text):\n",
    "    # print(problem_index)\n",
    "    if problem_index:\n",
    "        for i_list in problem_index:\n",
    "            s = copy.deepcopy(sentence)\n",
    "            s.insert(int(i_list[-1])+1, '[entity]')\n",
    "            s.insert(int(i_list[0]), '[entity]')\n",
    "            s = ' '.join(s)\n",
    "            sentences_with_problem.append(s)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] This is an 81 - year - old female with a history of [entity] emphysema [entity] ( not on home O2 ) , who presents with three days of shortness of breath thought by her primary care doctor to be a COPD flare . [SEP]',\n",
       " '[CLS] This is an 81 - year - old female with a history of emphysema ( not on home O2 ) , who presents with three days of [entity] shortness of breath [entity] thought by her primary care doctor to be a COPD flare . [SEP]',\n",
       " '[CLS] This is an 81 - year - old female with a history of emphysema ( not on home O2 ) , who presents with three days of shortness of breath thought by her primary care doctor to be [entity] a COPD flare [entity] . [SEP]',\n",
       " '[CLS] Review of systems is negative for the following : [entity] Fevers [entity] , chills , nausea , vomiting , night sweats , change in weight , gastrointestinal complaints , neurologic changes , rashes , palpitations , orthopnea . [SEP]',\n",
       " '[CLS] Review of systems is negative for the following : Fevers , [entity] chills [entity] , nausea , vomiting , night sweats , change in weight , gastrointestinal complaints , neurologic changes , rashes , palpitations , orthopnea . [SEP]',\n",
       " '[CLS] Review of systems is negative for the following : Fevers , chills , [entity] nausea [entity] , vomiting , night sweats , change in weight , gastrointestinal complaints , neurologic changes , rashes , palpitations , orthopnea . [SEP]',\n",
       " '[CLS] Review of systems is negative for the following : Fevers , chills , nausea , [entity] vomiting [entity] , night sweats , change in weight , gastrointestinal complaints , neurologic changes , rashes , palpitations , orthopnea . [SEP]',\n",
       " '[CLS] Review of systems is negative for the following : Fevers , chills , nausea , vomiting , [entity] night sweats [entity] , change in weight , gastrointestinal complaints , neurologic changes , rashes , palpitations , orthopnea . [SEP]',\n",
       " '[CLS] Review of systems is negative for the following : Fevers , chills , nausea , vomiting , night sweats , [entity] change in weight [entity] , gastrointestinal complaints , neurologic changes , rashes , palpitations , orthopnea . [SEP]',\n",
       " '[CLS] Review of systems is negative for the following : Fevers , chills , nausea , vomiting , night sweats , change in weight , [entity] gastrointestinal complaints [entity] , neurologic changes , rashes , palpitations , orthopnea . [SEP]',\n",
       " '[CLS] Review of systems is negative for the following : Fevers , chills , nausea , vomiting , night sweats , change in weight , gastrointestinal complaints , [entity] neurologic changes [entity] , rashes , palpitations , orthopnea . [SEP]',\n",
       " '[CLS] Review of systems is negative for the following : Fevers , chills , nausea , vomiting , night sweats , change in weight , gastrointestinal complaints , neurologic changes , [entity] rashes [entity] , palpitations , orthopnea . [SEP]',\n",
       " '[CLS] Review of systems is negative for the following : Fevers , chills , nausea , vomiting , night sweats , change in weight , gastrointestinal complaints , neurologic changes , rashes , [entity] palpitations [entity] , orthopnea . [SEP]',\n",
       " '[CLS] Review of systems is negative for the following : Fevers , chills , nausea , vomiting , night sweats , change in weight , gastrointestinal complaints , neurologic changes , rashes , palpitations , [entity] orthopnea [entity] . [SEP]',\n",
       " '[CLS] Is positive for the following : Chest pressure occasionally with [entity] shortness of breath [entity] with exertion , some shortness of breath that is positionally related , but is improved with nebulizer treatment . [SEP]',\n",
       " '[CLS] Is positive for the following : Chest pressure occasionally with shortness of breath with exertion , [entity] some shortness of breath [entity] that is positionally related , but is improved with nebulizer treatment . [SEP]',\n",
       " '[CLS] PAST MEDICAL HISTORY : 1 . [entity] COPD [entity] . [SEP]',\n",
       " '[CLS] The FVC , however , does significantly improve with bronchodilator treatment consistent with [entity] her known reversible air flow obstruction [entity] in addition to an underlying restrictive ventilatory defect . [SEP]',\n",
       " '[CLS] The FVC , however , does significantly improve with bronchodilator treatment consistent with her known reversible air flow obstruction in addition to [entity] an underlying restrictive ventilatory defect [entity] . [SEP]',\n",
       " '[CLS] The patient has never been on home oxygen prior to [entity] this recent episode [entity] . [SEP]',\n",
       " '[CLS] MRI of the head in [ * * 2114 - 11 - 4 * * ] demonstrates \" mild degree of [entity] multiple small foci of high T2 signal within the white matter [entity] of both cerebral hemispheres as well as the pons , in the latter region predominantly to the right of midline . [SEP]',\n",
       " '[CLS] [entity] The abnormalities [entity] , while nonspecific in etiology , are most likely secondary to chronic microvascular infarction . [SEP]',\n",
       " '[CLS] The abnormalities , while nonspecific in etiology , are most likely secondary to [entity] chronic microvascular infarction [entity] . [SEP]',\n",
       " '[CLS] There is no [entity] mass [entity] , lesion , shift of the normal midline strictures or hydrocephalus . [SEP]',\n",
       " '[CLS] There is no mass , [entity] lesion [entity] , shift of the normal midline strictures or hydrocephalus . [SEP]',\n",
       " '[CLS] There is no mass , lesion , [entity] shift of the normal midline strictures [entity] or hydrocephalus . [SEP]',\n",
       " '[CLS] There is no mass , lesion , shift of the normal midline strictures or [entity] hydrocephalus [entity] . [SEP]',\n",
       " '[CLS] There is [entity] moderate right maxillary , moderate bilateral ethmoid , mild left maxillary , minimal right sphenoid , and frontal sinus mucosal thickening [entity] . [SEP]',\n",
       " '[CLS] [entity] These abnormalities could represent an allergic or some other type of inflammatory process [entity] . [SEP]',\n",
       " '[CLS] Additionally noted is [entity] a moderately enlarged subtotally [entity] empty sella turcica \" . [SEP]',\n",
       " '[CLS] Most recent stress test was in [ * * 2118 - 1 - 3 * * ] going for four minutes with a rate pressure product of 10 , 000 , 64 % of maximum predicted heart rate without evidence of [entity] ischemic EKG changes [entity] or symptoms . [SEP]',\n",
       " '[CLS] Most recent stress test was in [ * * 2118 - 1 - 3 * * ] going for four minutes with a rate pressure product of 10 , 000 , 64 % of maximum predicted heart rate without evidence of ischemic EKG changes or [entity] symptoms [entity] . [SEP]',\n",
       " '[CLS] The imaging portion of the study demonstrated no evidence of [entity] myocardial ischemia [entity] and a calculated ejection fraction of 84 % . [SEP]',\n",
       " '[CLS] The patient denies [entity] angina [entity] at rest and gets angina with walking a few blocks . [SEP]',\n",
       " '[CLS] The patient denies angina at rest and gets [entity] angina [entity] with walking a few blocks . [SEP]',\n",
       " '[CLS] 4 . [entity] Hypothyroidism [entity] on Synthroid . [SEP]',\n",
       " '[CLS] 5 . [entity] Depression [entity] on Lexapro . [SEP]',\n",
       " '[CLS] 6 . [entity] Motor [entity] vehicle accident with head injury approximately 10 years ago . [SEP]',\n",
       " '[CLS] 6 . Motor vehicle accident with [entity] head injury [entity] approximately 10 years ago . [SEP]',\n",
       " '[CLS] 7 . Albuterol nebulizer q . 4 . prn . 8 . Lexapro 10 mg q . d . 9 . Protonix 40 mg q . d . 10 . Aspirin 81 mg q . d . ALLERGIES : Norvasc leads to [entity] lightheaded [entity] [SEP]',\n",
       " '[CLS] She was saturating at 100 % on CPAP with [entity] dry mucous membranes [entity] . [SEP]',\n",
       " '[CLS] An elderly female in no [entity] apparent distress [entity] . [SEP]',\n",
       " '[CLS] No evidence of [entity] jugular venous pressure [entity] , however , the strap from the CPAP machine obscures the neck exam . [SEP]',\n",
       " '[CLS] Neck is supple without [entity] lymphadenopathy [entity] . [SEP]',\n",
       " '[CLS] Heart exam : [entity] Tachycardic [entity] , regular , obscured by loud bilateral wheezing with increase in the expiratory phase as well as profuse scattered rhonchi throughout the lung fields . [SEP]',\n",
       " '[CLS] Heart exam : Tachycardic , regular , obscured by [entity] loud bilateral wheezing with increase in the expiratory phase [entity] as well as profuse scattered rhonchi throughout the lung fields . [SEP]',\n",
       " '[CLS] Heart exam : Tachycardic , regular , obscured by loud bilateral wheezing with increase in the expiratory phase as well as [entity] profuse scattered rhonchi throughout the lung fields [entity] . [SEP]',\n",
       " '[CLS] Positive bowel sounds , soft , [entity] nontender [entity] , nondistended , obese , no masses . [SEP]',\n",
       " '[CLS] Positive bowel sounds , soft , nontender , [entity] nondistended [entity] , obese , no masses . [SEP]',\n",
       " '[CLS] Positive bowel sounds , soft , nontender , nondistended , [entity] obese [entity] , no masses . [SEP]',\n",
       " '[CLS] Positive bowel sounds , soft , nontender , nondistended , obese , no [entity] masses [entity] . [SEP]',\n",
       " '[CLS] [entity] Mild edema of the lower extremities [entity] without clubbing or cyanosis , no rashes . [SEP]',\n",
       " '[CLS] Mild edema of the lower extremities without [entity] clubbing [entity] or cyanosis , no rashes . [SEP]',\n",
       " '[CLS] Mild edema of the lower extremities without clubbing or [entity] cyanosis [entity] , no rashes . [SEP]',\n",
       " '[CLS] Mild edema of the lower extremities without clubbing or cyanosis , no [entity] rashes [entity] . [SEP]',\n",
       " '[CLS] There is [entity] a right hand hematoma [entity] . [SEP]',\n",
       " '[CLS] Chest x - ray demonstrates [entity] a moderate sized hiatal hernia [entity] , segmental atelectasis , left lower lobe infiltrate versus segmental atelectasis . [SEP]',\n",
       " '[CLS] Chest x - ray demonstrates a moderate sized hiatal hernia , [entity] segmental atelectasis [entity] , left lower lobe infiltrate versus segmental atelectasis . [SEP]',\n",
       " '[CLS] Chest x - ray demonstrates a moderate sized hiatal hernia , segmental atelectasis , [entity] left lower lobe infiltrate [entity] versus segmental atelectasis . [SEP]',\n",
       " '[CLS] Chest x - ray demonstrates a moderate sized hiatal hernia , segmental atelectasis , left lower lobe infiltrate versus [entity] segmental atelectasis [entity] . [SEP]',\n",
       " '[CLS] EKG shows normal sinus rhythm at 113 beats per minute , normal axis , no evidence of [entity] ST - T wave changes [entity] . [SEP]',\n",
       " '[CLS] BRIEF SUMMARY OF HOSPITAL COURSE : 1 . [entity] COPD [entity] / dyspnea / pneumonia : [SEP]',\n",
       " '[CLS] BRIEF SUMMARY OF HOSPITAL COURSE : 1 . COPD / [entity] dyspnea [entity] / pneumonia : [SEP]',\n",
       " '[CLS] BRIEF SUMMARY OF HOSPITAL COURSE : 1 . COPD / dyspnea / [entity] pneumonia [entity] : [SEP]',\n",
       " '[CLS] The patient was initially placed on an aggressive steroid taper and admitted to the Medical Intensive Care Unit due to [entity] her difficulty with oxygenation [entity] despite CPAP machine . [SEP]',\n",
       " '[CLS] as well as [entity] chest [entity] PT . [SEP]',\n",
       " '[CLS] due to the fact that she continued to have [entity] labored breathing [entity] . [SEP]',\n",
       " '[CLS] Due to [entity] persistent respiratory failure [entity] and labored breathing , the patient was intubated on [ * * 2118 - 6 - 7 * * ] in order to improve oxygenation , ventilation , and ability to suction . [SEP]',\n",
       " '[CLS] Due to persistent respiratory failure and [entity] labored breathing [entity] , the patient was intubated on [ * * 2118 - 6 - 7 * * ] in order to improve oxygenation , ventilation , and ability to suction . [SEP]',\n",
       " '[CLS] A bronchoscopy was performed on [ * * 2118 - 6 - 7 * * ] , which demonstrated [entity] marked narrowing of the airways [entity] with expiration consistent with tracheomalacia . [SEP]',\n",
       " '[CLS] A bronchoscopy was performed on [ * * 2118 - 6 - 7 * * ] , which demonstrated marked narrowing of the airways with expiration consistent with [entity] tracheomalacia [entity] . [SEP]',\n",
       " '[CLS] The patient was ruled out for [entity] a MI [entity] . [SEP]',\n",
       " '[CLS] She did have another episode on the medical floor of [entity] chest pain [entity] , which showed no evidence of EKG changes and negative troponin , negative CKs x3 . [SEP]',\n",
       " '[CLS] She did have another episode on the medical floor of chest pain , which showed no evidence of [entity] EKG [entity] changes and negative troponin , negative CKs x3 . [SEP]',\n",
       " '[CLS] 3 . [entity] Hypertension [entity] : [SEP]',\n",
       " '[CLS] The patient had [entity] intermittent hematuria [entity] likely secondary to Foley placement . [SEP]',\n",
       " '[CLS] She had [entity] serial urinalyses [entity] , which were all negative for signs of infection . [SEP]',\n",
       " '[CLS] She had serial urinalyses , which were all negative for signs of [entity] infection [entity] . [SEP]',\n",
       " '[CLS] 5 . [entity] Hyperglycemia [entity] : Patient was placed on insulin - sliding scale due to hyperglycemia , which was steroid induced . [SEP]',\n",
       " '[CLS] 5 . Hyperglycemia : Patient was placed on insulin - sliding scale due to [entity] hyperglycemia [entity] , which was steroid induced . [SEP]',\n",
       " '[CLS] 6 . [entity] Leukocytosis [entity] : Patient did have a profound leukocytosis of 20 to 22 during much of her hospital course . [SEP]',\n",
       " '[CLS] 6 . Leukocytosis : Patient did have [entity] a profound leukocytosis [entity] of 20 to 22 during much of her hospital course . [SEP]',\n",
       " '[CLS] It was felt that [entity] the leukocytosis [entity] was secondary to both steroids as well as question of a left lower lobe pneumonia . [SEP]',\n",
       " '[CLS] It was felt that the leukocytosis was secondary to both steroids as well as question of [entity] a left lower lobe pneumonia [entity] . [SEP]',\n",
       " '[CLS] 7 . For [entity] the left lower lobe pneumonia [entity] , the patient had initially received a course of levofloxacin 500 p . o . q . d . from [ * * 2118 - 6 - 4 * * ] to [ * * 2118 - 6 - 10 * * ] . [SEP]',\n",
       " '[CLS] This was restarted on [ * * 2118 - 6 - 12 * * ] for an additional seven day course given the fact that she still had [entity] the leukocytosis [entity] and still had marked rales at the left lower lobe . [SEP]',\n",
       " '[CLS] This was restarted on [ * * 2118 - 6 - 12 * * ] for an additional seven day course given the fact that she still had the leukocytosis and still had [entity] marked rales at the left lower lobe [entity] . [SEP]',\n",
       " '[CLS] 8 . [entity] Hypothyroidism [entity] : [SEP]',\n",
       " '[CLS] 9 . [entity] Depression [entity] : [SEP]',\n",
       " '[CLS] It is recommended that she follow up with a therapist as an outpatient due to the fact that she did have [entity] a blunted affect [entity] throughout much of the hospital course , and did appear clinically to be depressed . [SEP]',\n",
       " '[CLS] It is recommended that she follow up with a therapist as an outpatient due to the fact that she did have a blunted affect throughout much of the hospital course , and did appear clinically to be [entity] depressed [entity] . [SEP]',\n",
       " '[CLS] 10 . [entity] Prophylaxis [entity] : [SEP]',\n",
       " '[CLS] The patient did have [entity] a sore throat [entity] for much of the hospital course post extubation . [SEP]',\n",
       " '[CLS] 13 . [entity] Muscle weakness [entity] : [SEP]',\n",
       " '[CLS] The patient did have [entity] profound muscle weakness [entity] and was evaluated by Physical Therapy , and was found to have impaired functional mobility , impaired musculoskeletal performance , impaired gas exchange , impaired endurance , impaired ventilation , and needed help with supine to sit . [SEP]',\n",
       " '[CLS] The patient did have profound muscle weakness and was evaluated by Physical Therapy , and was found to have [entity] impaired functional mobility [entity] , impaired musculoskeletal performance , impaired gas exchange , impaired endurance , impaired ventilation , and needed help with supine to sit . [SEP]',\n",
       " '[CLS] The patient did have profound muscle weakness and was evaluated by Physical Therapy , and was found to have impaired functional mobility , [entity] impaired musculoskeletal performance [entity] , impaired gas exchange , impaired endurance , impaired ventilation , and needed help with supine to sit . [SEP]',\n",
       " '[CLS] The patient did have profound muscle weakness and was evaluated by Physical Therapy , and was found to have impaired functional mobility , impaired musculoskeletal performance , [entity] impaired gas exchange [entity] , impaired endurance , impaired ventilation , and needed help with supine to sit . [SEP]',\n",
       " '[CLS] The patient did have profound muscle weakness and was evaluated by Physical Therapy , and was found to have impaired functional mobility , impaired musculoskeletal performance , impaired gas exchange , [entity] impaired endurance [entity] , impaired ventilation , and needed help with supine to sit . [SEP]',\n",
       " '[CLS] The patient did have profound muscle weakness and was evaluated by Physical Therapy , and was found to have impaired functional mobility , impaired musculoskeletal performance , impaired gas exchange , impaired endurance , [entity] impaired ventilation [entity] , and needed help with supine to sit . [SEP]',\n",
       " '[CLS] It was felt that [entity] this weakness [entity] was most likely due to a combination of steroid myopathy as well as muscle atrophy secondary to deconditioning after a prolonged hospital course . [SEP]',\n",
       " '[CLS] It was felt that this weakness was most likely due to a combination of [entity] steroid myopathy [entity] as well as muscle atrophy secondary to deconditioning after a prolonged hospital course . [SEP]',\n",
       " '[CLS] It was felt that this weakness was most likely due to a combination of steroid myopathy as well as [entity] muscle atrophy [entity] secondary to deconditioning after a prolonged hospital course . [SEP]',\n",
       " '[CLS] It was felt that this weakness was most likely due to a combination of steroid myopathy as well as muscle atrophy secondary to [entity] deconditioning [entity] after a prolonged hospital course . [SEP]',\n",
       " '[CLS] The patient had a Speech and Swallow evaluation showing no evidence of [entity] dysphagia [entity] , no evidence of vocal cord damage status post tracheal stent placement . [SEP]',\n",
       " '[CLS] The patient had a Speech and Swallow evaluation showing no evidence of dysphagia , no evidence of [entity] vocal cord damage [entity] status post tracheal stent placement . [SEP]',\n",
       " '[CLS] She was [entity] profoundly weak [entity] , but was no longer tachycardic and had a normal blood pressure . [SEP]',\n",
       " '[CLS] She was profoundly weak , but was no longer [entity] tachycardic [entity] and had a normal blood pressure . [SEP]',\n",
       " '[CLS] Her respirations were much improved albeit with [entity] transmitted upper airway [entity] sounds . [SEP]',\n",
       " '[CLS] 17 . Kaopectate / Benadryl / lidocaine 5 mL p . o . b . i . d . prn , not to be given around mealtimes for concern of [entity] dysphagia [entity] induced by lidocaine . [SEP]',\n",
       " '[CLS] She is also recommended to followup with a neurologist if [entity] her muscle weakness [entity] does not improve within one week on physical therapy with concern for steroid - induced myopathy . [SEP]',\n",
       " '[CLS] She is also recommended to followup with a neurologist if her muscle weakness does not improve within one week on physical therapy with concern for [entity] steroid - induced myopathy [entity] . [SEP]',\n",
       " '[CLS] FINAL DIAGNOSES : 1 . [entity] Tracheomalacia [entity] status post tracheal and left main stem bronchial stent placement . [SEP]',\n",
       " '[CLS] 2 . [entity] Hypertension [entity] . [SEP]',\n",
       " '[CLS] 3 . [entity] Hypothyroidism [entity] . [SEP]',\n",
       " '[CLS] 4 . [entity] Restrictive lung defect [entity] . [SEP]',\n",
       " '[CLS] 5 . [entity] Depression [entity] . [SEP]']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_with_problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer, BertForSequenceClassification\n",
    "# no of classifier: present, not-present\n",
    "num_labels = 3\n",
    "MODEL_CLASSES = {\n",
    "  'bert': (AutoConfig, BertForSequenceClassification, AutoTokenizer),\n",
    "}\n",
    "MODEL_ADDRESS = 'emilyalsentzer/Bio_ClinicalBERT'\n",
    "config_class, model_class, tokenizer_class = MODEL_CLASSES['bert']\n",
    "model_config = config_class.from_pretrained(MODEL_ADDRESS, num_labels=num_labels)\n",
    "tokenizer = tokenizer_class.from_pretrained(MODEL_ADDRESS, do_lower_case=False)\n",
    "model = model_class.from_pretrained(MODEL_ADDRESS, config=model_config)\n",
    "output_dir = '../trained_models/Assertion/3_lable_model'\n",
    "model = model_class.from_pretrained(output_dir)\n",
    "tokenizer = tokenizer_class.from_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "# For every sentence...\n",
    "for sent in sentences_with_problem:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = False, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 128,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "# labels = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-1be4e8e87b42>:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = torch.tensor(input_ids)\n",
      "<ipython-input-19-1be4e8e87b42>:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_masks = torch.tensor(attention_masks)\n"
     ]
    }
   ],
   "source": [
    "input_ids = torch.tensor(input_ids)\n",
    "attention_masks = torch.tensor(attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions , true_labels = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    result = model(input_ids, token_type_ids=None, attention_mask=attention_masks, return_dict=True)\n",
    "\n",
    "logits = result.logits\n",
    "logits = logits.detach().cpu().numpy()\n",
    "predictions.append(logits)\n",
    "\n",
    "# print('sentences: ', sentences)\n",
    "pred_labels_i = np.argmax(logits, axis=1).flatten()\n",
    "# print('Label prediction: ', pred_labels_i) \n",
    "\n",
    "for index, sentence in enumerate(sentences_with_problem):\n",
    "  print(sentence)\n",
    "  print(pred_labels_i[index])\n",
    "  if pred_labels_i[index] == 0:\n",
    "    print ('Present')\n",
    "  elif pred_labels_i[index] == 1:\n",
    "    print ('Possible')\n",
    "  elif pred_labels_i[index] == 2:\n",
    "    print ('Not-present')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 2, 2, 2, 2, 0, 1, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 2, 2, 2,\n",
       "       0, 0, 0, 2, 2, 2, 2, 0, 2, 2, 2, 0, 0, 0, 1, 1, 2, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 2, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_labels_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_text = [['He',\n",
    " 'was',\n",
    " 'admitted',\n",
    " ',',\n",
    " 'taken',\n",
    " 'to',\n",
    " 'the',\n",
    " 'operating',\n",
    " 'room',\n",
    " 'where',\n",
    " 'he',\n",
    " 'underwent',\n",
    " 'L5-S1',\n",
    " 'right',\n",
    " 'hemilaminectomy',\n",
    " 'and',\n",
    " 'discectomy',\n",
    " '.']\n",
    ", \n",
    "['Over',\n",
    " 'the',\n",
    " 'next',\n",
    " 'three',\n",
    " 'days',\n",
    " 'he',\n",
    " 'increased',\n",
    " 'his',\n",
    " 'activity',\n",
    " 'gradually',\n",
    " ',',\n",
    " 'was',\n",
    " 'able',\n",
    " 'to',\n",
    " 'do',\n",
    " 'stairs',\n",
    " 'with',\n",
    " 'Physical',\n",
    " 'Therapy',\n",
    " 'and',\n",
    " 'had',\n",
    " 'pain',\n",
    " 'which',\n",
    " 'could',\n",
    " 'be',\n",
    " 'controlled',\n",
    " 'with',\n",
    " 'oral',\n",
    " 'analgesics',\n",
    " '.'],\n",
    "\n",
    "['However', ',', 'her', 'creatinine', 'continued', 'to', 'increase', '.']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_list = [[[4, 5, 6], [10]], '', [[2, 3], [6,7]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [['yes', 'no'], '', ['yes', 'yes']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "ss = clinical_text[0]\n",
    "pp = problem_list[1]\n",
    "ll = labels[0]\n",
    "index_in_sentence = []\n",
    "for i, index in enumerate(pp):\n",
    "    n = len(index)\n",
    "    index_in_sentence.append([ll[i]] * n)\n",
    "print(index_in_sentence)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<map at 0x7fc357de2d60>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = map(lambda l: [item for elem in l for item in elem], problem_list)\n",
    "print(list(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "flatList = [ item for elem in pp for item in elem]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 5]\n",
      "=====\n",
      "He was admitted , [entity] taken to [entity] the operating room where he underwent L5-S1 right hemilaminectomy and discectomy .\n",
      "[8]\n",
      "=====\n",
      "He was admitted , taken to the operating [entity] room [entity] where he underwent L5-S1 right hemilaminectomy and discectomy .\n",
      "[2, 3]\n",
      "=====\n",
      "However , [entity] her creatinine [entity] continued to increase .\n",
      "[6, 7]\n",
      "=====\n",
      "However , her creatinine continued to [entity] increase . [entity]\n"
     ]
    }
   ],
   "source": [
    "for sentence, problem_index in zip(clinical_text, problem_list):\n",
    "    # print(sentence)\n",
    "    if problem_index:\n",
    "        for i_list in problem_index:\n",
    "            s = copy.deepcopy(sentence)\n",
    "            print(i_list)\n",
    "            print(\"=====\")\n",
    "            # print(i_list)\n",
    "            s.insert(int(i_list[-1])+1, '[entity]')\n",
    "            s.insert(int(i_list[0]), '[entity]')\n",
    "            s = ' '.join(s)\n",
    "            print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['He',\n",
       "  'was',\n",
       "  'admitted',\n",
       "  ',',\n",
       "  '[entity]',\n",
       "  'taken',\n",
       "  'to',\n",
       "  '[entity]',\n",
       "  '[entity]',\n",
       "  'the',\n",
       "  '[entity]',\n",
       "  'operating',\n",
       "  'room',\n",
       "  'where',\n",
       "  'he',\n",
       "  'underwent',\n",
       "  'L5-S1',\n",
       "  'right',\n",
       "  'hemilaminectomy',\n",
       "  'and',\n",
       "  'discectomy',\n",
       "  '.'],\n",
       " ['Over',\n",
       "  'the',\n",
       "  'next',\n",
       "  'three',\n",
       "  'days',\n",
       "  'he',\n",
       "  'increased',\n",
       "  'his',\n",
       "  'activity',\n",
       "  'gradually',\n",
       "  ',',\n",
       "  'was',\n",
       "  'able',\n",
       "  'to',\n",
       "  'do',\n",
       "  'stairs',\n",
       "  'with',\n",
       "  'Physical',\n",
       "  'Therapy',\n",
       "  'and',\n",
       "  'had',\n",
       "  'pain',\n",
       "  'which',\n",
       "  'could',\n",
       "  'be',\n",
       "  'controlled',\n",
       "  'with',\n",
       "  'oral',\n",
       "  'analgesics',\n",
       "  '.'],\n",
       " ['However', ',', 'her', 'creatinine', 'continued', 'to', 'increase', '.']]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clinical_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_tags = [['O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-treatment',\n",
    " 'I-treatment',\n",
    " 'I-treatment',\n",
    " 'O',\n",
    " 'B-treatment',\n",
    " 'O'],\n",
    "                 \n",
    " ['O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-treatment',\n",
    " 'I-treatment',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-problem',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-treatment',\n",
    " 'I-treatment',\n",
    " 'O'],\n",
    " \n",
    " ['O', 'O', 'B-test', 'I-test', 'O', 'O', 'O', 'O']                \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_input_ids, query_input_tags, query_attention_masks = process_data(clinical_text, clinical_tags, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_inputs = torch.tensor(query_input_ids)\n",
    "query_tags = torch.tensor(query_input_tags)\n",
    "query_masks = torch.tensor(query_attention_masks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluation(input_ids, label_ids, input_mask):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, token_type_ids=None,\n",
    "        attention_mask=input_mask)\n",
    "        # For eval mode, the first result of outputs is logits\n",
    "        logits = outputs[0] \n",
    "    # Get NER predict result\n",
    "    logits = torch.argmax(F.log_softmax(logits, dim=2), dim=2)\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    # Get NER true result\n",
    "    label_ids = label_ids.to('cpu').numpy()\n",
    "    # Only predict the real word, mark=0, will not calculate\n",
    "    input_mask = input_mask.to('cpu').numpy()\n",
    "    # Compare the valuable predict result\n",
    "    for i, mask in enumerate(input_mask):\n",
    "        # Real one\n",
    "        temp_1 = []\n",
    "        # Predict one\n",
    "        temp_2 = []\n",
    "        for j, m in enumerate(mask):\n",
    "            # Mark=0, meaning its a pad word, dont compare\n",
    "            if m:\n",
    "                if tag2name[label_ids[i][j]] != \"X\" and tag2name[label_ids[i][j]] != \"[CLS]\" and tag2name[label_ids[i][j]] != \"[SEP]\" : # Exclude the X label\n",
    "                    # print(tag2name[logits[i][j]])\n",
    "                    temp_1.append(tag2name[label_ids[i][j]])\n",
    "                    temp_2.append(tag2name[logits[i][j]])\n",
    "            else:\n",
    "                break\n",
    "        y_true.append(temp_1)\n",
    "        y_pred.append(temp_2)\n",
    "\n",
    "    return y_true, y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred = model_evaluation(query_inputs, query_tags, query_masks)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_true, y_pred, digits=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Eval results *****\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     problem     1.0000    1.0000    1.0000         1\n",
      "        test     1.0000    1.0000    1.0000         1\n",
      "   treatment     1.0000    1.0000    1.0000         4\n",
      "\n",
      "   micro avg     1.0000    1.0000    1.0000         6\n",
      "   macro avg     1.0000    1.0000    1.0000         6\n",
      "weighted avg     1.0000    1.0000    1.0000         6\n",
      "\n",
      "f1 socre: 1.000000\n",
      "Accuracy score: 1.000000\n"
     ]
    }
   ],
   "source": [
    "# Get acc , recall, F1 result report\n",
    "report = classification_report(y_true, y_pred, digits=4)\n",
    "\n",
    "# Save the report into file\n",
    "output_eval_file = \"eval_results.txt\"\n",
    "\n",
    "with open(output_eval_file, \"w\") as writer:\n",
    "    print(\"***** Eval results *****\")\n",
    "    print(\"\\n%s\"%(report))\n",
    "    print(\"f1 socre: %f\"%(f1_score(y_true, y_pred)))\n",
    "    print(\"Accuracy score: %f\"%(accuracy_score(y_true, y_pred)))\n",
    "    \n",
    "    writer.write(\"f1 socre:\\n\")\n",
    "    writer.write(str(f1_score(y_true, y_pred)))\n",
    "    writer.write(\"\\n\\nAccuracy score:\\n\")\n",
    "    writer.write(str(accuracy_score(y_true, y_pred)))\n",
    "    writer.write(\"\\n\\n\")  \n",
    "    writer.write(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['              precision    recall  f1-score   support',\n",
       " '',\n",
       " '     problem     1.0000    1.0000    1.0000         1',\n",
       " '        test     1.0000    1.0000    1.0000         1',\n",
       " '   treatment     1.0000    1.0000    1.0000         4',\n",
       " '',\n",
       " '   micro avg     1.0000    1.0000    1.0000         6',\n",
       " '   macro avg     1.0000    1.0000    1.0000         6',\n",
       " 'weighted avg     1.0000    1.0000    1.0000         6',\n",
       " '']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForTokenClassification.from_pretrained(bert_out_address, num_labels=len(tag2idx))\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_out_address, do_lower_case=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = ' '.join(test_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Over the next three days he increased his activity gradually , was able to do stairs with Physical Therapy and had pain which could be controlled with oral analgesics .'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts = []\n",
    "temp_token = []\n",
    "temp_token.append('[CLS]')\n",
    "token_list = tokenizer.tokenize(test_query)\n",
    "temp_token.extend(token_list)\n",
    "temp_token = temp_token[:max_len-1]\n",
    "temp_token.append('[SEP]')\n",
    "input_id = tokenizer.convert_tokens_to_ids(temp_token)\n",
    "padding_len = max_len - len(input_id)\n",
    "input_id = input_id + ([0] * padding_len)\n",
    "tokenized_texts = []\n",
    "tokenized_texts.append(input_id)\n",
    "attention_masks = [[int(i>0) for i in input_id]]\n",
    "\n",
    "tokenized_texts = torch.tensor(tokenized_texts)\n",
    "attention_masks = torch.tensor(attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set save model to Evalue loop\n",
    "model.eval()\n",
    "# Get model predict result\n",
    "with torch.no_grad():\n",
    "        outputs = model(tokenized_texts, token_type_ids=None,\n",
    "        attention_mask=None,)\n",
    "        # For eval mode, the first result of outputs is logits\n",
    "        logits = outputs[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_results = logits.detach().cpu().numpy()\n",
    "result_arrays_soft = softmax(predict_results[0])\n",
    "result_list = np.argmax(result_arrays_soft,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [tag2name[t] for t in result_list]\n",
    "pretok_sent = \"\"\n",
    "pretags = \"\"\n",
    "for i, tok in enumerate(temp_token):\n",
    "     if tok.startswith(\"##\"):\n",
    "         pretok_sent += tok[2:]\n",
    "     else:\n",
    "         pretok_sent += \" \" + tok\n",
    "         pretags += \" \" + result[i]\n",
    "pretok_sent = pretok_sent[1:]\n",
    "pretags = pretags[1:]\n",
    "\n",
    "s = pretok_sent.split()\n",
    "t = pretags.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token:[CLS]\n",
      "Predict_Tag:[CLS]\n",
      "\n",
      "Token:Over\n",
      "Predict_Tag:O\n",
      "\n",
      "Token:the\n",
      "Predict_Tag:O\n",
      "\n",
      "Token:next\n",
      "Predict_Tag:O\n",
      "\n",
      "Token:three\n",
      "Predict_Tag:O\n",
      "\n",
      "Token:days\n",
      "Predict_Tag:O\n",
      "\n",
      "Token:he\n",
      "Predict_Tag:O\n",
      "\n",
      "Token:increased\n",
      "Predict_Tag:O\n",
      "\n",
      "Token:his\n",
      "Predict_Tag:O\n",
      "\n",
      "Token:activity\n",
      "Predict_Tag:O\n",
      "\n",
      "Token:gradually\n",
      "Predict_Tag:O\n",
      "\n",
      "Token:,\n",
      "Predict_Tag:O\n",
      "\n",
      "Token:was\n",
      "Predict_Tag:O\n",
      "\n",
      "Token:able\n",
      "Predict_Tag:O\n",
      "\n",
      "Token:to\n",
      "Predict_Tag:O\n",
      "\n",
      "Token:do\n",
      "Predict_Tag:O\n",
      "\n",
      "Token:stairs\n",
      "Predict_Tag:O\n",
      "\n",
      "Token:with\n",
      "Predict_Tag:O\n",
      "\n",
      "Token:Physical\n",
      "Predict_Tag:B-treatment\n",
      "\n",
      "Token:Therapy\n",
      "Predict_Tag:I-treatment\n",
      "\n",
      "Token:and\n",
      "Predict_Tag:O\n",
      "\n",
      "Token:had\n",
      "Predict_Tag:O\n",
      "\n",
      "Token:pain\n",
      "Predict_Tag:B-problem\n",
      "\n",
      "Token:which\n",
      "Predict_Tag:O\n",
      "\n",
      "Token:could\n",
      "Predict_Tag:O\n",
      "\n",
      "Token:be\n",
      "Predict_Tag:O\n",
      "\n",
      "Token:controlled\n",
      "Predict_Tag:O\n",
      "\n",
      "Token:with\n",
      "Predict_Tag:O\n",
      "\n",
      "Token:oral\n",
      "Predict_Tag:B-treatment\n",
      "\n",
      "Token:anal\n",
      "Predict_Tag:I-treatment\n",
      "\n",
      "Token:##ges\n",
      "Predict_Tag:X\n",
      "\n",
      "Token:##ics\n",
      "Predict_Tag:X\n",
      "\n",
      "Token:.\n",
      "Predict_Tag:O\n",
      "\n",
      "Token:[SEP]\n",
      "Predict_Tag:[SEP]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, mark in enumerate(attention_masks[0]):\n",
    "    if mark>0:\n",
    "        print(\"Token:%s\"%(temp_token[i]))\n",
    "#         print(\"Tag:%s\"%(result_list[i]))\n",
    "        print(\"Predict_Tag:%s\"%(tag2name[result_list[i]]))\n",
    "        #print(\"Posibility:%f\"%(result_array[i][result_list[i]]))\n",
    "        print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ' '.join(temp_token)\n",
    "re = ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "from docx.enum.text import WD_COLOR_INDEX\n",
    "# Create an instance of a word document\n",
    "doc = docx.Document()\n",
    "\n",
    "# Add a Title to the document \n",
    "doc.add_heading('Results', 0)\n",
    "\n",
    "# Creating paragraph with some content\n",
    "para = doc.add_paragraph(''' ''')\n",
    "  \n",
    "flag_treatment, flag_problem, flag_test = 0, 0, 0 \n",
    "for i in range(1, len(t)-1):\n",
    "    if t[i] == 'B-treatment':\n",
    "        flag_treatment = 1\n",
    "        para.add_run(s[i]+' ').font.highlight_color = WD_COLOR_INDEX.RED\n",
    "        # print(print_treatment(s[i]), end=' ')\n",
    "    elif (t[i] == 'I-treatment' or t[i] == 'X') and flag_treatment == 1 :\n",
    "        para.add_run(s[i]+' ').font.highlight_color = WD_COLOR_INDEX.RED\n",
    "    elif t[i] == 'B-test':\n",
    "        flag_test = 1\n",
    "        para.add_run(s[i]+' ').font.highlight_color = WD_COLOR_INDEX.PINK\n",
    "    elif (t[i] == 'I-test' or t[i] == 'X') and flag_test == 1 :\n",
    "        para.add_run(s[i]+' ').font.highlight_color = WD_COLOR_INDEX.PINK\n",
    "    elif t[i] == 'B-problem':\n",
    "        flag_problem = 1\n",
    "        para.add_run(s[i]+' ').font.highlight_color = WD_COLOR_INDEX.TURQUOISE\n",
    "    elif (t[i] == 'I-problem' or t[i] == 'X') and flag_problem == 1 :\n",
    "        para.add_run(s[i]+' ').font.highlight_color = WD_COLOR_INDEX.TURQUOISE    \n",
    "    elif t[i] == 'O':\n",
    "        flag_treatment, flag_problem, flag_test = 0, 0, 0 \n",
    "        para.add_run(s[i]+' ').font.highlight_color = WD_COLOR_INDEX.AUTO\n",
    "        \n",
    "\n",
    "# # Adding more content to paragraph and highlighting them\n",
    "# para.add_run(''' It contains well written, well thought and well-explained '''\n",
    "#             ).font.highlight_color = WD_COLOR_INDEX.YELLOW\n",
    "  \n",
    "# # Adding more content to paragraph\n",
    "# para.add_run('''computer science and programming articles, quizzes etc.''')\n",
    "  \n",
    "# Now save the document to a location \n",
    "doc.save('result.docx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from termcolor import colored\n",
    "def print_treatment(word):\n",
    "    return colored(word,'white','on_red', attrs=['underline'])\n",
    "def print_test(word):\n",
    "    return colored(word, 'white','on_magenta')\n",
    "def print_problem(word):\n",
    "    return colored(word, 'magenta','on_cyan', attrs=['reverse', 'bold'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[7m\u001b[46m\u001b[35mproblem\u001b[0m\n",
      "\u001b[4m\u001b[41m\u001b[37mtreatment\u001b[0m\n",
      "\u001b[45m\u001b[37mtest\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(print_problem('problem'))\n",
    "print(print_treatment('treatment'))\n",
    "print(print_test('test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over the next three days he increased his activity gradually , was able to do stairs with \u001b[4m\u001b[41m\u001b[37mPhysical\u001b[0m \u001b[4m\u001b[41m\u001b[37mTherapy\u001b[0m and had \u001b[1m\u001b[7m\u001b[46m\u001b[35mpain\u001b[0m which could be controlled with \u001b[4m\u001b[41m\u001b[37moral\u001b[0m \u001b[4m\u001b[41m\u001b[37manalgesics\u001b[0m . "
     ]
    }
   ],
   "source": [
    "from termcolor import colored\n",
    "location = {}\n",
    "flag_treatment, flag_problem, flag_test = 0, 0, 0 \n",
    "for i in range(1, len(t)-1):\n",
    "    if t[i] == 'B-treatment':\n",
    "        flag_treatment = 1\n",
    "        print(print_treatment(s[i]), end=' ')\n",
    "    elif (t[i] == 'I-treatment' or t[i] == 'X') and flag_treatment == 1 :\n",
    "        print(print_treatment(s[i]), end=' ')\n",
    "    elif t[i] == 'B-test':\n",
    "        flag_test = 1\n",
    "        print(print_test(s[i]), end=' ')\n",
    "    elif (t[i] == 'I-test' or t[i] == 'X') and flag_test == 1 :\n",
    "        print(print_test(s[i]), end=' ')\n",
    "    elif t[i] == 'B-problem':\n",
    "        flag_problem = 1\n",
    "        print(print_problem(s[i]), end=' ')\n",
    "    elif (t[i] == 'I-problem' or t[i] == 'X') and flag_problem == 1 :\n",
    "        print(print_problem(s[i]), end=' ')    \n",
    "    elif t[i] == 'O':\n",
    "        flag_treatment, flag_problem, flag_test = 0, 0, 0 \n",
    "        print(s[i], end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_lable = []\n",
    "temp_token = []\n",
    "\n",
    "# Add [CLS] at the front \n",
    "temp_lable.append('[CLS]')\n",
    "temp_token.append('[CLS]')\n",
    "\n",
    "# Tokenize words\n",
    "# [lidocaine patch] -> lid ##oc ##aine patch\n",
    "# [B-treatment I-treatment] -> B-treatment X X I-treatment\n",
    "for word, lab in zip(word_list, label):\n",
    "    token_list = tokenizer.tokenize(word)\n",
    "    for m, token in enumerate(token_list):\n",
    "        temp_token.append(token)\n",
    "        if m == 0:\n",
    "            temp_lable.append(lab)\n",
    "        else:\n",
    "            temp_lable.append('X')  \n",
    "            \n",
    "# Add [SEP] at the end\n",
    "temp_lable.append('[SEP]')\n",
    "temp_token.append('[SEP]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object) :\n",
    "    \"\"\"\n",
    "    input a dataframe\n",
    "    \n",
    "    \n",
    "    Generate sets of words and tags.\n",
    "    self.sentence:\n",
    "    Each sentence is a list\n",
    "    [('Supraventricular', 'B-problem'),\n",
    "    ('tachycardia', 'I-problem'),\n",
    "    ('(', 'O'),\n",
    "    ('on', 'O'),\n",
    "    ('a', 'B-treatment'),\n",
    "    ('beta', 'I-treatment'),\n",
    "    ('blocker', 'I-treatment'),\n",
    "    (')', 'O')]\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, t) for w, t in zip(s[\"word\"].values.tolist(),\n",
    "                                                        #    s[\"POS\"].values.tolist(),\n",
    "                                                           s[\"tag\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"sentence #\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def get_examples(self):\n",
    "        return random.sample(self.sentences, 10)\n",
    "    \n",
    "    def get_sentences(self):\n",
    "        return[[s[0] for s in sent] for sent in self.sentences]\n",
    "        \n",
    "    def get_labels(self):\n",
    "        return [[s[1] for s in sent] for sent in self.sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputGenerater(object):\n",
    "    def __init__(self, sentences, labels) -> None:\n",
    "        self.tokenized_texts = []\n",
    "        self.word_piece_labels = []\n",
    "        i_inc = 0\n",
    "        for word_list,label in (zip(sentences,labels)):\n",
    "            temp_lable = []\n",
    "            temp_token = []\n",
    "            \n",
    "            # Add [CLS] at the front \n",
    "            temp_lable.append('[CLS]')\n",
    "            temp_token.append('[CLS]')\n",
    "            \n",
    "            for word,lab in zip(word_list,label):\n",
    "                token_list = tokenizer.tokenize(word)\n",
    "                for m,token in enumerate(token_list):\n",
    "                    temp_token.append(token)\n",
    "                    if m==0:\n",
    "                        temp_lable.append(lab)\n",
    "                    else:\n",
    "                        temp_lable.append('X')  \n",
    "                        \n",
    "            # Add [SEP] at the end\n",
    "            temp_lable.append('[SEP]')\n",
    "            temp_token.append('[SEP]')\n",
    "            \n",
    "            self.tokenized_texts.append(temp_token)\n",
    "            self.word_piece_labels.append(temp_lable)\n",
    "            \n",
    "            if 5 > i_inc:\n",
    "                print(\"No.%d,len:%d\"%(i_inc,len(temp_token)))\n",
    "                print(\"texts:%s\"%(\" \".join(temp_token)))\n",
    "                print(\"No.%d,len:%d\"%(i_inc,len(temp_lable)))\n",
    "                print(\"lables:%s\"%(\" \".join(temp_lable)))\n",
    "            i_inc +=1\n",
    "            \n",
    "        self.input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in self.tokenized_texts],\n",
    "                    maxlen=config.MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "            \n",
    "    def get_input_ids(self):    \n",
    "        return self.input_ids\n",
    "    \n",
    "    def get_tags(self):\n",
    "        tags = pad_sequences([[get_tag2idx().get(l) for l in lab] for lab in self.word_piece_labels],\n",
    "                        maxlen=config.MAX_LEN, value=get_tag2idx()[\"O\"], padding=\"post\",\n",
    "                        dtype=\"long\", truncating=\"post\")\n",
    "        return tags\n",
    "    def get_attention_masks(self):\n",
    "        attention_masks = [[int(i>0) for i in ii] for ii in self.input_ids]\n",
    "        return attention_masks\n",
    "    def get_segment_ids(self):\n",
    "        segment_ids = [[0] * len(input_id) for input_id in self.input_ids]\n",
    "        return segment_ids\n",
    "\n",
    "\n",
    "def convert_to_tensor(*inputs, drop_last=False):\n",
    "    data = TensorDataset(*tuple(torch.tensor(inputs)))\n",
    "    data_sampler = RandomSampler(data)\n",
    "    # Drop last can make batch training better for the last one\n",
    "    dataloader = DataLoader(data, sampler=data_sampler, batch_size=config.BATCH_NUM, drop_last=drop_last)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.0,len:5\n",
      "texts:[CLS] admission date : [SEP]\n",
      "No.0,len:5\n",
      "lables:[CLS] O O O [SEP]\n",
      "No.1,len:7\n",
      "texts:[CLS] 2014 - 12 - 29 [SEP]\n",
      "No.1,len:7\n",
      "lables:[CLS] O X X X X [SEP]\n",
      "No.2,len:6\n",
      "texts:[CLS] all ##er ##gies : [SEP]\n",
      "No.2,len:6\n",
      "lables:[CLS] O X X O [SEP]\n",
      "No.3,len:4\n",
      "texts:[CLS] 17 units [SEP]\n",
      "No.3,len:4\n",
      "lables:[CLS] O O [SEP]\n",
      "No.4,len:22\n",
      "texts:[CLS] includes a history of at ##rial fi ##bri ##lla ##tion with good heart rate control on dig ##ox ##in . [SEP]\n",
      "No.4,len:22\n",
      "lables:[CLS] O O O O B-problem X I-problem X X X O O B-treatment I-treatment I-treatment O B-treatment X X O [SEP]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pad_sequences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-b054d79cb97e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtest_sets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInputGenerater\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtest_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_sets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-dec2e08486b9>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, labels)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mi_inc\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         self.input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in self.tokenized_texts],\n\u001b[0m\u001b[1;32m     38\u001b[0m                     maxlen=config.MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pad_sequences' is not defined"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv(config.data_path_test, sep=\"\\t\").astype(str)\n",
    "sg = SentenceGetter(df_test)\n",
    "sentences = sg.get_sentences()\n",
    "tags = sg.get_labels()\n",
    "test_sets = InputGenerater(sentences=sentences, labels=tags)\n",
    "\n",
    "test_inputs = test_sets.get_input_ids()\n",
    "test_tags = test_sets.get_tags()\n",
    "test_attetion_masks = test_sets.get_attention_masks()\n",
    "\n",
    "test_dataloader = utils.convert_to_tensor(test_inputs, test_attetion_masks, test_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = glob.glob('processed/test/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0289.tsv',\n",
       " '0090.tsv',\n",
       " '0246.tsv',\n",
       " '0390.tsv',\n",
       " '0365.tsv',\n",
       " '0174.tsv',\n",
       " '0282.tsv',\n",
       " '0305.tsv',\n",
       " '0150.tsv',\n",
       " '0101.tsv',\n",
       " '0086.tsv',\n",
       " '0357.tsv',\n",
       " '0230.tsv',\n",
       " '0294.tsv',\n",
       " '0377.tsv',\n",
       " '0266.tsv',\n",
       " '0050.tsv',\n",
       " '0026.tsv',\n",
       " '0049.tsv',\n",
       " '0029.tsv',\n",
       " '0005.tsv',\n",
       " '0081.tsv',\n",
       " '0466.tsv',\n",
       " '0245.tsv',\n",
       " '0378.tsv',\n",
       " '0463.tsv',\n",
       " '0445.tsv',\n",
       " '0053.tsv',\n",
       " '0477.tsv',\n",
       " '0461.tsv',\n",
       " '0362.tsv',\n",
       " '0473.tsv',\n",
       " '0309.tsv',\n",
       " '0415.tsv',\n",
       " '0222.tsv',\n",
       " '0329.tsv',\n",
       " '0474.tsv',\n",
       " '0393.tsv',\n",
       " '0345.tsv',\n",
       " '0109.tsv',\n",
       " '0129.tsv',\n",
       " '0454.tsv',\n",
       " '0366.tsv',\n",
       " '0082.tsv',\n",
       " '0439.tsv',\n",
       " '0173.tsv',\n",
       " '0094.tsv',\n",
       " '0025.tsv',\n",
       " '0270.tsv',\n",
       " '0133.tsv',\n",
       " '0261.tsv',\n",
       " '0153.tsv',\n",
       " '0237.tsv',\n",
       " '0074.tsv',\n",
       " '0046.tsv',\n",
       " '0066.tsv',\n",
       " '0037.tsv',\n",
       " '0338.tsv',\n",
       " '0285.tsv',\n",
       " '0425.tsv',\n",
       " '0322.tsv',\n",
       " '0385.tsv',\n",
       " '0446.tsv',\n",
       " '0141.tsv',\n",
       " '0138.tsv',\n",
       " '0121.tsv',\n",
       " '0421.tsv',\n",
       " '0389.tsv',\n",
       " '0186.tsv',\n",
       " '0233.tsv',\n",
       " '0134.tsv',\n",
       " '0054.tsv',\n",
       " '0013.tsv',\n",
       " '0190.tsv',\n",
       " '0185.tsv',\n",
       " '0409.tsv',\n",
       " '0205.tsv',\n",
       " '0442.tsv',\n",
       " '0433.tsv',\n",
       " '0321.tsv',\n",
       " '0374.tsv',\n",
       " '0342.tsv',\n",
       " '0465.tsv',\n",
       " '0009.tsv',\n",
       " '0229.tsv',\n",
       " '0437.tsv',\n",
       " '0427.tsv',\n",
       " '0318.tsv',\n",
       " '0073.tsv',\n",
       " '0214.tsv',\n",
       " '0431.tsv',\n",
       " '0333.tsv',\n",
       " '0405.tsv',\n",
       " '0457.tsv',\n",
       " '0310.tsv',\n",
       " '0460.tsv',\n",
       " '0146.tsv',\n",
       " '0041.tsv',\n",
       " '0234.tsv',\n",
       " '0193.tsv',\n",
       " '0269.tsv',\n",
       " '0102.tsv',\n",
       " '0349.tsv',\n",
       " '0301.tsv',\n",
       " '0394.tsv',\n",
       " '0386.tsv',\n",
       " '0238.tsv',\n",
       " '0157.tsv',\n",
       " '0061.tsv',\n",
       " '0428.tsv',\n",
       " '0117.tsv',\n",
       " '0078.tsv',\n",
       " '0262.tsv',\n",
       " '0464.tsv',\n",
       " '0468.tsv',\n",
       " '0249.tsv',\n",
       " '0265.tsv',\n",
       " '0470.tsv',\n",
       " '0225.tsv',\n",
       " '0369.tsv',\n",
       " '0302.tsv',\n",
       " '0058.tsv',\n",
       " '0105.tsv',\n",
       " '0197.tsv',\n",
       " '0118.tsv',\n",
       " '0217.tsv',\n",
       " '0346.tsv',\n",
       " '0209.tsv',\n",
       " '0113.tsv',\n",
       " '0242.tsv',\n",
       " '0142.tsv',\n",
       " '0410.tsv',\n",
       " '0449.tsv',\n",
       " '0034.tsv',\n",
       " '0475.tsv',\n",
       " '0354.tsv',\n",
       " '0382.tsv',\n",
       " '0337.tsv',\n",
       " '0436.tsv',\n",
       " '0169.tsv',\n",
       " '0430.tsv',\n",
       " '0448.tsv',\n",
       " '0154.tsv',\n",
       " '0202.tsv',\n",
       " '0093.tsv',\n",
       " '0070.tsv',\n",
       " '0210.tsv',\n",
       " '0341.tsv',\n",
       " '0325.tsv',\n",
       " '0440.tsv',\n",
       " '0401.tsv',\n",
       " '0194.tsv',\n",
       " '0434.tsv',\n",
       " '0182.tsv',\n",
       " '0181.tsv',\n",
       " '0398.tsv',\n",
       " '0122.tsv',\n",
       " '0033.tsv',\n",
       " '0002.tsv',\n",
       " '0277.tsv',\n",
       " '0257.tsv',\n",
       " '0361.tsv',\n",
       " '0038.tsv',\n",
       " '0014.tsv',\n",
       " '0158.tsv',\n",
       " '0198.tsv',\n",
       " '0290.tsv',\n",
       " '0418.tsv',\n",
       " '0467.tsv',\n",
       " '0201.tsv',\n",
       " '0334.tsv',\n",
       " '0057.tsv',\n",
       " '0126.tsv',\n",
       " '0443.tsv',\n",
       " '0042.tsv',\n",
       " '0098.tsv',\n",
       " '0451.tsv',\n",
       " '0177.tsv',\n",
       " '0125.tsv',\n",
       " '0189.tsv',\n",
       " '0476.tsv',\n",
       " '0018.tsv',\n",
       " '0472.tsv',\n",
       " '0166.tsv',\n",
       " '0001.tsv',\n",
       " '0022.tsv',\n",
       " '0462.tsv',\n",
       " '0298.tsv',\n",
       " '0258.tsv',\n",
       " '0254.tsv',\n",
       " '0370.tsv',\n",
       " '0458.tsv',\n",
       " '0030.tsv',\n",
       " '0062.tsv',\n",
       " '0213.tsv',\n",
       " '0422.tsv',\n",
       " '0170.tsv',\n",
       " '0226.tsv',\n",
       " '0162.tsv',\n",
       " '0416.tsv',\n",
       " '0278.tsv',\n",
       " '0273.tsv',\n",
       " '0306.tsv',\n",
       " '0419.tsv',\n",
       " '0314.tsv',\n",
       " '0297.tsv',\n",
       " '0381.tsv',\n",
       " '0161.tsv',\n",
       " '0085.tsv',\n",
       " '0069.tsv',\n",
       " '0250.tsv',\n",
       " '0350.tsv',\n",
       " '0281.tsv',\n",
       " '0317.tsv',\n",
       " '0130.tsv',\n",
       " '0218.tsv',\n",
       " '0110.tsv',\n",
       " '0241.tsv',\n",
       " '0413.tsv',\n",
       " '0424.tsv',\n",
       " '0330.tsv',\n",
       " '0326.tsv',\n",
       " '0402.tsv',\n",
       " '0397.tsv',\n",
       " '0178.tsv',\n",
       " '0114.tsv',\n",
       " '0358.tsv',\n",
       " '0045.tsv',\n",
       " '0286.tsv',\n",
       " '0137.tsv',\n",
       " '0006.tsv',\n",
       " '0106.tsv',\n",
       " '0373.tsv',\n",
       " '0471.tsv',\n",
       " '0149.tsv',\n",
       " '0452.tsv',\n",
       " '0145.tsv',\n",
       " '0274.tsv',\n",
       " '0406.tsv',\n",
       " '0021.tsv',\n",
       " '0253.tsv',\n",
       " '0065.tsv',\n",
       " '0206.tsv',\n",
       " '0455.tsv',\n",
       " '0221.tsv',\n",
       " '0097.tsv',\n",
       " '0077.tsv',\n",
       " '0469.tsv',\n",
       " '0412.tsv',\n",
       " '0089.tsv',\n",
       " '0017.tsv',\n",
       " '0353.tsv',\n",
       " '0293.tsv',\n",
       " '0313.tsv',\n",
       " '0010.tsv',\n",
       " '0165.tsv']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(config.INDIVIDUAL_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 socre:\n",
      "1.0\n",
      "\n",
      "Accuracy score:\n",
      "1.0\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     problem     1.0000    1.0000    1.0000         1\n",
      "        test     1.0000    1.0000    1.0000         1\n",
      "   treatment     1.0000    1.0000    1.0000         4\n",
      "\n",
      "   micro avg     1.0000    1.0000    1.0000         6\n",
      "   macro avg     1.0000    1.0000    1.0000         6\n",
      "weighted avg     1.0000    1.0000    1.0000         6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f = open(\"eval_results.txt\", \"r\")\n",
    "print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
