{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.3.0.json:   0%|   â€¦",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a098d4d3db77401ab8a9c4d6356a2946"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-18 09:16:38 INFO: Downloading default packages for language: en (English)...\n",
      "2022-04-18 09:16:39 INFO: File exists: /Users/administrator/stanza_resources/en/default.zip.\n",
      "2022-04-18 09:16:42 INFO: Finished downloading models and saved to /Users/administrator/stanza_resources.\n",
      "2022-04-18 09:16:42 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "========================\n",
      "\n",
      "2022-04-18 09:16:42 INFO: Use device: cpu\n",
      "2022-04-18 09:16:42 INFO: Loading: tokenize\n",
      "2022-04-18 09:16:42 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "from lib2to3.pgen2 import token\n",
    "import re\n",
    "from flask import Flask,render_template,url_for,request\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import os  \n",
    "import random\n",
    "import torch\n",
    "\n",
    "import config\n",
    "from utils import *\n",
    "from processor import *\n",
    "\n",
    "import stanza\n",
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize')\n",
    "\n",
    "from transformers import BertForTokenClassification, BertTokenizer\n",
    "num_labels = len(tag2idx)\n",
    "save_model_address = './trained_models/C-Bert-test'\n",
    "model = BertForTokenClassification.from_pretrained(save_model_address, num_labels=num_labels)\n",
    "tokenizer = BertTokenizer.from_pretrained(save_model_address, do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "long_text = \"ALLERGIES:  Norvasc leads to lightheadedness and headache.  FAMILY HISTORY:  Noncontributory.  SOCIAL HISTORY:  Lives with her husband, Dr. [**Known lastname 1809**] an eminent Pediatric Neurologist at [**Hospital3 1810**].  The patient is a prior smoker, but has not smoked in over 10 years.  She has no known alcohol use and she is a full code.\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Sentence 1 tokens =======\n",
      "====== Sentence 2 tokens =======\n",
      "====== Sentence 3 tokens =======\n",
      "====== Sentence 4 tokens =======\n",
      "====== Sentence 5 tokens =======\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(long_text)\n",
    "all_sentences = []\n",
    "all_tags = []\n",
    "for no, sentence in enumerate(doc.sentences):\n",
    "    print(f'====== Sentence {no + 1} tokens =======')\n",
    "    # temp_token: tokenized words\n",
    "    # input_ids: convert temp_token to id\n",
    "    temp_token, input_ids, attention_masks = create_query(sentence, tokenizer)\n",
    "    result_list = model_inference(model, input_ids)\n",
    "    result = [tag2name[t] for t in result_list]\n",
    "    pretok_sent = \"\"\n",
    "    pretags = \"\"\n",
    "    for i, tok in enumerate(temp_token):\n",
    "        if tok.startswith(\"##\"):\n",
    "            pretok_sent += tok[2:]\n",
    "        else:\n",
    "            pretok_sent += \" \" + tok\n",
    "            pretags += \" \" + result[i]\n",
    "    pretok_sent = pretok_sent[1:]\n",
    "    pretags = pretags[1:]\n",
    "    s = pretok_sent.split()\n",
    "    t = pretags.split()\n",
    "    all_sentences.append(s)\n",
    "    all_tags.append(t)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: [CLS]   tag: [CLS]\n",
      "word: SOCIAL   tag: O\n",
      "word: HISTORY   tag: O\n",
      "word: :   tag: O\n",
      "word: Lives   tag: O\n",
      "word: with   tag: O\n",
      "word: her   tag: O\n",
      "word: husband   tag: O\n",
      "word: ,   tag: O\n",
      "word: Dr   tag: O\n",
      "word: .   tag: X\n",
      "word: [   tag: O\n",
      "word: *   tag: X\n",
      "word: *   tag: X\n",
      "word: Known   tag: O\n",
      "word: lastname   tag: O\n",
      "word: 1809   tag: O\n",
      "word: *   tag: X\n",
      "word: *   tag: X\n",
      "word: ]   tag: X\n",
      "word: an   tag: O\n",
      "word: eminent   tag: O\n",
      "word: Pediatric   tag: O\n",
      "word: Neurologist   tag: O\n",
      "word: at   tag: O\n",
      "word: [   tag: O\n",
      "word: *   tag: X\n",
      "word: *   tag: X\n",
      "word: Hospital3   tag: O\n",
      "word: 1810   tag: O\n",
      "word: *   tag: X\n",
      "word: *   tag: X\n",
      "word: ]   tag: X\n",
      "word: .   tag: O\n",
      "word: [SEP]   tag: [SEP]\n"
     ]
    }
   ],
   "source": [
    "for s, t in zip(all_sentences[2], all_tags[2]):\n",
    "    print('word:', s, end='   ')\n",
    "    print('tag:', t)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for s, t in zip(all_sentences, all_tags):\n",
    "    flag_treatment, flag_problem, flag_test = 0, 0, 0\n",
    "    for i in range(1, len(t)):\n",
    "        print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def print_treatment(word):\n",
    "    return colored(word,'white','on_red', attrs=['underline'])\n",
    "def print_test(word):\n",
    "    return colored(word, 'white','on_magenta')\n",
    "def print_problem(word):\n",
    "    return colored(word, 'magenta','on_cyan', attrs=['reverse', 'bold'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALLERGIES : Norvasc leads to \u001B[1m\u001B[7m\u001B[46m\u001B[35mlightheadedness\u001B[0m and \u001B[1m\u001B[7m\u001B[46m\u001B[35mheadache\u001B[0m . FAMILY HISTORY : Noncontributory . SOCIAL HISTORY : Lives with her husband , Dr . [ * * Known lastname 1809 * * ] an eminent Pediatric Neurologist at [ * * Hospital3 1810 * * ] . The patient is a prior smoker , but has not smoked in over 10 years . She has no known alcohol use and she is a full code . "
     ]
    }
   ],
   "source": [
    "location = {}\n",
    "for s, t in zip(all_sentences, all_tags):\n",
    "    flag_treatment, flag_problem, flag_test = 0, 0, 0\n",
    "    for i in range(1, len(t)):\n",
    "        if t[i] == 'B-treatment':\n",
    "            flag_treatment = 1\n",
    "            print(print_treatment(s[i]), end=' ')\n",
    "        elif t[i] == 'I-treatment' or t[i] == 'X' and flag_treatment == 1 :\n",
    "            print(print_treatment(s[i]), end=' ')\n",
    "        elif t[i] == 'B-test':\n",
    "            flag_test = 1\n",
    "            print(print_test(s[i]), end=' ')\n",
    "        elif t[i] == 'I-test' or t[i] == 'X' and flag_test == 1 :\n",
    "            print(print_test(s[i]), end=' ')\n",
    "        elif t[i] == 'B-problem':\n",
    "            flag_problem = 1\n",
    "            print(print_problem(s[i]), end=' ')\n",
    "        elif t[i] == 'I-problem' or t[i] == 'X' and flag_problem == 1 :\n",
    "            print(print_problem(s[i]), end=' ')\n",
    "        elif t[i] == 'O' or t[i] == 'X':\n",
    "            flag_treatment, flag_problem, flag_test = 0, 0, 0\n",
    "            print(s[i], end=' ')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "['[CLS]',\n 'She',\n 'has',\n 'no',\n 'known',\n 'alcohol',\n 'use',\n 'and',\n 'she',\n 'is',\n 'a',\n 'full',\n 'code',\n '.',\n '[SEP]']"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'tokens'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[0;32mIn [3]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m temp_token, input_ids, attention_masks \u001B[38;5;241m=\u001B[39m \u001B[43mcreate_query\u001B[49m\u001B[43m(\u001B[49m\u001B[43msentence\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Programs/Projects/Medical-Terminology-Detection-Classification/processor.py:86\u001B[0m, in \u001B[0;36mcreate_query\u001B[0;34m(sentence, tokenizer)\u001B[0m\n\u001B[1;32m     83\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate_query\u001B[39m(sentence, tokenizer):\n\u001B[1;32m     85\u001B[0m     temp_token \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m[CLS]\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m---> 86\u001B[0m     word_list \u001B[38;5;241m=\u001B[39m [token\u001B[38;5;241m.\u001B[39mtext \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m \u001B[43msentence\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtokens\u001B[49m]\n\u001B[1;32m     87\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m word_list:\n\u001B[1;32m     88\u001B[0m         temp_token\u001B[38;5;241m.\u001B[39mextend(tokenizer\u001B[38;5;241m.\u001B[39mtokenize(word))\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'str' object has no attribute 'tokens'"
     ]
    }
   ],
   "source": [
    "temp_token, input_ids, attention_masks = create_query(sentence, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"\"\"HISTORY OF PRESENT ILLNESS :\n",
    "The patient is an 80 year old female with breast cancer , status post lumpectomy / radiation therapy / Tamoxifen ( 2000 ) , hypertension , hyperlipidemia , multiple urinary tract infections who presents with a four day prodrome of dry cough , rhinorrhea , coryza , malaise , chills , headache , decreased p.o. intake , loose bowel movements with diarrhea and no blood , decreased urine output , no sick contacts , had flu shot this year .\n",
    "In the Emergency Department , she had labile blood pressure with systolic blood pressure in the 80 s .  \n",
    "Her usual is systolic blood pressure in the 120 s , this was despite two liters of intravenous fluids and she was transferred to the Medical Intensive Care Unit for closer monitoring for possible early sepsis .\n",
    "The Intensive Care Unit course was notable for initially receiving broad spectrum antibiotics as well as a white blood cell count of 11.0 with 15 bands .\n",
    "She was ultimately changed to Levaquin for a possible early pneumonia pending cultures .\n",
    "The Intensive Care Unit course was also notable for negative chest x-ray , two units of packed red blood cells for a hematocrit of 24.0 with appropriate bump in her hematocrit and no evidence of bleeding , stable blood pressure despite a net fluid balance of negative 2.5 liters .\n",
    "No central access was needed .\n",
    "The sepsis protocol was aborted .\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotation(document, 0, 27, HISTORY OF PRESENT ILLNESS :, {'sentence': '0'})\n",
      "Annotation(document, 29, 466, The patient is an 80 year old female with breast cancer , status post lumpectomy / radiation therapy / Tamoxifen ( 2000 ) , hypertension , hyperlipidemia , multiple urinary tract infections who presents with a four day prodrome of dry cough , rhinorrhea , coryza , malaise , chills , headache , decreased p.o. intake , loose bowel movements with diarrhea and no blood , decreased urine output , no sick contacts , had flu shot this year ., {'sentence': '1'})\n",
      "Annotation(document, 468, 569, In the Emergency Department , she had labile blood pressure with systolic blood pressure in the 80 s ., {'sentence': '2'})\n",
      "Annotation(document, 573, 782, Her usual is systolic blood pressure in the 120 s , this was despite two liters of intravenous fluids and she was transferred to the Medical Intensive Care Unit for closer monitoring for possible early sepsis ., {'sentence': '3'})\n",
      "Annotation(document, 784, 936, The Intensive Care Unit course was notable for initially receiving broad spectrum antibiotics as well as a white blood cell count of 11.0 with 15 bands ., {'sentence': '4'})\n",
      "Annotation(document, 938, 1025, She was ultimately changed to Levaquin for a possible early pneumonia pending cultures ., {'sentence': '5'})\n",
      "Annotation(document, 1027, 1306, The Intensive Care Unit course was also notable for negative chest x-ray , two units of packed red blood cells for a hematocrit of 24.0 with appropriate bump in her hematocrit and no evidence of bleeding , stable blood pressure despite a net fluid balance of negative 2.5 liters ., {'sentence': '6'})\n",
      "Annotation(document, 1308, 1337, No central access was needed ., {'sentence': '7'})\n",
      "Annotation(document, 1339, 1371, The sepsis protocol was aborted ., {'sentence': '8'})\n"
     ]
    }
   ],
   "source": [
    "for anno in sd_model.fullAnnotate(message)[0][\"sentences\"]:\n",
    "    print(anno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from scipy.special import softmax\n",
    "from tqdm import trange\n",
    "import torch\n",
    "\n",
    "from scipy.special import softmax\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import BertForTokenClassification, BertTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_text = [['He',\n",
    " 'was',\n",
    " 'admitted',\n",
    " ',',\n",
    " 'taken',\n",
    " 'to',\n",
    " 'the',\n",
    " 'operating',\n",
    " 'room',\n",
    " 'where',\n",
    " 'he',\n",
    " 'underwent',\n",
    " 'L5-S1',\n",
    " 'right',\n",
    " 'hemilaminectomy',\n",
    " 'and',\n",
    " 'discectomy',\n",
    " '.']\n",
    ", \n",
    "['Over',\n",
    " 'the',\n",
    " 'next',\n",
    " 'three',\n",
    " 'days',\n",
    " 'he',\n",
    " 'increased',\n",
    " 'his',\n",
    " 'activity',\n",
    " 'gradually',\n",
    " ',',\n",
    " 'was',\n",
    " 'able',\n",
    " 'to',\n",
    " 'do',\n",
    " 'stairs',\n",
    " 'with',\n",
    " 'Physical',\n",
    " 'Therapy',\n",
    " 'and',\n",
    " 'had',\n",
    " 'pain',\n",
    " 'which',\n",
    " 'could',\n",
    " 'be',\n",
    " 'controlled',\n",
    " 'with',\n",
    " 'oral',\n",
    " 'analgesics',\n",
    " '.'],\n",
    "\n",
    "['However', ',', 'her', 'creatinine', 'continued', 'to', 'increase', '.']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_tags = [['O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-treatment',\n",
    " 'I-treatment',\n",
    " 'I-treatment',\n",
    " 'O',\n",
    " 'B-treatment',\n",
    " 'O'],\n",
    "                 \n",
    " ['O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-treatment',\n",
    " 'I-treatment',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-problem',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-treatment',\n",
    " 'I-treatment',\n",
    " 'O'],\n",
    " \n",
    " ['O', 'O', 'B-test', 'I-test', 'O', 'O', 'O', 'O']                \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bert_out_address = './trained_models/C-Bert-test'\n",
    "\n",
    "tag2idx={'B-problem': 0,\n",
    " 'B-test': 1,\n",
    " 'B-treatment': 2,\n",
    " 'I-problem': 3,\n",
    " 'I-test': 4,\n",
    " 'I-treatment': 5,\n",
    " 'O': 6,\n",
    " 'X': 7,\n",
    " '[CLS]': 8,\n",
    " '[SEP]': 9\n",
    " }\n",
    "\n",
    "tag2name = {0: 'B-problem',\n",
    "    1: 'B-test',\n",
    "    2: 'B-treatment',\n",
    "    3: 'I-problem',\n",
    "    4: 'I-test',\n",
    "    5: 'I-treatment',\n",
    "    6: 'O',\n",
    "    7: 'X',\n",
    "    8: '[CLS]',\n",
    "    9: '[SEP]'}\n",
    "\n",
    "max_len  = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = clinical_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForTokenClassification.from_pretrained(bert_out_address, num_labels=len(tag2idx))\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_out_address, do_lower_case=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = ' '.join(test_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He was admitted , taken to the operating room where he underwent L5-S1 right hemilaminectomy and discectomy .'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts = []\n",
    "temp_token = []\n",
    "temp_token.append('[CLS]')\n",
    "token_list = tokenizer.tokenize(test_query)\n",
    "temp_token.extend(token_list)\n",
    "temp_token = temp_token[:max_len-1]\n",
    "temp_token.append('[SEP]')\n",
    "input_id = tokenizer.convert_tokens_to_ids(temp_token)\n",
    "padding_len = max_len - len(input_id)\n",
    "input_id = input_id + ([0] * padding_len)\n",
    "tokenized_texts.append(input_id)\n",
    "attention_masks = [[int(i>0) for i in input_id]]\n",
    "\n",
    "tokenized_texts = torch.tensor(tokenized_texts)\n",
    "attention_masks = torch.tensor(attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  1124,  1108,  4120,   117,  1678,  1106,  1103,  3389,  1395,\n",
       "          1187,  1119,  9315,   149,  1571,   118,   156,  1475,  1268, 23123,\n",
       "          8009,  9685,  5822, 18574,  1105,  6187, 10294, 18778,  1183,   119,\n",
       "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set save model to Evalue loop\n",
    "model.eval()\n",
    "# Get model predict result\n",
    "with torch.no_grad():\n",
    "        outputs = model(tokenized_texts, token_type_ids=None,\n",
    "        attention_mask=None,)\n",
    "        # For eval mode, the first result of outputs is logits\n",
    "        logits = outputs[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_results = logits.detach().cpu().numpy()\n",
    "result_arrays_soft = softmax(predict_results[0])\n",
    "result_list = np.argmax(result_arrays_soft,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [tag2name[t] for t in result_list]\n",
    "pretok_sent = \"\"\n",
    "pretags = \"\"\n",
    "for i, tok in enumerate(temp_token):\n",
    "     if tok.startswith(\"##\"):\n",
    "         pretok_sent += tok[2:]\n",
    "     else:\n",
    "         pretok_sent += \" \" + tok\n",
    "         pretags += \" \" + result[i]\n",
    "pretok_sent = pretok_sent[1:]\n",
    "pretags = pretags[1:]\n",
    "\n",
    "s = pretok_sent.split()\n",
    "t = pretags.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token:[CLS]\n",
      "Predict_Tag:O\n",
      "\n",
      "Token:He\n",
      "Predict_Tag:O\n",
      "\n",
      "Token:was\n",
      "Predict_Tag:O\n",
      "\n",
      "Token:admitted\n",
      "Predict_Tag:O\n",
      "\n",
      "Token:,\n",
      "Predict_Tag:O\n",
      "\n",
      "Token:taken\n",
      "Predict_Tag:O\n",
      "\n",
      "Token:to\n",
      "Predict_Tag:O\n",
      "\n",
      "Token:the\n",
      "Predict_Tag:O\n",
      "\n",
      "Token:operating\n",
      "Predict_Tag:O\n",
      "\n",
      "Token:room\n",
      "Predict_Tag:O\n",
      "\n",
      "Token:where\n",
      "Predict_Tag:O\n",
      "\n",
      "Token:he\n",
      "Predict_Tag:O\n",
      "\n",
      "Token:underwent\n",
      "Predict_Tag:O\n",
      "\n",
      "Token:L\n",
      "Predict_Tag:B-treatment\n",
      "\n",
      "Token:##5\n",
      "Predict_Tag:X\n",
      "\n",
      "Token:-\n",
      "Predict_Tag:X\n",
      "\n",
      "Token:S\n",
      "Predict_Tag:X\n",
      "\n",
      "Token:##1\n",
      "Predict_Tag:X\n",
      "\n",
      "Token:right\n",
      "Predict_Tag:I-treatment\n",
      "\n",
      "Token:hem\n",
      "Predict_Tag:I-treatment\n",
      "\n",
      "Token:##ila\n",
      "Predict_Tag:X\n",
      "\n",
      "Token:##mine\n",
      "Predict_Tag:X\n",
      "\n",
      "Token:##ct\n",
      "Predict_Tag:X\n",
      "\n",
      "Token:##omy\n",
      "Predict_Tag:X\n",
      "\n",
      "Token:and\n",
      "Predict_Tag:O\n",
      "\n",
      "Token:disc\n",
      "Predict_Tag:B-treatment\n",
      "\n",
      "Token:##ec\n",
      "Predict_Tag:X\n",
      "\n",
      "Token:##tom\n",
      "Predict_Tag:X\n",
      "\n",
      "Token:##y\n",
      "Predict_Tag:X\n",
      "\n",
      "Token:.\n",
      "Predict_Tag:O\n",
      "\n",
      "Token:[SEP]\n",
      "Predict_Tag:[SEP]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, mark in enumerate(attention_masks[0]):\n",
    "    if mark>0:\n",
    "        print(\"Token:%s\"%(temp_token[i]))\n",
    "#         print(\"Tag:%s\"%(result_list[i]))\n",
    "        print(\"Predict_Tag:%s\"%(tag2name[result_list[i]]))\n",
    "        #print(\"Posibility:%f\"%(result_array[i][result_list[i]]))\n",
    "        print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ' '.join(temp_token)\n",
    "re = ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "from docx.enum.text import WD_COLOR_INDEX\n",
    "# Create an instance of a word document\n",
    "doc = docx.Document()\n",
    "\n",
    "# Add a Title to the document \n",
    "doc.add_heading('Results', 0)\n",
    "\n",
    "# Creating paragraph with some content\n",
    "para = doc.add_paragraph(''' ''')\n",
    "  \n",
    "flag_treatment, flag_problem, flag_test = 0, 0, 0 \n",
    "for i in range(1, len(t)-1):\n",
    "    if t[i] == 'B-treatment':\n",
    "        flag_treatment = 1\n",
    "        para.add_run(s[i]+' ').font.highlight_color = WD_COLOR_INDEX.RED\n",
    "        # print(print_treatment(s[i]), end=' ')\n",
    "    elif (t[i] == 'I-treatment' or t[i] == 'X') and flag_treatment == 1 :\n",
    "        para.add_run(s[i]+' ').font.highlight_color = WD_COLOR_INDEX.RED\n",
    "    elif t[i] == 'B-test':\n",
    "        flag_test = 1\n",
    "        para.add_run(s[i]+' ').font.highlight_color = WD_COLOR_INDEX.PINK\n",
    "    elif (t[i] == 'I-test' or t[i] == 'X') and flag_test == 1 :\n",
    "        para.add_run(s[i]+' ').font.highlight_color = WD_COLOR_INDEX.PINK\n",
    "    elif t[i] == 'B-problem':\n",
    "        flag_problem = 1\n",
    "        para.add_run(s[i]+' ').font.highlight_color = WD_COLOR_INDEX.TURQUOISE\n",
    "    elif (t[i] == 'I-problem' or t[i] == 'X') and flag_problem == 1 :\n",
    "        para.add_run(s[i]+' ').font.highlight_color = WD_COLOR_INDEX.TURQUOISE    \n",
    "    elif t[i] == 'O':\n",
    "        flag_treatment, flag_problem, flag_test = 0, 0, 0 \n",
    "        para.add_run(s[i]+' ').font.highlight_color = WD_COLOR_INDEX.AUTO\n",
    "        \n",
    "\n",
    "# # Adding more content to paragraph and highlighting them\n",
    "# para.add_run(''' It contains well written, well thought and well-explained '''\n",
    "#             ).font.highlight_color = WD_COLOR_INDEX.YELLOW\n",
    "  \n",
    "# # Adding more content to paragraph\n",
    "# para.add_run('''computer science and programming articles, quizzes etc.''')\n",
    "  \n",
    "# Now save the document to a location \n",
    "doc.save('result.docx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from termcolor import colored\n",
    "def print_treatment(word):\n",
    "    return colored(word,'white','on_red', attrs=['underline'])\n",
    "def print_test(word):\n",
    "    return colored(word, 'white','on_magenta')\n",
    "def print_problem(word):\n",
    "    return colored(word, 'magenta','on_cyan', attrs=['reverse', 'bold'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m\u001B[7m\u001B[46m\u001B[35mproblem\u001B[0m\n",
      "\u001B[4m\u001B[41m\u001B[37mtreatment\u001B[0m\n",
      "\u001B[45m\u001B[37mtest\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "print(print_problem('problem'))\n",
    "print(print_treatment('treatment'))\n",
    "print(print_test('test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He was admitted , taken to the operating room where he underwent \u001B[4m\u001B[41m\u001B[37mL5\u001B[0m \u001B[4m\u001B[41m\u001B[37m-\u001B[0m \u001B[4m\u001B[41m\u001B[37mS1\u001B[0m \u001B[4m\u001B[41m\u001B[37mright\u001B[0m \u001B[4m\u001B[41m\u001B[37mhemilaminectomy\u001B[0m and \u001B[4m\u001B[41m\u001B[37mdiscectomy\u001B[0m . "
     ]
    }
   ],
   "source": [
    "from termcolor import colored\n",
    "location = {}\n",
    "flag_treatment, flag_problem, flag_test = 0, 0, 0 \n",
    "for i in range(1, len(t)-1):\n",
    "    if t[i] == 'B-treatment':\n",
    "        flag_treatment = 1\n",
    "        print(print_treatment(s[i]), end=' ')\n",
    "    elif t[i] == 'I-treatment' or t[i] == 'X' and flag_treatment == 1 :\n",
    "        print(print_treatment(s[i]), end=' ')\n",
    "    elif t[i] == 'B-test':\n",
    "        flag_test = 1\n",
    "        print(print_test(s[i]), end=' ')\n",
    "    elif t[i] == 'I-test' or t[i] == 'X' and flag_test == 1 :\n",
    "        print(print_test(s[i]), end=' ')\n",
    "    elif t[i] == 'B-problem':\n",
    "        flag_problem = 1\n",
    "        print(print_problem(s[i]), end=' ')\n",
    "    elif t[i] == 'I-problem' or t[i] == 'X' and flag_problem == 1 :\n",
    "        print(print_problem(s[i]), end=' ')    \n",
    "    elif t[i] == 'O':\n",
    "        flag_treatment, flag_problem, flag_test = 0, 0, 0 \n",
    "        print(s[i], end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_lable = []\n",
    "temp_token = []\n",
    "\n",
    "# Add [CLS] at the front \n",
    "temp_lable.append('[CLS]')\n",
    "temp_token.append('[CLS]')\n",
    "\n",
    "# Tokenize words\n",
    "# [lidocaine patch] -> lid ##oc ##aine patch\n",
    "# [B-treatment I-treatment] -> B-treatment X X I-treatment\n",
    "for word, lab in zip(word_list, label):\n",
    "    token_list = tokenizer.tokenize(word)\n",
    "    for m, token in enumerate(token_list):\n",
    "        temp_token.append(token)\n",
    "        if m == 0:\n",
    "            temp_lable.append(lab)\n",
    "        else:\n",
    "            temp_lable.append('X')  \n",
    "            \n",
    "# Add [SEP] at the end\n",
    "temp_lable.append('[SEP]')\n",
    "temp_token.append('[SEP]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object) :\n",
    "    \"\"\"\n",
    "    input a dataframe\n",
    "    \n",
    "    \n",
    "    Generate sets of words and tags.\n",
    "    self.sentence:\n",
    "    Each sentence is a list\n",
    "    [('Supraventricular', 'B-problem'),\n",
    "    ('tachycardia', 'I-problem'),\n",
    "    ('(', 'O'),\n",
    "    ('on', 'O'),\n",
    "    ('a', 'B-treatment'),\n",
    "    ('beta', 'I-treatment'),\n",
    "    ('blocker', 'I-treatment'),\n",
    "    (')', 'O')]\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, t) for w, t in zip(s[\"word\"].values.tolist(),\n",
    "                                                        #    s[\"POS\"].values.tolist(),\n",
    "                                                           s[\"tag\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"sentence #\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def get_examples(self):\n",
    "        return random.sample(self.sentences, 10)\n",
    "    \n",
    "    def get_sentences(self):\n",
    "        return[[s[0] for s in sent] for sent in self.sentences]\n",
    "        \n",
    "    def get_labels(self):\n",
    "        return [[s[1] for s in sent] for sent in self.sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputGenerater(object):\n",
    "    def __init__(self, sentences, labels) -> None:\n",
    "        self.tokenized_texts = []\n",
    "        self.word_piece_labels = []\n",
    "        i_inc = 0\n",
    "        for word_list,label in (zip(sentences,labels)):\n",
    "            temp_lable = []\n",
    "            temp_token = []\n",
    "            \n",
    "            # Add [CLS] at the front \n",
    "            temp_lable.append('[CLS]')\n",
    "            temp_token.append('[CLS]')\n",
    "            \n",
    "            for word,lab in zip(word_list,label):\n",
    "                token_list = tokenizer.tokenize(word)\n",
    "                for m,token in enumerate(token_list):\n",
    "                    temp_token.append(token)\n",
    "                    if m==0:\n",
    "                        temp_lable.append(lab)\n",
    "                    else:\n",
    "                        temp_lable.append('X')  \n",
    "                        \n",
    "            # Add [SEP] at the end\n",
    "            temp_lable.append('[SEP]')\n",
    "            temp_token.append('[SEP]')\n",
    "            \n",
    "            self.tokenized_texts.append(temp_token)\n",
    "            self.word_piece_labels.append(temp_lable)\n",
    "            \n",
    "            if 5 > i_inc:\n",
    "                print(\"No.%d,len:%d\"%(i_inc,len(temp_token)))\n",
    "                print(\"texts:%s\"%(\" \".join(temp_token)))\n",
    "                print(\"No.%d,len:%d\"%(i_inc,len(temp_lable)))\n",
    "                print(\"lables:%s\"%(\" \".join(temp_lable)))\n",
    "            i_inc +=1\n",
    "            \n",
    "        self.input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in self.tokenized_texts],\n",
    "                    maxlen=config.MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "            \n",
    "    def get_input_ids(self):    \n",
    "        return self.input_ids\n",
    "    \n",
    "    def get_tags(self):\n",
    "        tags = pad_sequences([[get_tag2idx().get(l) for l in lab] for lab in self.word_piece_labels],\n",
    "                        maxlen=config.MAX_LEN, value=get_tag2idx()[\"O\"], padding=\"post\",\n",
    "                        dtype=\"long\", truncating=\"post\")\n",
    "        return tags\n",
    "    def get_attention_masks(self):\n",
    "        attention_masks = [[int(i>0) for i in ii] for ii in self.input_ids]\n",
    "        return attention_masks\n",
    "    def get_segment_ids(self):\n",
    "        segment_ids = [[0] * len(input_id) for input_id in self.input_ids]\n",
    "        return segment_ids\n",
    "\n",
    "\n",
    "def convert_to_tensor(*inputs, drop_last=False):\n",
    "    data = TensorDataset(*tuple(torch.tensor(inputs)))\n",
    "    data_sampler = RandomSampler(data)\n",
    "    # Drop last can make batch training better for the last one\n",
    "    dataloader = DataLoader(data, sampler=data_sampler, batch_size=config.BATCH_NUM, drop_last=drop_last)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.0,len:5\n",
      "texts:[CLS] admission date : [SEP]\n",
      "No.0,len:5\n",
      "lables:[CLS] O O O [SEP]\n",
      "No.1,len:7\n",
      "texts:[CLS] 2014 - 12 - 29 [SEP]\n",
      "No.1,len:7\n",
      "lables:[CLS] O X X X X [SEP]\n",
      "No.2,len:6\n",
      "texts:[CLS] all ##er ##gies : [SEP]\n",
      "No.2,len:6\n",
      "lables:[CLS] O X X O [SEP]\n",
      "No.3,len:4\n",
      "texts:[CLS] 17 units [SEP]\n",
      "No.3,len:4\n",
      "lables:[CLS] O O [SEP]\n",
      "No.4,len:22\n",
      "texts:[CLS] includes a history of at ##rial fi ##bri ##lla ##tion with good heart rate control on dig ##ox ##in . [SEP]\n",
      "No.4,len:22\n",
      "lables:[CLS] O O O O B-problem X I-problem X X X O O B-treatment I-treatment I-treatment O B-treatment X X O [SEP]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pad_sequences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-10-b054d79cb97e>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0msentences\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msg\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_sentences\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mtags\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msg\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_labels\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0mtest_sets\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mInputGenerater\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msentences\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0msentences\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabels\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtags\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      6\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0mtest_inputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtest_sets\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_input_ids\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-6-dec2e08486b9>\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, sentences, labels)\u001B[0m\n\u001B[1;32m     35\u001B[0m             \u001B[0mi_inc\u001B[0m \u001B[0;34m+=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     36\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 37\u001B[0;31m         self.input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in self.tokenized_texts],\n\u001B[0m\u001B[1;32m     38\u001B[0m                     maxlen=config.MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n\u001B[1;32m     39\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'pad_sequences' is not defined"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv(config.data_path_test, sep=\"\\t\").astype(str)\n",
    "sg = SentenceGetter(df_test)\n",
    "sentences = sg.get_sentences()\n",
    "tags = sg.get_labels()\n",
    "test_sets = InputGenerater(sentences=sentences, labels=tags)\n",
    "\n",
    "test_inputs = test_sets.get_input_ids()\n",
    "test_tags = test_sets.get_tags()\n",
    "test_attetion_masks = test_sets.get_attention_masks()\n",
    "\n",
    "test_dataloader = utils.convert_to_tensor(test_inputs, test_attetion_masks, test_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 2.7.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}